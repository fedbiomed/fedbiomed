# End-to-end test

## Introduction: what are end-to-end tests?

In opposition to **unit-tests**, which are basically made for testing  small components or functionalities of the code, **End-to-end tests** are tests that test the whole functionality of the software, as if an end-user was using the software. It therefore may include secure aggregation facility configuration, loading datasets, loading certificates, and every other upcoming or existing functionalities.

Hence an **end-to-end** testing facility won't require fakes or Mocks, but the real components. For **end-to-end tests**, you can just make sure the usage of the software won't fail and that the final results are correct, you don't need to do assertions for each test.

## Material

End-to-end tests are run with [pytest](https://docs.pytest.org/) as test framework, as well as methods designed in `tests/end2end/helpers` folder.

## How to run tests

* setup the python environment

```
source ../scripts/fedbiomed_environment researcher
```
***
* run all tests

```
cd tests
pytest -s -v end2end/e2e_*.py
```

* run a specific test file

```
cd tests
pytest -s -v end2end/e2e_xxxx.py
```

* run a specific test
(for instance `test_experiment_run_01` in `tests/end2end/e2e_mnist_pytorch.py` end-to-end test file)

```
cd tests
pytest -s -v end2end/e2e_mnist_pytorch.py::test_experiment_run_01
```
## How to write end-to-end tests

### Naming convention

Tests file should be located in the folder `tests/end2end/`. They should be named with the `e2e` prefix:
 for instance: `e2e_my_test.py`


### Writing end2end test

* `setup` method: is run at the beginning of the tests of a single test file. It contains instructions on how to set up Components. Methods for `helpers.py` can be used here for setting up Nodes and dataset.

* `test_experiment_run_xxx` methods: contains the instructions of the tests

* `training plans` should be separated from the tests (and defined in the folder `tests/end2end/experiments/training_plans`)

The basic structure of an **end-to-end test** file is the following:

```
import pytest
from helpers import (
    create_component,
    add_dataset_to_node,
    start_nodes,
    kill_subprocesses,
    clear_component_data,
    clear_experiment_data)

from experiments.training_plans.mnist_pytorch_training_plan import MyTrainingPlan
from fedbiomed.researcher.experiment import Experiment

@pytest.fixture(scope="module", autouse=True)
def setup(request):
    '''
    before each individual test
    '''
    node_1 = create_component(ComponentType.NODE, config_name="config_n1.ini")  # create one node

    mnist_dataset = {
        "name": "MNIST",
        "description": "MNIST DATASET",
        "tags": "#MNIST,#dataset",
        "data_type": "default",
        "path": "./data/"
    }
    add_dataset_to_node(node_1, mnist_dataset)


def test_XXX_01_whatever(self):
    '''
    test the whatever feature
    '''
    exp = Experiment(
        ...)

    exp.run()

    clear_experiment_data(exp)  # remove folder created when running an Experiment

    # some code
    ...

```

#### Post actions

After all the tests are completed the data generated by the nodes should be removed, and the node processes  **HAVE TO BE STOPPED**. Leaving the node processes running on the background will have side effects on the next tests being executed. You can use the helper methods `clear_component_data` and `kill_subprocesses` as shown in the following code snippet to clear node components and stop the processes.


```python

from .helpers import temproray_test_directory

@pytest.fixture(scope="module", autouse=True)
def setup(request):
    """Setup fixture for the module"""

    print("Creating components ---------------------------------------------")
    node_1 = create_component(
        ComponentType.NODE,
        directory=temproray_test_directory.name,
        component_name="node-1.ini"
    )
    node_2 = create_component(
        ComponentType.NODE,
        directory=temproray_test_directory.name,
        config_name="node-2"
    )

    researcher = create_component(
        ComponentType.RESEARCHER,
        directory=temproray_test_directory.name,
        component_name="my-researhcer"
    )
    dataset = {
        "name": "MNIST",
        "description": "MNIST DATASET",
        "tags": "#MNIST,#dataset",
        "data_type": "default",
        "path": "./data/"
    }

    print("Adding first dataset --------------------------------------------")
    add_dataset_to_node(node_1, dataset)
    print("adding second dataset")
    add_dataset_to_node(node_2, dataset)

    time.sleep(1)

    # Starts the nodes
    node_processes, _ = start_nodes([node_1, node_2])

    # Clear files and processes created for the tests
    def clear():
        kill_subprocesses(node_processes)

        print("Clearing component data")
        clear_component_data(node_1)
        clear_component_data(node_2)

    # Good to wait 3 second to give time to nodes start
    print("Sleep 5 seconds. Giving some time for nodes to start")
    time.sleep(5)

    request.addfinalizer(clear)


```

Another post action is the cleaning of the experiment after it is completed. **IT IS IMPORTANT** to call `clear_experiment_data` after `experiment.run` is completed. This method will make sure that the experiment data is deleted, and most importantly gRPC server is stopped before starting a new experiment. Please see following example code snippet that uses `clear_experiment_data`.

```python
def test_experiment_run_01():
    """Tests running training mnist with basic configuration"""
    model_args = {}
    tags = ['#MNIST', '#dataset']
    rounds = 1
    training_args = {
        'loader_args': { 'batch_size': 48, },
        'optimizer_args': {
            "lr" : 1e-3
        },
        'num_updates': 100,
        'dry_run': False,

    }

    exp = Experiment(
        tags=tags,
        model_args=model_args,
        training_plan_class=MyTrainingPlan,
        training_args=training_args,
        round_limit=rounds,
        aggregator=FedAverage(),
        node_selection_strategy=None,)

    exp.run()

    # Clean experiment data
    clear_experiment_data(exp)

```

### Dataset Description

The datasets are described as Python dict, and converted to JSON afterwasds to be pass it to CLI command `dataset add --file`.

```
{
    "name": "Mednist data",
    "description": "Mednist",
    "tags": "mednist",
    "data_type": "images",
    "path": "$HOME/tmp/MedNIST"
}
```

You can use OS environment variables in this script via `os.environ`.


