{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data imputation with Fedbiomed using MIWAE, using ADNI (non-iid distributed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show:\n",
    "* how to obtain mean and std in a federated manner, to perform afterwards local dataset standardization with respect to the global dataset\n",
    "* how to impute missing not at random (MAR) data in a federated setting using MIWAE (https://arxiv.org/abs/2006.12871). \n",
    "\n",
    "We will compare results of federated training using FedAvg, FedProx (with both local standardization and federated standardization), with local results.\n",
    "\n",
    "We are going to use data extracted from ADNI. In particular, we consider 311 participants extracted from the ADNI dataset, among cognitively normal (NL) (104 subjects) and patients diagnosed with AD (207 subjects). All participants are associated with multiple data views: cognitive scores including MMSE, CDR-SB, ADAS-Cog-11 and RAVLT (CLINIC), Magnetic resonance imaging (MRI), Fluorodeoxyglucose-PET (FDG) and AV45-Amyloid PET (AV45) images. We are going to generate a test dataset containing the 20% of the samples (using `train_test_split` from `sklearn.model_selection`), while the training dataset will be further distributed across 2 clients, one containing exclusively patients affectd with Alzheimer Disease (171 sample), and the other one containing only healthy patients (77 sample). From each local dataset we will remove randomly 30% of the observations, while we remove 50 of observations randomly from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data (skip this part if done offline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment we will use the data extracted from ADNI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "cwd = os.getcwd()\n",
    "data_file = os.path.join(cwd, \"data/ADNI/data_irene_dx_2g_corrected.csv\")\n",
    "raw_df = pd.read_csv(data_file, sep=\",\",index_col=\"RID\")\n",
    "target = raw_df[\"DX\"]\n",
    "data = deepcopy(raw_df)\n",
    "del data[\"DX\"]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train test split\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data, target, test_size=0.20, random_state=42)\n",
    "\n",
    "# split train across datasets in a non-iid manner: \n",
    "#client 1 will only contain ADNI subjects, client 2 will only contain controls\n",
    "client_1 = data_train.loc[labels_train == 'AD']\n",
    "client_2 = data_train.loc[labels_train == 'NL']\n",
    "\n",
    "#Clients_data contains original full data of each client \n",
    "Clients_data=[client_1, client_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from each dataset we will remove randomly 30% of data\n",
    "np.random.seed(1234)\n",
    "\n",
    "# 30% of missing data for client 1, 30% for client 2\n",
    "perc_miss_list = [0.3,0.3] \n",
    "\n",
    "#Clients_data contains data of each client with missing entries wrt to perc_miss_list\n",
    "Clients_missing = []\n",
    "for perc,c in enumerate(Clients_data):\n",
    "    perc_miss=perc_miss_list[perc]\n",
    "    n = c.shape[0] # number of observations\n",
    "    p = c.shape[1] # number of features\n",
    "    xmiss = np.copy(c)\n",
    "    xmiss_flat = xmiss.flatten()\n",
    "    miss_pattern = np.random.choice(n*p, np.floor(n*p*perc_miss).astype(np.int_),\\\n",
    "                                    replace=False)\n",
    "    xmiss_flat[miss_pattern] = np.nan \n",
    "    xmiss = xmiss_flat.reshape([n,p]) # in xmiss, the missing values are represented by nans\n",
    "    mask = np.isfinite(xmiss) # binary mask that indicates which values are missing\n",
    "    Clients_missing.append(xmiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we save the datasets, which will be distributed across three nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.makedirs('data/clients_data', exist_ok=True) \n",
    "for i in range(len(Clients_missing)):\n",
    "    pd.DataFrame(Clients_missing[i]).to_csv('data/clients_data/client_'+str(i+1)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover full dataset and test dataset for testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "N_cl = 2\n",
    "Split_type='notIID'\n",
    "\n",
    "Clients_data=[]\n",
    "Clients_missing=[]\n",
    "for i in range(N_cl):\n",
    "    data_full_file = os.path.join(cwd, \"data/clients_data/client_full_\"+Split_type+\"_\"+str(i+1)+\".csv\")\n",
    "    data_full = pd.read_csv(data_full_file, sep=\",\",index_col=False)\n",
    "    Clients_data.append(data_full)\n",
    "    data_file = os.path.join(cwd, \"data/clients_data/client_\"+Split_type+\"_\"+str(i+1)+\".csv\")\n",
    "    data = pd.read_csv(data_file, sep=\",\",index_col=False)\n",
    "    Clients_missing.append(data)\n",
    "\n",
    "test_file = os.path.join(cwd, \"data/test_data/test_full_\"+Split_type+\".csv\")\n",
    "data_test = pd.read_csv(test_file, sep=\",\",index_col=False)\n",
    "test_missing_file = os.path.join(cwd, \"data/test_data/test_\"+Split_type+\".csv\")\n",
    "data_test_missing = pd.read_csv(test_missing_file, sep=\",\",index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os\n",
    "\n",
    "os.makedirs('results', exist_ok=True) \n",
    "if not os.path.exists('results/output_notiid.csv'):\n",
    "    output = open(\"results/output_notiid.csv\", \"w\")\n",
    "    writer = csv.DictWriter(output, \n",
    "                            fieldnames=['Split_type', 'Test_data', 'model', \n",
    "                                        'N_train_centers', 'Size', 'N_rounds', 'N_epochs',\n",
    "                                        'std_training', 'std_testing', 'MSE'])\n",
    "    writer.writeheader()\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the network\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the nodes up\n",
    "It is necessary to previously configure a node:\n",
    "1. `./scripts/fedbiomed_run node add`\n",
    "  * Select option 1 (csv) to add client_1 dataset to the first node\n",
    "  * Provide the correct tag by entering:  `adni`\n",
    "  * Pick the folder where client_1 dataset has been saved\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "3. Run the node using `./scripts/fedbiomed_run node start`. Wait until you get `Starting task manager`. it means you are online.\n",
    "4. Following the same procedure, you can create additional nodes for clients 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.requests import Requests\n",
    "req = Requests()\n",
    "req.list(verbose=True)\n",
    "xx = req.list()\n",
    "dataset_size = [xx[i][0]['shape'][1] for i in xx]\n",
    "assert min(dataset_size)==max(dataset_size)\n",
    "data_size = dataset_size[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover global mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from fedbiomed.common.constants import ProcessTypes\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class FedMeanStdTrainingPlan(TorchTrainingPlan):\n",
    "    \n",
    "    def init_dependencies(self):\n",
    "        deps = [\"import pandas as pd\",\n",
    "               \"import numpy as np\",\n",
    "               \"from copy import deepcopy\"]\n",
    "        return deps\n",
    "        \n",
    "    def init_model(self,model_args):\n",
    "        \n",
    "        model = self.MeanStd(model_args)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    class MeanStd(nn.Module):\n",
    "        def __init__(self, model_args):\n",
    "            super().__init__()\n",
    "            self.n_features=model_args['n_features']\n",
    "            \n",
    "            self.mean = nn.Parameter(torch.zeros(self.n_features,dtype=torch.float64),requires_grad=False)\n",
    "            self.std = nn.Parameter(torch.zeros(self.n_features,dtype=torch.float64),requires_grad=False)\n",
    "            self.size = nn.Parameter(torch.zeros(self.n_features,dtype=torch.float64),requires_grad=False)\n",
    "            self.fake = nn.Parameter(torch.randn(1),requires_grad=True)\n",
    "\n",
    "        def forward(self, data):\n",
    "            data_np = data.numpy()\n",
    "        \n",
    "            ### Implementing with np.nanmean, np.nanstd\n",
    "            \n",
    "            self.size += torch.Tensor([data_np[:,dim].size - np.count_nonzero(np.isnan(data_np[:,dim]))\\\n",
    "                                       for dim in range(self.n_features)])\n",
    "            self.mean += torch.from_numpy(np.nanmean(data_np,0))\n",
    "            self.std += torch.from_numpy(np.nanstd(data_np,0))\n",
    "            \n",
    "            # ### Implementing with torch.mean, torch.std\n",
    "        \n",
    "            # size_loc = torch.zeros(self.n_features)\n",
    "            # mean_loc = torch.zeros(self.n_features)\n",
    "            # std_loc = torch.zeros(self.n_features)\n",
    "            # for dim in range(self.n_features):\n",
    "            #     data_i = deepcopy(data[:,dim][mask[:,dim].bool()])\n",
    "            #     size_loc[dim] = data_i.shape[0]\n",
    "            #     mean_loc[dim] = torch.mean(data_i, dim=0)\n",
    "            #     std_loc[dim] = torch.std(data_i, unbiased=False, dim=0)\n",
    "            # self.size += size_loc\n",
    "            # self.mean += mean_loc\n",
    "            # self.std += std_loc\n",
    "            \n",
    "            return self.fake\n",
    "    \n",
    "        \n",
    "    def training_data(self):\n",
    "        \n",
    "        df = pd.read_csv(self.dataset_path, sep=',', index_col=False)\n",
    "        \n",
    "        ### NOTE: batch_size should be == dataset size ###\n",
    "        batch_size = df.shape[0]\n",
    "        x_train = df.values\n",
    "        x_mask = np.isfinite(x_train)\n",
    "        xhat_0 = np.copy(x_train)\n",
    "        ### NOTE: we keep nan when data is missing\n",
    "        #xhat_0[np.isnan(x_train)] = 0\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        \n",
    "        data_manager = DataManager(dataset=xhat_0 , target=x_mask , **train_kwargs)\n",
    "        \n",
    "        return data_manager\n",
    "    \n",
    "    def training_step(self, data, mask):\n",
    "        \n",
    "        return self.model().forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: we need to perform only 1 round of 1 epoch\n",
    "\n",
    "model_args = {'n_features':data_size}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'optimizer_args': {\n",
    "        'lr': 0\n",
    "    }, \n",
    "    'log_interval' : 1,\n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    #'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "tags =  ['adni']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedstandard import FedStandard\n",
    "\n",
    "fed_mean_std = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=FedMeanStdTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=1,\n",
    "                 aggregator=FedStandard(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_mean_std.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_mean = fed_mean_std.aggregated_params()[0]['params']['fed_mean']\n",
    "fed_std = fed_mean_std.aggregated_params()[0]['params']['fed_std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an experiment model and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a torch.nn MIWAETrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we include a function, ``standardize_data``, which allow to standardize data either with respect to a mean and std provided by the user, or locally, considering only local data for each client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.distributions as td\n",
    "import pandas as pd\n",
    "\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from fedbiomed.common.constants import ProcessTypes\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class MIWAETrainingPlan(TorchTrainingPlan):\n",
    "    \n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "               \"import torch.distributions as td\",\n",
    "               \"import pandas as pd\",\n",
    "               \"import numpy as np\"]\n",
    "        return deps\n",
    "        \n",
    "    def init_model(self,model_args):\n",
    "        \n",
    "        if 'standardization' in model_args:\n",
    "            self.standardization = True\n",
    "            if (('fed_mean' in model_args['standardization']) and ('fed_std' in model_args['standardization'])):\n",
    "                self.fed_mean = np.array(model_args['standardization']['fed_mean'])\n",
    "                self.fed_std = np.array(model_args['standardization']['fed_std'])\n",
    "            else:\n",
    "                self.fed_mean = None\n",
    "                self.fed_std = None\n",
    "                \n",
    "        self.n_features=model_args['n_features']\n",
    "        self.n_latent=model_args['n_latent']\n",
    "        self.n_hidden=model_args['n_hidden']\n",
    "        self.n_samples=model_args['n_samples']\n",
    "        \n",
    "        model = self.MIWAE(model_args)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    class MIWAE(nn.Module):\n",
    "        def __init__(self, model_args):\n",
    "            super().__init__()\n",
    "\n",
    "            n_features=model_args['n_features']\n",
    "            n_latent=model_args['n_latent']\n",
    "            n_hidden=model_args['n_hidden']\n",
    "            n_samples=model_args['n_samples']\n",
    "\n",
    "            # the encoder will output both the mean and the diagonal covariance\n",
    "            self.encoder=nn.Sequential(\n",
    "                            torch.nn.Linear(n_features, n_hidden),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(n_hidden, n_hidden),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(n_hidden, 2*n_latent),  \n",
    "                            )\n",
    "            # the decoder will output both the mean, the scale, \n",
    "            # and the number of degrees of freedoms (hence the 3*p)\n",
    "            self.decoder = nn.Sequential(\n",
    "                            torch.nn.Linear(n_latent, n_hidden),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(n_hidden, n_hidden),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(n_hidden, 3*n_features),  \n",
    "                            )\n",
    "\n",
    "            self.encoder.apply(self.weights_init)\n",
    "            self.decoder.apply(self.weights_init)\n",
    "    \n",
    "        def weights_init(self,layer):\n",
    "            if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)\n",
    "    \n",
    "    def init_optimizer(self,optimizer_args):\n",
    "        \n",
    "        optimizer = torch.optim.Adam(list(self.model().encoder.parameters()) \\\n",
    "                                    + list(self.model().decoder.parameters()),lr = optimizer_args['lr'])\n",
    "        \n",
    "        return optimizer\n",
    "        \n",
    "        \n",
    "    def miwae_loss(self,iota_x,mask):\n",
    "        # prior\n",
    "        self.p_z = td.Independent(td.Normal(loc=torch.zeros(self.n_latent).to(self._device)\\\n",
    "                                       ,scale=torch.ones(self.n_latent).to(self._device)),1)\n",
    "        \n",
    "        batch_size = iota_x.shape[0]\n",
    "        out_encoder = self.model().encoder(iota_x)\n",
    "        \n",
    "        q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :self.n_latent],\\\n",
    "                                                scale=torch.nn.Softplus()\\\n",
    "                                                (out_encoder[..., self.n_latent:\\\n",
    "                                                             (2*self.n_latent)])),1)\n",
    "\n",
    "        zgivenx = q_zgivenxobs.rsample([self.n_samples])\n",
    "        zgivenx_flat = zgivenx.reshape([self.n_samples*batch_size,self.n_latent])\n",
    "\n",
    "        out_decoder = self.model().decoder(zgivenx_flat)\n",
    "        all_means_obs_model = out_decoder[..., :self.n_features]\n",
    "        all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., self.n_features:\\\n",
    "                                                               (2*self.n_features)]) + 0.001\n",
    "        all_degfreedom_obs_model = torch.nn.Softplus()\\\n",
    "        (out_decoder[..., (2*self.n_features):(3*self.n_features)]) + 3\n",
    "\n",
    "        data_flat = torch.Tensor.repeat(iota_x,[self.n_samples,1]).reshape([-1,1])\n",
    "        tiledmask = torch.Tensor.repeat(mask,[self.n_samples,1])\n",
    "\n",
    "        all_log_pxgivenz_flat = torch.distributions.StudentT\\\n",
    "        (loc=all_means_obs_model.reshape([-1,1]),\\\n",
    "         scale=all_scales_obs_model.reshape([-1,1]),\\\n",
    "         df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
    "        all_log_pxgivenz = all_log_pxgivenz_flat.reshape([self.n_samples*batch_size,self.n_features])\n",
    "\n",
    "        logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([self.n_samples,batch_size])\n",
    "        logpz = self.p_z.log_prob(zgivenx)\n",
    "        logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "        neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq,0))\n",
    "\n",
    "        return neg_bound\n",
    "\n",
    "    def training_data(self,  batch_size = 48):\n",
    "        \n",
    "        df = pd.read_csv(self.dataset_path, sep=',', index_col=False)\n",
    "        x_train = df.values\n",
    "        x_mask = np.isfinite(x_train)\n",
    "        # xhat_0: missing values are replaced by zeros. \n",
    "        #This x_hat0 is what will be fed to our encoder.\n",
    "        xhat_0 = np.copy(x_train)\n",
    "        \n",
    "        # Data standardization\n",
    "        if self.standardization:\n",
    "            xhat_0 = self.standardize_data(xhat_0)\n",
    "            \n",
    "        xhat_0[np.isnan(x_train)] = 0\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        \n",
    "        data_manager = DataManager(dataset=xhat_0 , target=x_mask , **train_kwargs)\n",
    "        \n",
    "        return data_manager\n",
    "    \n",
    "    def standardize_data(self,data):\n",
    "        data_norm = np.copy(data)\n",
    "        if ((self.fed_mean is not None) and (self.fed_std is not None)):\n",
    "            print('FEDERATED STANDARDIZATION')\n",
    "            data_norm = (data_norm - self.fed_mean)/self.fed_std\n",
    "        else:\n",
    "            print('LOCAL STANDARDIZATION')\n",
    "            data_norm = (data_norm - np.nanmean(data_norm,0))/np.nanstd(data_norm,0)\n",
    "        return data_norm\n",
    "    \n",
    "    def training_step(self, data, mask):\n",
    "        self.model().encoder.zero_grad()\n",
    "        self.model().decoder.zero_grad()\n",
    "        loss = self.miwae_loss(iota_x = data,mask = mask)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side. \n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "* data `tags` to search nodes for training.\n",
    "* total number of `rounds`.\n",
    "If FedProx optimisation is requested, `fedprox_mu` parameter must be defined here. It also must be a float between XX and YY.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "h = 128 # number of hidden units in (same for all MLPs)\n",
    "d = 5 # dimension of the latent space, we choose d=1 for visualisation purposes\n",
    "K = 100 # number of IS during training\n",
    "\n",
    "n_epochs=10\n",
    "batch_size = 32\n",
    "\n",
    "model_args = {'n_features':data_size, 'n_latent':d,'n_hidden':h,'n_samples':K, 'use_gpu': True,\n",
    "             'standardization':{'fed_mean':fed_mean.tolist(),'fed_std':fed_std.tolist()}}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': batch_size, \n",
    "    'optimizer_args':\n",
    "    {'lr': 1e-3}, \n",
    "    'log_interval' : 1,\n",
    "    'epochs': n_epochs, \n",
    "    'dry_run': False,  \n",
    "    #'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "tags =  ['adni']\n",
    "rounds = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and run the experiment\n",
    "\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `round_limit` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MIWAETrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `round_limit` rounds are done for all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment with FedProx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the federated training but using FedProx as aggregation scheme (starting from the second iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "# During the first round we will simply use FedAvg \n",
    "# with standard optimization scheme: the FedProx penalization\n",
    "# term will be introduced exclusively from the second round.\n",
    "# training_args.update(fedprox_mu = 0.)\n",
    "if 'fedprox_mu' in training_args:\n",
    "    del training_args['fedprox_mu'] \n",
    "\n",
    "exp_fedprox = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MIWAETrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_fedprox.run_once()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from the second round, FedProx is used with mu=0.1\n",
    "# We first update the training args\n",
    "training_args.update(fedprox_mu = 0.1)\n",
    "\n",
    "# Then update training args in the experiment\n",
    "exp_fedprox.set_training_args(training_args)\n",
    "exp_fedprox.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment with FedProx and performing the standardization locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we propose to use FedProx aagain, but this time each client standardize his dataset using local mean and std:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "if 'fedprox_mu' in training_args:\n",
    "    del training_args['fedprox_mu'] \n",
    "\n",
    "model_args.update(standardization = {})\n",
    "\n",
    "exp_fedprox_std_local = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MIWAETrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_fedprox_std_local.run_once()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.update(fedprox_mu = 0.1)\n",
    "\n",
    "exp_fedprox_std_local.set_training_args(training_args)\n",
    "exp_fedprox_std_local.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and comparison to local training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the imputation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1000\n",
    "\n",
    "def miwae_impute(encoder,decoder,iota_x,mask,d,L):\n",
    "    \n",
    "    p_z = td.Independent(td.Normal(loc=torch.zeros(d),scale=torch.ones(d)),1)\n",
    "    \n",
    "    batch_size = iota_x.shape[0]\n",
    "    out_encoder = encoder(iota_x)\n",
    "    q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d],scale=torch.nn.Softplus()(out_encoder[..., d:(2*d)])),1)\n",
    "\n",
    "    zgivenx = q_zgivenxobs.rsample([L])\n",
    "    zgivenx_flat = zgivenx.reshape([L*batch_size,d])\n",
    "\n",
    "    out_decoder = decoder(zgivenx_flat)\n",
    "    all_means_obs_model = out_decoder[..., :p]\n",
    "    all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., p:(2*p)]) + 0.001\n",
    "    all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*p):(3*p)]) + 3\n",
    "\n",
    "    data_flat = torch.Tensor.repeat(iota_x,[L,1]).reshape([-1,1])\n",
    "    tiledmask = torch.Tensor.repeat(mask,[L,1])\n",
    "\n",
    "    all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1,1]),scale=all_scales_obs_model.reshape([-1,1]),df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
    "    all_log_pxgivenz = all_log_pxgivenz_flat.reshape([L*batch_size,p])\n",
    "\n",
    "    logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([L,batch_size])\n",
    "    logpz = p_z.log_prob(zgivenx)\n",
    "    logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "    xgivenz = td.Independent(td.StudentT(loc=all_means_obs_model, scale=all_scales_obs_model, df=all_degfreedom_obs_model),1)\n",
    "\n",
    "    imp_weights = torch.nn.functional.softmax(logpxobsgivenz + logpz - logq,0) # these are w_1,....,w_L for all observations in the batch\n",
    "    xms = xgivenz.mean.reshape([L,batch_size,p])  # that's the only line that changed!\n",
    "    xm=torch.einsum('ki,kij->ij', imp_weights, xms) \n",
    "\n",
    "    return xm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the MSE function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(xhat,xtrue,mask): # MSE function for imputations\n",
    "    xhat = np.array(xhat)\n",
    "    xtrue = np.array(xtrue)\n",
    "    return np.mean(np.power(xhat-xtrue,2)[~mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Testing on an external dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we are going to test the performance of the final federated model to impute missing data on a test dataset. To this extent we are going to remove randomly 50% of samples from the test dataset, `data_test`, defined at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = data_test_missing.shape[0] # number of observations\n",
    "p = data_test_missing.shape[1] # number of features\n",
    "\n",
    "xmiss_test = np.copy(data_test_missing)\n",
    "mask_test = np.isfinite(xmiss_test) # binary mask that indicates which values are missing\n",
    "\n",
    "# Evaluate local mean and std of test dataset\n",
    "mean_test = np.nanmean(xmiss_test,0)\n",
    "std_test = np.nanstd(xmiss_test,0)\n",
    "\n",
    "# Evaluate global mean and std including test dataset\n",
    "#N_cl_test = [fed_mean_std.aggregated_params()[0]['params']['N_tot'],\n",
    "#           torch.Tensor([xmiss_test[:,dim].size - np.count_nonzero(np.isnan(xmiss_test[:,dim]))\\\n",
    "#                                   for dim in range(p)])]\n",
    "#mean_cl_test = [fed_mean,torch.from_numpy(mean_test)]\n",
    "#std_cl_test = [fed_std,torch.from_numpy(std_test)]\n",
    "\n",
    "#cl_test = 2 # 1 global training dataset + 1 test dataset\n",
    "#N_tot_cl_test = sum([N_cl_test[c] for c in range(cl_test)])\n",
    "#Mean_cl_test = sum([N_cl_test[i]*mean_cl_test[i]/N_tot_cl_test for i in range(cl_test)])\n",
    "#Std_cl_test = torch.sqrt(sum([((N_cl_test[i]-1)*(std_cl_test[i]**2)+N_cl_test[i]*(mean_cl_test[i]**2))/(N_tot_cl_test-cl_test)\\\n",
    "#                              for i in range(cl_test)])-(N_tot_cl_test/(N_tot_cl_test-cl_test))*(Mean_cl_test**2))\n",
    "\n",
    "# standardization with respect to the fed dataset\n",
    "xmiss_test_global_std = np.copy(data_test_missing)\n",
    "xmiss_test_global_std = (xmiss_test_global_std - fed_mean.numpy())/fed_std.numpy()\n",
    "xhat_0_test_global_std = np.copy(xmiss_test_global_std)\n",
    "xhat_0_test_global_std[np.isnan(xmiss_test_global_std)] = 0\n",
    "xhat_test_global_std = np.copy(xhat_0_test_global_std) # This will be out imputed data matrix\n",
    "xfull_test_global_std = np.copy(data_test)\n",
    "xfull_test_global_std = (xfull_test_global_std - fed_mean.numpy())/fed_std.numpy()\n",
    "\n",
    "# local standardization\n",
    "xmiss_test_local_std = np.copy(data_test_missing)\n",
    "xmiss_test_local_std = (xmiss_test_local_std - mean_test)/std_test\n",
    "xhat_0_test_local_std = np.copy(xmiss_test_local_std)\n",
    "xhat_0_test_local_std[np.isnan(xmiss_test_local_std)] = 0\n",
    "xhat_test_local_std = np.copy(xhat_0_test_local_std) # This will be out imputed data matrix\n",
    "xfull_test_local_std = np.copy(data_test)\n",
    "xfull_test_local_std = (xfull_test_local_std - mean_test)/std_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the model using last updated federated parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract federated model into PyTorch framework\n",
    "model = exp.model_instance()\n",
    "model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "encoder = model.encoder\n",
    "decoder = model.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we finally do the imputation and evaluate the corresponding imputation error through MSE for each federated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat_test = np.copy(xhat_test_global_std)\n",
    "xhat_0_test = np.copy(xhat_0_test_global_std)\n",
    "xfull_test = np.copy(xfull_test_global_std)\n",
    "\n",
    "xhat_test[~mask_test] = miwae_impute(encoder = encoder,decoder = decoder,iota_x = torch.from_numpy(xhat_0_test).float(),mask = torch.from_numpy(mask_test).float(),d = d,L= L).cpu().data.numpy()[~mask_test]\n",
    "err_test_data_global_std = np.array([mse(xhat_test,xfull_test,mask_test)])\n",
    "print('Imputation MSE of fed model on testing data %g' %err_test_data_global_std)\n",
    "print('-----')\n",
    "\n",
    "xhat_test = np.copy(xhat_test_local_std)\n",
    "xhat_0_test = np.copy(xhat_0_test_local_std)\n",
    "xfull_test = np.copy(xfull_test_local_std)\n",
    "\n",
    "xhat_test[~mask_test] = miwae_impute(encoder = encoder,decoder = decoder,iota_x = torch.from_numpy(xhat_0_test).float(),mask = torch.from_numpy(mask_test).float(),d = d,L= L).cpu().data.numpy()[~mask_test]\n",
    "err_test_data_local_std = np.array([mse(xhat_test,xfull_test,mask_test)])\n",
    "print('Imputation MSE of fed model on testing data (with local standardization in testing data) %g' %err_test_data_local_std)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save the results in the ouptut file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names\n",
    "field_names = ['Split_type', 'Test_data', 'model', 'N_train_centers', 'Size', \n",
    "               'N_rounds', 'N_epochs', 'std_training', 'std_testing', 'MSE']\n",
    "\n",
    "# Dictionary\n",
    "dict_out_1={'Split_type': Split_type, 'Test_data': 'Test', 'model': 'FedAvg', \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Fed', 'std_testing': 'global', 'MSE': float(err_test_data_global_std)}\n",
    "dict_out_2={'Split_type': Split_type, 'Test_data': 'Test', 'model': 'FedAvg',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Fed', 'std_testing': 'local', 'MSE': float(err_test_data_local_std)}\n",
    "\n",
    "with open('results/output_notiid.csv', 'a') as output_file:\n",
    "    dictwriter_object = csv.DictWriter(output_file, fieldnames=field_names)\n",
    "    dictwriter_object.writerow(dict_out_1)\n",
    "    dictwriter_object.writerow(dict_out_2)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for fedprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract federated model with fedprox\n",
    "model_fedprox = exp_fedprox.model_instance()\n",
    "model_fedprox.load_state_dict(exp_fedprox.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "encoder_fedprox = model_fedprox.encoder\n",
    "decoder_fedprox = model_fedprox.decoder\n",
    "\n",
    "xhat_test = np.copy(xhat_test_global_std)\n",
    "xhat_0_test = np.copy(xhat_0_test_global_std)\n",
    "xfull_test = np.copy(xfull_test_global_std)\n",
    "\n",
    "xhat_test[~mask_test] = miwae_impute(encoder = encoder_fedprox,decoder = decoder_fedprox,iota_x = torch.from_numpy(xhat_0_test).float(),mask = torch.from_numpy(mask_test).float(),d = d,L= L).cpu().data.numpy()[~mask_test]\n",
    "err_test_data_fedprox_global_std = np.array([mse(xhat_test,xfull_test,mask_test)])\n",
    "print('Imputation MSE of fed model (with fedprox) on testing data  %g' %err_test_data_fedprox_global_std)\n",
    "print('-----')\n",
    "\n",
    "xhat_test = np.copy(xhat_test_local_std)\n",
    "xhat_0_test = np.copy(xhat_0_test_local_std)\n",
    "xfull_test = np.copy(xfull_test_local_std)\n",
    "\n",
    "xhat_test[~mask_test] = miwae_impute(encoder = encoder_fedprox,decoder = decoder_fedprox,iota_x = torch.from_numpy(xhat_0_test).float(),mask = torch.from_numpy(mask_test).float(),d = d,L= L).cpu().data.numpy()[~mask_test]\n",
    "err_test_data_fedprox_local_std = np.array([mse(xhat_test,xfull_test,mask_test)])\n",
    "print('Imputation MSE of fed model (with fedprox) on testing data (with local standardization in testing data)  %g' %err_test_data_fedprox_local_std)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names\n",
    "field_names = ['Split_type', 'Test_data', 'model', 'N_train_centers', 'Size', \n",
    "               'N_rounds', 'N_epochs', 'std_training', 'std_testing', 'MSE']\n",
    "\n",
    "# Dictionary\n",
    "dict_out_1={'Split_type': Split_type, 'Test_data': 'Test', 'model': 'FedProx',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Fed', 'std_testing': 'global', 'MSE': float(err_test_data_fedprox_global_std)}\n",
    "dict_out_2={'Split_type': Split_type, 'Test_data': 'Test', 'model': 'FedProx',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Fed', 'std_testing': 'local', 'MSE': float(err_test_data_fedprox_local_std)}\n",
    "\n",
    "with open('results/output_notiid.csv', 'a') as output_file:\n",
    "    dictwriter_object = csv.DictWriter(output_file, fieldnames=field_names)\n",
    "    dictwriter_object.writerow(dict_out_1)\n",
    "    dictwriter_object.writerow(dict_out_2)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing on the client's datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to use the final federated model to impute missing data of client 1, which have been used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first recover data (full and with missing entries) from client 1\n",
    "n = Clients_data[0].shape[0] # number of observations\n",
    "p = Clients_data[0].shape[1] # number of features\n",
    "\n",
    "xmiss_cl1 = np.copy(Clients_missing[0])\n",
    "mask_cl1 = np.isfinite(xmiss_cl1) # binary mask that indicates which values are missing\n",
    "mean_cl1 = np.nanmean(xmiss_cl1,0)\n",
    "std_cl1 = np.nanstd(xmiss_cl1,0)\n",
    "\n",
    "# standardization with respect to the whole dataset (training+test)\n",
    "xmiss_cl1_global_std = np.copy(Clients_missing[0])\n",
    "xmiss_cl1_global_std = (xmiss_cl1_global_std - fed_mean.numpy())/fed_std.numpy()\n",
    "xhat_0_cl1_global_std = np.copy(xmiss_cl1_global_std)\n",
    "xhat_0_cl1_global_std[np.isnan(xmiss_cl1_global_std)] = 0\n",
    "xhat_cl1_global_std = np.copy(xhat_0_cl1_global_std) # This will be out imputed data matrix\n",
    "xfull_cl1_global_std = np.copy(Clients_data[0])\n",
    "xfull_cl1_global_std = (xfull_cl1_global_std - fed_mean.numpy())/fed_std.numpy()\n",
    "\n",
    "# local standardization\n",
    "xmiss_cl1_local_std = np.copy(Clients_missing[0])\n",
    "xmiss_cl1_local_std = (xmiss_cl1_local_std - mean_cl1)/std_cl1\n",
    "xhat_0_cl1_local_std = np.copy(xmiss_cl1_local_std)\n",
    "xhat_0_cl1_local_std[np.isnan(xmiss_cl1_local_std)] = 0\n",
    "xhat_cl1_local_std = np.copy(xhat_0_cl1_local_std) # This will be out imputed data matrix\n",
    "xfull_cl1_local_std = np.copy(Clients_data[0])\n",
    "xfull_cl1_local_std = (xfull_cl1_local_std - mean_cl1)/std_cl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now we do the imputation\n",
    "\n",
    "xhat_cl1 = np.copy(xhat_cl1_global_std)\n",
    "xhat_0_cl1 = np.copy(xhat_0_cl1_global_std)\n",
    "xfull_cl1 = np.copy(xfull_cl1_global_std)\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder,decoder = decoder, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_cl1_data_global_std = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of fed model on data from client 1  %g' %err_cl1_data_global_std)\n",
    "print('-----')\n",
    "\n",
    "xhat_cl1 = np.copy(xhat_cl1_local_std)\n",
    "xhat_0_cl1 = np.copy(xhat_0_cl1_local_std)\n",
    "xfull_cl1 = np.copy(xfull_cl1_local_std)\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder,decoder = decoder, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_cl1_data_local_std = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of fed model on data from client 1 (with local standardization in client 1 data) %g' %err_cl1_data_local_std)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names\n",
    "field_names = ['Split_type', 'Test_data', 'model', 'N_train_centers', 'Size', \n",
    "               'N_rounds', 'N_epochs', 'std_training', 'std_testing', 'MSE']\n",
    "\n",
    "# Dictionary\n",
    "dict_out_1={'Split_type': Split_type, 'Test_data': 'Client1', 'model': 'FedAvg',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Fed', 'std_testing': 'global', 'MSE': float(err_cl1_data_global_std)}\n",
    "dict_out_2={'Split_type': Split_type, 'Test_data': 'Client1', 'model': 'FedAvg',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Fed', 'std_testing': 'local', 'MSE': float(err_cl1_data_local_std)}\n",
    "\n",
    "with open('results/output_notiid.csv', 'a') as output_file:\n",
    "    dictwriter_object = csv.DictWriter(output_file, fieldnames=field_names)\n",
    "    dictwriter_object.writerow(dict_out_1)\n",
    "    dictwriter_object.writerow(dict_out_2)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat_cl1 = np.copy(xhat_cl1_global_std)\n",
    "xhat_0_cl1 = np.copy(xhat_0_cl1_global_std)\n",
    "xfull_cl1 = np.copy(xfull_cl1_global_std)\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder_fedprox,decoder = decoder_fedprox, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_cl1_data_fedprox_global_std = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of fed model (with fedprox) on data from client 1  %g' %err_cl1_data_fedprox_global_std)\n",
    "print('-----')\n",
    "\n",
    "xhat_cl1 = np.copy(xhat_cl1_local_std)\n",
    "xhat_0_cl1 = np.copy(xhat_0_cl1_local_std)\n",
    "xfull_cl1 = np.copy(xfull_cl1_local_std)\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder_fedprox,decoder = decoder_fedprox, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_cl1_data_fedprox_local_std = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of fed model (with fedprox) on data from client 1 (with local standardization in client 1 data) %g' %err_cl1_data_fedprox_local_std)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names\n",
    "field_names = ['Split_type', 'Test_data', 'model', 'N_train_centers', 'Size', \n",
    "               'N_rounds', 'N_epochs', 'std_training', 'std_testing', 'MSE']\n",
    "\n",
    "# Dictionary\n",
    "dict_out_1={'Split_type': Split_type, 'Test_data': 'Client1', 'model': 'FedProx',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Fed', 'std_testing': 'global', 'MSE': float(err_cl1_data_fedprox_global_std)}\n",
    "dict_out_2={'Split_type': Split_type, 'Test_data': 'Client1', 'model': 'FedProx',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Fed', 'std_testing': 'local', 'MSE': float(err_cl1_data_fedprox_local_std)}\n",
    "\n",
    "with open('results/output_notiid.csv', 'a') as output_file:\n",
    "    dictwriter_object = csv.DictWriter(output_file, fieldnames=field_names)\n",
    "    dictwriter_object.writerow(dict_out_1)\n",
    "    dictwriter_object.writerow(dict_out_2)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And on client 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first recover data (full and with missing entries) from client 1\n",
    "n = Clients_data[1].shape[0] # number of observations\n",
    "p = Clients_data[1].shape[1] # number of features\n",
    "\n",
    "xmiss_cl2 = np.copy(Clients_missing[1])\n",
    "mask_cl2 = np.isfinite(xmiss_cl2) # binary mask that indicates which values are missing\n",
    "mean_cl2 = np.nanmean(xmiss_cl2,0)\n",
    "std_cl2 = np.nanstd(xmiss_cl2,0)\n",
    "\n",
    "# standardization with respect to the whole dataset (training+test)\n",
    "xmiss_cl2_global_std = np.copy(Clients_missing[1])\n",
    "xmiss_cl2_global_std = (xmiss_cl2_global_std - fed_mean.numpy())/fed_std.numpy()\n",
    "xhat_0_cl2_global_std = np.copy(xmiss_cl2_global_std)\n",
    "xhat_0_cl2_global_std[np.isnan(xmiss_cl2_global_std)] = 0\n",
    "xhat_cl2_global_std = np.copy(xhat_0_cl2_global_std) # This will be out imputed data matrix\n",
    "xfull_cl2_global_std = np.copy(Clients_data[1])\n",
    "xfull_cl2_global_std = (xfull_cl2_global_std - fed_mean.numpy())/fed_std.numpy()\n",
    "\n",
    "# local standardization\n",
    "xmiss_cl2_local_std = np.copy(Clients_missing[1])\n",
    "xmiss_cl2_local_std = (xmiss_cl2_local_std - mean_cl2)/std_cl2\n",
    "xhat_0_cl2_local_std = np.copy(xmiss_cl2_local_std)\n",
    "xhat_0_cl2_local_std[np.isnan(xmiss_cl2_local_std)] = 0\n",
    "xhat_cl2_local_std = np.copy(xhat_0_cl2_local_std) # This will be out imputed data matrix\n",
    "xfull_cl2_local_std = np.copy(Clients_data[1])\n",
    "xfull_cl2_local_std = (xfull_cl2_local_std - mean_cl2)/std_cl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat_cl2 = np.copy(xhat_cl2_global_std)\n",
    "xhat_0_cl2 = np.copy(xhat_0_cl2_global_std)\n",
    "xfull_cl2 = np.copy(xfull_cl2_global_std)\n",
    "\n",
    "xhat_cl2[~mask_cl2] = miwae_impute(encoder = encoder,decoder = decoder, iota_x = torch.from_numpy(xhat_0_cl2).float(),mask = torch.from_numpy(mask_cl2).float(),d = d,L= L).cpu().data.numpy()[~mask_cl2]\n",
    "err_cl2_data_global_std = np.array([mse(xhat_cl2,xfull_cl2,mask_cl2)])\n",
    "print('Imputation MSE of fed model on data from client 2  %g' %err_cl2_data_global_std)\n",
    "print('-----')\n",
    "\n",
    "xhat_cl2 = np.copy(xhat_cl2_local_std)\n",
    "xhat_0_cl2 = np.copy(xhat_0_cl2_local_std)\n",
    "xfull_cl2 = np.copy(xfull_cl2_local_std)\n",
    "\n",
    "xhat_cl2[~mask_cl2] = miwae_impute(encoder = encoder,decoder = decoder, iota_x = torch.from_numpy(xhat_0_cl2).float(),mask = torch.from_numpy(mask_cl2).float(),d = d,L= L).cpu().data.numpy()[~mask_cl2]\n",
    "err_cl2_data_local_std = np.array([mse(xhat_cl2,xfull_cl2,mask_cl2)])\n",
    "print('Imputation MSE of fed model on data from client 2 (with local standardization in client 2 data) %g' %err_cl2_data_local_std)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names\n",
    "field_names = ['Split_type', 'Test_data', 'model', 'N_train_centers', 'Size', \n",
    "               'N_rounds', 'N_epochs', 'std_training', 'std_testing', 'MSE']\n",
    "\n",
    "# Dictionary\n",
    "dict_out_1={'Split_type': Split_type, 'Test_data': 'Client2', 'model': 'FedAvg',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Fed', 'std_testing': 'global', 'MSE': float(err_cl2_data_global_std)}\n",
    "dict_out_2={'Split_type': Split_type, 'Test_data': 'Client2', 'model': 'FedAvg',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Fed', 'std_testing': 'local', 'MSE': float(err_cl2_data_local_std)}\n",
    "\n",
    "with open('results/output_notiid.csv', 'a') as output_file:\n",
    "    dictwriter_object = csv.DictWriter(output_file, fieldnames=field_names)\n",
    "    dictwriter_object.writerow(dict_out_1)\n",
    "    dictwriter_object.writerow(dict_out_2)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat_cl2 = np.copy(xhat_cl2_global_std)\n",
    "xhat_0_cl2 = np.copy(xhat_0_cl2_global_std)\n",
    "xfull_cl2 = np.copy(xfull_cl2_global_std)\n",
    "\n",
    "xhat_cl2[~mask_cl2] = miwae_impute(encoder = encoder_fedprox,decoder = decoder_fedprox, iota_x = torch.from_numpy(xhat_0_cl2).float(),mask = torch.from_numpy(mask_cl2).float(),d = d,L= L).cpu().data.numpy()[~mask_cl2]\n",
    "err_cl2_data_fedprox_global_std = np.array([mse(xhat_cl2,xfull_cl2,mask_cl2)])\n",
    "print('Imputation MSE of fed model (with fedprox) on data from client 2  %g' %err_cl2_data_fedprox_global_std)\n",
    "print('-----')\n",
    "\n",
    "xhat_cl2 = np.copy(xhat_cl2_local_std)\n",
    "xhat_0_cl2 = np.copy(xhat_0_cl2_local_std)\n",
    "xfull_cl2 = np.copy(xfull_cl2_local_std)\n",
    "\n",
    "xhat_cl2[~mask_cl2] = miwae_impute(encoder = encoder_fedprox,decoder = decoder_fedprox, iota_x = torch.from_numpy(xhat_0_cl2).float(),mask = torch.from_numpy(mask_cl2).float(),d = d,L= L).cpu().data.numpy()[~mask_cl2]\n",
    "err_cl2_data_fedprox_local_std = np.array([mse(xhat_cl2,xfull_cl2,mask_cl2)])\n",
    "print('Imputation MSE of fed model (with fedprox) on data from client 2 (with local standardization in client 2 data) %g' %err_cl2_data_fedprox_local_std)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names\n",
    "field_names = ['Split_type', 'Test_data', 'model', 'N_train_centers', 'Size', \n",
    "               'N_rounds', 'N_epochs', 'std_training', 'std_testing', 'MSE']\n",
    "\n",
    "# Dictionary\n",
    "dict_out_1={'Split_type': Split_type, 'Test_data': 'Client2', 'model': 'FedProx',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Fed', 'std_testing': 'global', 'MSE': float(err_cl2_data_fedprox_global_std)}\n",
    "dict_out_2={'Split_type': Split_type, 'Test_data': 'Client2', 'model': 'FedProx',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Fed', 'std_testing': 'local', 'MSE': float(err_cl2_data_fedprox_local_std)}\n",
    "\n",
    "with open('results/output_notiid.csv', 'a') as output_file:\n",
    "    dictwriter_object = csv.DictWriter(output_file, fieldnames=field_names)\n",
    "    dictwriter_object.writerow(dict_out_1)\n",
    "    dictwriter_object.writerow(dict_out_2)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing of FedProx model with local standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test the federated model with FedProx, where data standardization is performed locally. In order to be as much coherent as possible, each time the standardization will be realized locally as well in the testing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We recover the model\n",
    "model_fedprox_std_local = exp_fedprox_std_local.model_instance()\n",
    "model_fedprox_std_local.load_state_dict(exp_fedprox_std_local.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "encoder_fedprox_std_local = model_fedprox_std_local.encoder\n",
    "decoder_fedprox_std_local = model_fedprox_std_local.decoder\n",
    "\n",
    "# We do the imputation on the test data, standardized locally\n",
    "# (since information on the global mean/std are supposed not being available)\n",
    "n = data_test.shape[0] # number of observations\n",
    "p = data_test.shape[1] # number of features\n",
    "\n",
    "xhat_test = np.copy(xhat_test_local_std)\n",
    "xhat_0_test = np.copy(xhat_0_test_local_std)\n",
    "xfull_test = np.copy(xfull_test_local_std)\n",
    "\n",
    "xhat_test[~mask_test] = miwae_impute(encoder = encoder_fedprox_std_local,decoder = decoder_fedprox_std_local,iota_x = torch.from_numpy(xhat_0_test).float(),mask = torch.from_numpy(mask_test).float(),d = d,L= L).cpu().data.numpy()[~mask_test]\n",
    "err_test_data_fedprox_std_local = np.array([mse(xhat_test,xfull_test,mask_test)])\n",
    "print('Imputation MSE of fed model (with fedprox and local standardization) on testing data  %g' %err_test_data_fedprox_std_local)\n",
    "print('-----')\n",
    "\n",
    "# Same for the dataset from client 1. \n",
    "# In this case the dataset standardization is done with respect to his own data.\n",
    "n = Clients_data[0].shape[0] # number of observations\n",
    "p = Clients_data[0].shape[1] # number of features\n",
    "\n",
    "xhat_cl1 = np.copy(xhat_cl1_local_std)\n",
    "xhat_0_cl1 = np.copy(xhat_0_cl1_local_std)\n",
    "xfull_cl1 = np.copy(xfull_cl1_local_std)\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder_fedprox_std_local,decoder = decoder_fedprox_std_local, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_cl1_data_fedprox_std_local = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of fed model (with fedprox and local standardization) on data from client 1  %g' %err_cl1_data_fedprox_std_local)\n",
    "print('-----')\n",
    "\n",
    "# And the dataset from client 2. \n",
    "# In this case the dataset standardization is done with respect to his own data.\n",
    "n = Clients_data[1].shape[0] # number of observations\n",
    "p = Clients_data[1].shape[1] # number of features\n",
    "\n",
    "xhat_cl2 = np.copy(xhat_cl2_local_std)\n",
    "xhat_0_cl2 = np.copy(xhat_0_cl2_local_std)\n",
    "xfull_cl2 = np.copy(xfull_cl2_local_std)\n",
    "\n",
    "xhat_cl2[~mask_cl2] = miwae_impute(encoder = encoder_fedprox_std_local,decoder = decoder_fedprox_std_local, iota_x = torch.from_numpy(xhat_0_cl2).float(),mask = torch.from_numpy(mask_cl2).float(),d = d,L= L).cpu().data.numpy()[~mask_cl2]\n",
    "err_cl2_data_fedprox_std_local = np.array([mse(xhat_cl2,xfull_cl2,mask_cl2)])\n",
    "print('Imputation MSE of fed model (with fedprox and local standardization) on data from client 2  %g' %err_cl2_data_fedprox_std_local)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names\n",
    "field_names = ['Split_type', 'Test_data', 'model', 'N_train_centers', 'Size', \n",
    "               'N_rounds', 'N_epochs', 'std_training', 'std_testing', 'MSE']\n",
    "\n",
    "# Dictionary\n",
    "dict_out_1={'Split_type': Split_type, 'Test_data': 'Test', 'model': 'FedProx',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Loc', 'std_testing': 'local', 'MSE': float(err_test_data_fedprox_std_local)}\n",
    "dict_out_2={'Split_type': Split_type, 'Test_data': 'Client1', 'model': 'FedProx',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Loc', 'std_testing': 'local', 'MSE': float(err_cl1_data_fedprox_std_local)}\n",
    "dict_out_3={'Split_type': Split_type, 'Test_data': 'Client2', 'model': 'FedProx',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[i]) for i in range(N_cl)], \n",
    "            'N_rounds': rounds, 'N_epochs': n_epochs,\n",
    "          'std_training': 'Loc', 'std_testing': 'local', 'MSE': float(err_cl2_data_fedprox_std_local)}\n",
    "\n",
    "with open('results/output_notiid.csv', 'a') as output_file:\n",
    "    dictwriter_object = csv.DictWriter(output_file, fieldnames=field_names)\n",
    "    dictwriter_object.writerow(dict_out_1)\n",
    "    dictwriter_object.writerow(dict_out_2)\n",
    "    dictwriter_object.writerow(dict_out_3)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Local training on client 1,  and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test the performance of the same model trained locally and tested on the dataset from client 1. We will use a total of `epochs`x`rounds` local epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_z = td.Independent(td.Normal(loc=torch.zeros(d),scale=torch.ones(d)),1)\n",
    "def miwae_loss(encoder, decoder, iota_x, mask, d, p, K, batch_size):\n",
    "    \n",
    "    batch_size = iota_x.shape[0]\n",
    "    out_encoder = encoder(iota_x)\n",
    "\n",
    "    q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d],scale=torch.nn.Softplus()(out_encoder[..., d:(2*d)])),1)\n",
    "\n",
    "    zgivenx = q_zgivenxobs.rsample([K])\n",
    "    zgivenx_flat = zgivenx.reshape([K*batch_size,d])\n",
    "\n",
    "    out_decoder = decoder(zgivenx_flat)\n",
    "    all_means_obs_model = out_decoder[..., :p]\n",
    "    all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., p:(2*p)]) + 0.001\n",
    "    all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*p):(3*p)]) + 3\n",
    "\n",
    "    data_flat = torch.Tensor.repeat(iota_x,[K,1]).reshape([-1,1])\n",
    "    tiledmask = torch.Tensor.repeat(mask,[K,1])\n",
    "\n",
    "    all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1,1]),scale=all_scales_obs_model.reshape([-1,1]),df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
    "    all_log_pxgivenz = all_log_pxgivenz_flat.reshape([K*batch_size,p])\n",
    "\n",
    "    logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([K,batch_size])\n",
    "    logpz = p_z.log_prob(zgivenx)\n",
    "    logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "    neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq,0))\n",
    "\n",
    "    return neg_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the local training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall all hyperparameters\n",
    "\n",
    "n_epochs_local = n_epochs*rounds\n",
    "\n",
    "bs = training_args.get('batch_size')\n",
    "lr = training_args.get('lr')\n",
    "\n",
    "h = model_args.get('n_hidden') \n",
    "d = model_args.get('n_latent') \n",
    "K = model_args.get('n_samples') \n",
    "\n",
    "# Data\n",
    "\n",
    "n = Clients_data[0].shape[0] # number of observations\n",
    "p = Clients_data[0].shape[1] # number of features\n",
    "\n",
    "xhat_cl1 = np.copy(xhat_cl1_local_std)\n",
    "xhat_0_cl1 = np.copy(xhat_0_cl1_local_std)\n",
    "xfull_cl1 = np.copy(xfull_cl1_local_std)\n",
    "\n",
    "encoder_cl1 = nn.Sequential(\n",
    "    torch.nn.Linear(p, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 2*d),  # the encoder will output both the mean and the diagonal covariance\n",
    ")\n",
    "\n",
    "decoder_cl1 = nn.Sequential(\n",
    "    torch.nn.Linear(d, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 3*p),  # the decoder will output both the mean, the scale, and the number of degrees of freedoms (hence the 3*p)\n",
    ")\n",
    "\n",
    "optimizer_cl1 = torch.optim.Adam(list(encoder_cl1.parameters()) + list(decoder_cl1.parameters()),lr=1e-3)\n",
    "\n",
    "def weights_init(layer):\n",
    "    if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)\n",
    "        \n",
    "encoder_cl1.apply(weights_init)\n",
    "decoder_cl1.apply(weights_init)\n",
    "\n",
    "for ep in range(1,n_epochs_local):\n",
    "    perm = np.random.permutation(n) # We use the \"random reshuffling\" version of SGD\n",
    "    batches_data = np.array_split(xhat_0_cl1[perm,], n/bs)\n",
    "    batches_mask = np.array_split(mask_cl1[perm,], n/bs)\n",
    "    for it in range(len(batches_data)):\n",
    "        optimizer_cl1.zero_grad()\n",
    "        encoder_cl1.zero_grad()\n",
    "        decoder_cl1.zero_grad()\n",
    "        b_data = torch.from_numpy(batches_data[it]).float()\n",
    "        b_mask = torch.from_numpy(batches_mask[it]).float()\n",
    "        loss = miwae_loss(encoder = encoder_cl1,decoder = decoder_cl1, iota_x = b_data,mask = b_mask, d = d, p = p, K = K, batch_size = bs)\n",
    "        loss.backward()\n",
    "        optimizer_cl1.step()\n",
    "    if ep % rounds == 1:\n",
    "        print('Epoch %g' %ep)\n",
    "        print('MIWAE likelihood bound  %g' %(-np.log(K)-miwae_loss(encoder = encoder_cl1,decoder = decoder_cl1, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(), d = d, p = p, K = K, batch_size = bs).cpu().data.numpy())) # Gradient step      \n",
    "        print('Loss: {:.6f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we do the imputation on the same dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder_cl1, decoder = decoder_cl1, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_local_cl1_data = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of local model on data from same client (cl 1)  %g' %err_local_cl1_data)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = data_test.shape[0] # number of observations\n",
    "p = data_test.shape[1] # number of features\n",
    "xhat_test = np.copy(xhat_test_local_std)\n",
    "xhat_0_test = np.copy(xhat_0_test_local_std)\n",
    "xfull_test = np.copy(xfull_test_local_std)\n",
    "\n",
    "xhat_test[~mask_test] = miwae_impute(encoder = encoder_cl1,decoder = decoder_cl1,iota_x = torch.from_numpy(xhat_0_test).float(),mask = torch.from_numpy(mask_test).float(),d = d,L= L).cpu().data.numpy()[~mask_test]\n",
    "err_local_cl1_test_data = np.array([mse(xhat_test,xfull_test,mask_test)])\n",
    "print('Imputation MSE of local model on testing data %g' %err_local_cl1_test_data)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names\n",
    "field_names = ['Split_type', 'Test_data', 'model', 'N_train_centers', 'Size', \n",
    "               'N_rounds', 'N_epochs', 'std_training', 'std_testing', 'MSE']\n",
    "\n",
    "# Dictionary\n",
    "dict_out_1={'Split_type': Split_type, 'Test_data': 'Test', 'model': 'Local_cl1',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[0])], \n",
    "            'N_rounds': 1, 'N_epochs': n_epochs_local,\n",
    "          'std_training': 'Loc', 'std_testing': 'local', 'MSE': float(err_local_cl1_test_data)}\n",
    "dict_out_2={'Split_type': Split_type, 'Test_data': 'Client1', 'model': 'Local_cl1', \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[0])], \n",
    "            'N_rounds': 1, 'N_epochs': n_epochs*rounds,\n",
    "          'std_training': 'Loc', 'std_testing': 'local', 'MSE': float(err_local_cl1_data)}\n",
    "\n",
    "with open('results/output_notiid.csv', 'a') as output_file:\n",
    "    dictwriter_object = csv.DictWriter(output_file, fieldnames=field_names)\n",
    "    dictwriter_object.writerow(dict_out_1)\n",
    "    dictwriter_object.writerow(dict_out_2)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Local training on client 2,  and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "n = Clients_data[1].shape[0] # number of observations\n",
    "p = Clients_data[1].shape[1] # number of features\n",
    "\n",
    "xhat_cl2 = np.copy(xhat_cl2_local_std)\n",
    "xhat_0_cl2 = np.copy(xhat_0_cl2_local_std)\n",
    "xfull_cl2 = np.copy(xfull_cl2_local_std)\n",
    "\n",
    "encoder_cl2 = nn.Sequential(\n",
    "    torch.nn.Linear(p, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 2*d),  # the encoder will output both the mean and the diagonal covariance\n",
    ")\n",
    "\n",
    "decoder_cl2 = nn.Sequential(\n",
    "    torch.nn.Linear(d, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 3*p),  # the decoder will output both the mean, the scale, and the number of degrees of freedoms (hence the 3*p)\n",
    ")\n",
    "\n",
    "optimizer_cl2 = torch.optim.Adam(list(encoder_cl2.parameters()) + list(decoder_cl2.parameters()),lr=1e-3)\n",
    "\n",
    "def weights_init(layer):\n",
    "    if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)\n",
    "        \n",
    "encoder_cl2.apply(weights_init)\n",
    "decoder_cl2.apply(weights_init)\n",
    "\n",
    "for ep in range(1,n_epochs_local):\n",
    "    perm = np.random.permutation(n) # We use the \"random reshuffling\" version of SGD\n",
    "    batches_data = np.array_split(xhat_0_cl2[perm,], n/bs)\n",
    "    batches_mask = np.array_split(mask_cl2[perm,], n/bs)\n",
    "    for it in range(len(batches_data)):\n",
    "        optimizer_cl2.zero_grad()\n",
    "        encoder_cl2.zero_grad()\n",
    "        decoder_cl2.zero_grad()\n",
    "        b_data = torch.from_numpy(batches_data[it]).float()\n",
    "        b_mask = torch.from_numpy(batches_mask[it]).float()\n",
    "        loss = miwae_loss(encoder = encoder_cl2,decoder = decoder_cl2, iota_x = b_data,mask = b_mask, d = d, p = p, K = K, batch_size = bs)\n",
    "        loss.backward()\n",
    "        optimizer_cl2.step()\n",
    "    if ep % rounds == 1:\n",
    "        print('Epoch %g' %ep)\n",
    "        print('MIWAE likelihood bound  %g' %(-np.log(K)-miwae_loss(encoder = encoder_cl2,decoder = decoder_cl2, iota_x = torch.from_numpy(xhat_0_cl2).float(),mask = torch.from_numpy(mask_cl2).float(), d = d, p = p, K = K, batch_size = bs).cpu().data.numpy())) # Gradient step      \n",
    "        print('Loss: {:.6f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat_cl2[~mask_cl2] = miwae_impute(encoder = encoder_cl2, decoder = decoder_cl2, iota_x = torch.from_numpy(xhat_0_cl2).float(),mask = torch.from_numpy(mask_cl2).float(),d = d,L= L).cpu().data.numpy()[~mask_cl2]\n",
    "err_local_cl2_data = np.array([mse(xhat_cl2,xfull_cl2,mask_cl2)])\n",
    "print('Imputation MSE of local model on data from same client (cl 2)  %g' %err_local_cl2_data)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = data_test.shape[0] # number of observations\n",
    "p = data_test.shape[1] # number of features\n",
    "xhat_test = np.copy(xhat_test_local_std)\n",
    "xhat_0_test = np.copy(xhat_0_test_local_std)\n",
    "xfull_test = np.copy(xfull_test_local_std)\n",
    "\n",
    "xhat_test[~mask_test] = miwae_impute(encoder = encoder_cl2,decoder = decoder_cl2,iota_x = torch.from_numpy(xhat_0_test).float(),mask = torch.from_numpy(mask_test).float(),d = d,L= L).cpu().data.numpy()[~mask_test]\n",
    "err_local_cl2_test_data = np.array([mse(xhat_test,xfull_test,mask_test)])\n",
    "print('Imputation MSE of local model on testing data %g' %err_local_cl2_test_data)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names\n",
    "field_names = ['Split_type', 'Test_data', 'model', 'N_train_centers', 'Size', \n",
    "               'N_rounds', 'N_epochs', 'std_training', 'std_testing', 'MSE']\n",
    "\n",
    "# Dictionary\n",
    "dict_out_1={'Split_type': Split_type, 'Test_data': 'Test', 'model': 'Local_cl2',  \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[0])], \n",
    "            'N_rounds': 1, 'N_epochs': n_epochs_local,\n",
    "          'std_training': 'Loc', 'std_testing': 'local', 'MSE': float(err_local_cl2_test_data)}\n",
    "dict_out_2={'Split_type': Split_type, 'Test_data': 'Client2', 'model': 'Local_cl2', \n",
    "            'N_train_centers': N_cl, 'Size': [len(Clients_missing[0])], \n",
    "            'N_rounds': 1, 'N_epochs': n_epochs*rounds,\n",
    "          'std_training': 'Loc', 'std_testing': 'local', 'MSE': float(err_local_cl2_data)}\n",
    "\n",
    "with open('results/output_notiid.csv', 'a') as output_file:\n",
    "    dictwriter_object = csv.DictWriter(output_file, fieldnames=field_names)\n",
    "    dictwriter_object.writerow(dict_out_1)\n",
    "    dictwriter_object.writerow(dict_out_2)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Local training on centralized data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We centralized data from all clients, and perform the local training, hence test results on the external testing dataset as well as data from client 1, as we deed for the others models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs\n",
    "\n",
    "n_epochs_local = n_epochs*rounds*len(Clients_missing)\n",
    "\n",
    "# Data\n",
    "\n",
    "xmiss_tot = np.concatenate(Clients_missing,axis=0)\n",
    "\n",
    "n = xmiss_tot.shape[0] # number of observations\n",
    "p = xmiss_tot.shape[1] # number of features\n",
    "\n",
    "mean_tot_missing = np.nanmean(xmiss_tot,0)\n",
    "std_tot_missing = np.nanstd(xmiss_tot,0)\n",
    "xmiss_tot = (xmiss_tot - mean_tot_missing)/std_tot_missing\n",
    "mask_tot = np.isfinite(xmiss_tot) # binary mask that indicates which values are missing\n",
    "xhat_0_tot = np.copy(xmiss_tot)\n",
    "xhat_0_tot[np.isnan(xmiss_tot)] = 0\n",
    "xhat_tot = np.copy(xhat_0_tot) # This will be out imputed data matrix\n",
    "\n",
    "xfull_tot = np.concatenate(Clients_data,axis=0)\n",
    "xfull_tot = (xfull_tot - mean_tot_missing)/std_tot_missing\n",
    "\n",
    "# Model\n",
    "\n",
    "encoder_tot = nn.Sequential(\n",
    "    torch.nn.Linear(p, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 2*d),  # the encoder will output both the mean and the diagonal covariance\n",
    ")\n",
    "\n",
    "decoder_tot = nn.Sequential(\n",
    "    torch.nn.Linear(d, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 3*p),  # the decoder will output both the mean, the scale, and the number of degrees of freedoms (hence the 3*p)\n",
    ")\n",
    "\n",
    "optimizer_tot = torch.optim.Adam(list(encoder_tot.parameters()) + list(decoder_tot.parameters()),lr=1e-3)\n",
    "\n",
    "def weights_init(layer):\n",
    "    if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)\n",
    "        \n",
    "encoder_tot.apply(weights_init)\n",
    "decoder_tot.apply(weights_init)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for ep in range(1,n_epochs_local):\n",
    "    perm = np.random.permutation(n) # We use the \"random reshuffling\" version of SGD\n",
    "    batches_data = np.array_split(xhat_0_tot[perm,], n/bs)\n",
    "    batches_mask = np.array_split(mask_tot[perm,], n/bs)\n",
    "    for it in range(len(batches_data)):\n",
    "        optimizer_tot.zero_grad()\n",
    "        encoder_tot.zero_grad()\n",
    "        decoder_tot.zero_grad()\n",
    "        b_data = torch.from_numpy(batches_data[it]).float()\n",
    "        b_mask = torch.from_numpy(batches_mask[it]).float()\n",
    "        loss = miwae_loss(encoder = encoder_tot,decoder = decoder_tot, iota_x = b_data,mask = b_mask, d = d, p = p, K = K, batch_size = bs)\n",
    "        loss.backward()\n",
    "        optimizer_tot.step()\n",
    "    if ep % rounds == 1:\n",
    "        print('Epoch %g' %ep)\n",
    "        print('MIWAE likelihood bound  %g' %(-np.log(K)-miwae_loss(encoder = encoder_tot,decoder = decoder_tot, iota_x = torch.from_numpy(xhat_0_tot).float(),mask = torch.from_numpy(mask_tot).float(), d = d, p = p, K = K, batch_size = bs).cpu().data.numpy())) # Gradient step      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Clients_data[0].shape[0] # number of observations\n",
    "p = Clients_data[0].shape[1] # number of features\n",
    "\n",
    "xhat_cl1 = np.copy(xhat_cl1_global_std)\n",
    "xhat_0_cl1 = np.copy(xhat_0_cl1_global_std)\n",
    "xfull_cl1 = np.copy(xfull_cl1_global_std)\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder_tot, decoder = decoder_tot, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_local_tot_cl1_data_global_std = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of local model on the whole dataset, on data from same client 1  %g' %err_local_tot_cl1_data_global_std)\n",
    "print('-----')\n",
    "\n",
    "xhat_cl1 = np.copy(xhat_cl1_local_std)\n",
    "xhat_0_cl1 = np.copy(xhat_0_cl1_local_std)\n",
    "xfull_cl1 = np.copy(xfull_cl1_local_std)\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder_tot, decoder = decoder_tot, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_local_tot_cl1_data_local_std = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of local model on the whole dataset, on data from same client 1 (with local standardization in client 1 data)  %g' %err_local_tot_cl1_data_local_std)\n",
    "print('-----')\n",
    "\n",
    "n = Clients_data[1].shape[0] # number of observations\n",
    "p = Clients_data[1].shape[1] # number of features\n",
    "\n",
    "xhat_cl2 = np.copy(xhat_cl2_global_std)\n",
    "xhat_0_cl2 = np.copy(xhat_0_cl2_global_std)\n",
    "xfull_cl2 = np.copy(xfull_cl2_global_std)\n",
    "\n",
    "xhat_cl2[~mask_cl2] = miwae_impute(encoder = encoder_tot, decoder = decoder_tot, iota_x = torch.from_numpy(xhat_0_cl2).float(),mask = torch.from_numpy(mask_cl2).float(),d = d,L= L).cpu().data.numpy()[~mask_cl2]\n",
    "err_local_tot_cl2_data_global_std = np.array([mse(xhat_cl2,xfull_cl2,mask_cl2)])\n",
    "print('Imputation MSE of local model on the whole dataset, on data from same client 2  %g' %err_local_tot_cl2_data_global_std)\n",
    "print('-----')\n",
    "\n",
    "xhat_cl2 = np.copy(xhat_cl2_local_std)\n",
    "xhat_0_cl2 = np.copy(xhat_0_cl2_local_std)\n",
    "xfull_cl2 = np.copy(xfull_cl2_local_std)\n",
    "\n",
    "xhat_cl2[~mask_cl2] = miwae_impute(encoder = encoder_tot, decoder = decoder_tot, iota_x = torch.from_numpy(xhat_0_cl2).float(),mask = torch.from_numpy(mask_cl2).float(),d = d,L= L).cpu().data.numpy()[~mask_cl2]\n",
    "err_local_tot_cl2_data_local_std = np.array([mse(xhat_cl2,xfull_cl2,mask_cl2)])\n",
    "print('Imputation MSE of local model on the whole dataset, on data from same client 2 (with local standardization in client 2 data)  %g' %err_local_tot_cl2_data_local_std)\n",
    "print('-----')\n",
    "\n",
    "n = data_test.shape[0] # number of observations\n",
    "p = data_test.shape[1] # number of features\n",
    "\n",
    "xhat_test = np.copy(xhat_test_global_std)\n",
    "xhat_0_test = np.copy(xhat_0_test_global_std)\n",
    "xfull_test = np.copy(xfull_test_global_std)\n",
    "\n",
    "xhat_test[~mask_test] = miwae_impute(encoder = encoder_tot,decoder = decoder_tot,iota_x = torch.from_numpy(xhat_0_test).float(),mask = torch.from_numpy(mask_test).float(),d = d,L= L).cpu().data.numpy()[~mask_test]\n",
    "err_local_tot_test_data_global_std = np.array([mse(xhat_test,xfull_test,mask_test)])\n",
    "print('Imputation MSE of local model on the whole dataset, on testing data %g' %err_local_tot_test_data_global_std)\n",
    "print('-----')\n",
    "\n",
    "xhat_test = np.copy(xhat_test_local_std)\n",
    "xhat_0_test = np.copy(xhat_0_test_local_std)\n",
    "xfull_test = np.copy(xfull_test_local_std)\n",
    "\n",
    "xhat_test[~mask_test] = miwae_impute(encoder = encoder_tot,decoder = decoder_tot,iota_x = torch.from_numpy(xhat_0_test).float(),mask = torch.from_numpy(mask_test).float(),d = d,L= L).cpu().data.numpy()[~mask_test]\n",
    "err_local_tot_test_data_local_std = np.array([mse(xhat_test,xfull_test,mask_test)])\n",
    "print('Imputation MSE of local model on the whole dataset, on testing data (with local standardization in testing data) %g' %err_local_tot_test_data_local_std)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names\n",
    "field_names = ['Split_type', 'Test_data', 'model', 'N_train_centers', 'Size', \n",
    "               'N_rounds', 'N_epochs', 'std_training', 'std_testing', 'MSE']\n",
    "\n",
    "# Dictionary\n",
    "dict_out_1={'Split_type': Split_type, 'Test_data': 'Test', 'model': 'Centralized', \n",
    "            'N_train_centers': N_cl, 'Size': [len(xmiss_tot)], \n",
    "            'N_rounds': 1, 'N_epochs': n_epochs_local,\n",
    "          'std_training': 'Loc', 'std_testing': 'local', 'MSE': float(err_local_tot_test_data_local_std)}\n",
    "dict_out_2={'Split_type': Split_type, 'Test_data': 'Client1', 'model': 'Centralized', \n",
    "            'N_train_centers': N_cl, 'Size': [len(xmiss_tot)], \n",
    "            'N_rounds': 1, 'N_epochs': n_epochs_local,\n",
    "          'std_training': 'Loc', 'std_testing': 'local', 'MSE': float(err_local_tot_cl1_data_local_std)}\n",
    "dict_out_3={'Split_type': Split_type, 'Test_data': 'Test', 'model': 'Centralized', \n",
    "            'N_train_centers': N_cl, 'Size': [len(xmiss_tot)], \n",
    "            'N_rounds': 1, 'N_epochs': n_epochs_local,\n",
    "          'std_training': 'Loc', 'std_testing': 'global', 'MSE': float(err_local_tot_test_data_global_std)}\n",
    "dict_out_4={'Split_type': Split_type, 'Test_data': 'Client1', 'model': 'Centralized', \n",
    "            'N_train_centers': N_cl, 'Size': [len(xmiss_tot)], \n",
    "            'N_rounds': 1, 'N_epochs': n_epochs_local,\n",
    "          'std_training': 'Loc', 'std_testing': 'global', 'MSE': float(err_local_tot_cl1_data_global_std)}\n",
    "dict_out_5={'Split_type': Split_type, 'Test_data': 'Client2', 'model': 'Centralized', \n",
    "            'N_train_centers': N_cl, 'Size': [len(xmiss_tot)], \n",
    "            'N_rounds': 1, 'N_epochs': n_epochs_local,\n",
    "          'std_training': 'Loc', 'std_testing': 'local', 'MSE': float(err_local_tot_cl2_data_local_std)}\n",
    "\n",
    "dict_out_6={'Split_type': Split_type, 'Test_data': 'Client2', 'model': 'Centralized', \n",
    "            'N_train_centers': N_cl, 'Size': [len(xmiss_tot)], \n",
    "            'N_rounds': 1, 'N_epochs': n_epochs_local,\n",
    "          'std_training': 'Loc', 'std_testing': 'global', 'MSE': float(err_local_tot_cl2_data_global_std)}\n",
    "\n",
    "\n",
    "\n",
    "with open('results/output_notiid.csv', 'a') as output_file:\n",
    "    dictwriter_object = csv.DictWriter(output_file, fieldnames=field_names)\n",
    "    dictwriter_object.writerow(dict_out_1)\n",
    "    dictwriter_object.writerow(dict_out_2)\n",
    "    dictwriter_object.writerow(dict_out_3)\n",
    "    dictwriter_object.writerow(dict_out_4)\n",
    "    dictwriter_object.writerow(dict_out_5)\n",
    "    dictwriter_object.writerow(dict_out_6)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary of obtained results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print('Imputation MSE on testing data')\n",
    "print('-----')\n",
    "data = [['FedAvg, global std', err_test_data_global_std],\n",
    "['FedAvg, local std', err_test_data_local_std],\n",
    "['FedProx, global std', err_test_data_fedprox_global_std],\n",
    "['FedProx, local std', err_test_data_fedprox_local_std], \n",
    "['FedLocStd, local std', err_test_data_fedprox_std_local],  \n",
    "['Local (cl1), local std', err_local_cl1_test_data], \n",
    "['Local (cl2), local std', err_local_cl2_test_data],  \n",
    "['Centralized, global std', err_local_tot_test_data_global_std],\n",
    "['Centralized, local std', err_local_tot_test_data_local_std]]\n",
    "print (tabulate(data, headers=[\"Model\", \"Mean Squared Error (\\u2193)\"]))\n",
    "print('-----')\n",
    "print('-----')\n",
    "print('Imputation MSE on local data from client 1')\n",
    "print('-----')\n",
    "data = [['FedAvg, global std', err_cl1_data_global_std], \n",
    "['FedAvg, local std', err_cl1_data_local_std],  \n",
    "['FedProx, global std', err_cl1_data_fedprox_global_std],\n",
    "['FedProx, local std', err_cl1_data_fedprox_local_std],\n",
    "['FedLocStd, local std', err_cl1_data_fedprox_std_local], \n",
    "['Local (cl1), local std', err_local_cl1_data], \n",
    "['Centralized, global std', err_local_tot_cl1_data_global_std],\n",
    "['Centralized, local std', err_local_tot_cl1_data_local_std]]\n",
    "print (tabulate(data, headers=[\"Model\", \"Mean Squared Error (\\u2193)\"]))\n",
    "print('-----')\n",
    "print('-----')\n",
    "print('Imputation MSE on local data from client 2')\n",
    "print('-----')\n",
    "data = [['FedAvg, global std', err_cl2_data_global_std], \n",
    "['FedAvg, local std', err_cl2_data_local_std], \n",
    "['FedProx, global std', err_cl2_data_fedprox_global_std],\n",
    "['FedProx, local std', err_cl2_data_fedprox_local_std],\n",
    "['FedLocStd, local std', err_cl2_data_fedprox_std_local], \n",
    "['Local (cl2), local std', err_local_cl2_data],\n",
    "['Centralized, global std', err_local_tot_cl2_data_global_std],\n",
    "['Centralized, local std', err_local_tot_cl2_data_local_std]]\n",
    "print (tabulate(data, headers=[\"Model\", \"Mean Squared Error (\\u2193)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
