{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing :  Used Cars Dataset \n",
    "\n",
    "\n",
    "Link for the download of Used cars dataset: https://www.kaggle.com/adityadesai13/used-car-dataset-ford-and-mercedes\n",
    "and extract the content into the `fedbiomed/notebooks/data` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change here with the directory where you downloaded the dataset\n",
    "data_dir = './data/used_cars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Details\n",
    "The data consists of used cars listings. 100,000 listings, which have been separated into files corresponding to each car manufacturer. Each file will simulate data for each node.\n",
    "\n",
    "# Goal\n",
    "\n",
    "The goal of this tutorial is to build a federated regression model on Non-IID dataset and generate the best model by performing validation on hold out dataset and tuning hyperparameters.The metric used to decide best model is RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use audi and bmw for training on 2 nodes\n",
    "audi = pd.read_csv(os.path.join(data_dir, \"audi.csv\"))\n",
    "bmw = pd.read_csv(os.path.join(data_dir, \"bmw.csv\"))\n",
    "\n",
    "# Use Ford for final validation at central researcher (test dataset)\n",
    "ford = pd.read_csv(os.path.join(data_dir, \"ford.csv\"))\n",
    "\n",
    "# Use the following csvs if you want to run more than 2 nodes. Uncomment Corresponding lines in the following cell blocks\n",
    "# cclass = pd.read_csv(os.path.join(data_dir, \"cclass.csv\"))\n",
    "# focus = pd.read_csv(os.path.join(data_dir, \"focus.csv\"))\n",
    "# hyundai = pd.read_csv(os.path.join(\"data_dir, \"huyndai.csv\"))\n",
    "# merc = pd.read_csv(os.path.join(data_dir, \"merc.csv\"))\n",
    "# skoda = pd.read_csv(os.path.join(data_dir, \"skoda.csv\"))\n",
    "# toyota = pd.read_csv(os.path.join(data_dir, \"toyota.csv\"))\n",
    "# vauxhall = pd.read_csv(os.path.join(data_dir, \"vauxhall.csv\"))\n",
    "# vw = pd.read_csv(os.path.join(data_dir, \"vw.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns model & fuelType as labels are not consistent across files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audi.drop(columns = ['model','fuelType'],inplace = True)\n",
    "bmw.drop(columns = ['model','fuelType'],inplace = True)\n",
    "ford.drop(columns = ['model','fuelType'],inplace = True)\n",
    "\n",
    "# cclass.drop(columns = ['model','fuelType'],inplace = True)\n",
    "# focus.drop(columns = ['model','fuelType'],inplace = True)\n",
    "# hyundai.drop(columns = ['model','fuelType'],inplace = True)\n",
    "# merc.drop(columns = ['model','fuelType'],inplace = True)\n",
    "# skoda.drop(columns = ['model','fuelType'],inplace = True)\n",
    "# toyata.drop(columns = ['model','fuelType'],inplace = True)\n",
    "# vauxhall.drop(columns = ['model','fuelType'],inplace = True)\n",
    "# vw.drop(columns = ['model','fuelType'],inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encode transmission column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audi['transmission'] = audi['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "bmw['transmission'] = bmw['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "ford['transmission'] = ford['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "\n",
    "# cclass['transmission'] = cclass['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "# focus['transmission'] = focus['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "# hyundai['transmission'] = hyundai['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "# merc['transmission'] = merc['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "# skoda['transmission'] = skoda['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "# toyata['transmission'] = toyata['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "# vauxhall['transmission'] = vauxhall['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "# vw['transmission'] = vw['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audi.to_csv(os.path.join(data_dir, 'audi_transformed.csv'),header = True,index= False)\n",
    "bmw.to_csv(os.path.join(data_dir, 'bmw_transformed.csv'),header = True,index= False)\n",
    "ford.to_csv(os.path.join(data_dir, 'ford_transformed.csv'),header = True,index= False)\n",
    "\n",
    "# cclass.to_csv('cclass_transformed.csv',header = True,index= False)\n",
    "# focus.to_csv('focus_transformed.csv',header = True,index= False)\n",
    "# hyundai.to_csv('huydai_transformed.csv',header = True,index= False)\n",
    "# merc.to_csv('merc_transformed.csv',header = True,index= False)\n",
    "# skoda.to_csv('skoda_transformed.csv',header = True,index= False)\n",
    "# toyata.to_csv('toyata_transformed.csv',header = True,index= False)\n",
    "# vauxhall.to_csv('vaxhall_transformed.csv',header = True,index= False)\n",
    "# vw.to_csv('vw_transformed.csv',header = True,index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed-BioMed Researcher to train a model on a Used Cars dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the network\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the nodes up\n",
    "It is necessary to previously configure 2 nodes:\n",
    "1. `./scripts/fedbiomed_run node config config1.ini add` (node1) and `./scripts/fedbiomed_run node config config2.ini add` (node2)\n",
    "  * Select option 1 to add a csv file to the node\n",
    "    * use the `audi_transformed.csv` file (node 1) and `bmw_transformed.csv` file (node 2)\n",
    "  * Choose the name, tags and description of the dataset\n",
    "    * choose tag `UsedCars` (or modify the used tag in this notebook)\n",
    "  * Spin as many nodes as you want(max nodes 11 for 11 csv files in used cars dataset). Hold out one file for testing.\n",
    "  * Load the .csv file generated using above mentioned notebook to individual nodes\n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node config config1.ini list` (node1) and `./scripts/fedbiomed_run node config config2.ini list` (node2)\n",
    "3. Run the node using `./scripts/fedbiomed_run node config config1.ini start` (node1) and `./scripts/fedbiomed_run node config config2.ini start` (node2). Wait until you get `Starting task manager`. it means you are online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment to train a model on the data found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a torch training plan MyTrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager \n",
    "\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'MyTrainingPlan')\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    \n",
    "    # Model\n",
    "    def init_model(self):\n",
    "        model_args = self.model_args()\n",
    "        model = self.Net(model_args)\n",
    "        return model\n",
    "        \n",
    "    # Dependencies\n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torch.utils.data import Dataset\",\n",
    "                \"import pandas as pd\"]\n",
    "        return deps\n",
    "    \n",
    "    # network\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, model_args):\n",
    "            super().__init__()\n",
    "            self.in_features = model_args['in_features']\n",
    "            self.out_features = model_args['out_features']\n",
    "            self.fc1 = nn.Linear(self.in_features, 5)\n",
    "            self.fc2 = nn.Linear(5, self.out_features)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data).float()\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss   = torch.sqrt(criterion(output, target.unsqueeze(1)))\n",
    "        return loss\n",
    "\n",
    "    class csv_Dataset(Dataset):\n",
    "    # Here we define a custom Dataset class inherited from the general torch Dataset class\n",
    "    # This class takes as argument a .csv file path and creates a torch Dataset \n",
    "        def __init__(self, dataset_path, x_dim):\n",
    "            self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)\n",
    "            x_train = self.input_file.loc[:,('year','transmission','mileage','tax','mpg','engineSize')].values\n",
    "            y_train = self.input_file.loc[:,'price'].values\n",
    "            self.X_train = torch.from_numpy(x_train).float()\n",
    "            self.Y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "        def __len__(self):            \n",
    "            return len(self.Y_train)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            return (self.X_train[idx], self.Y_train[idx])\n",
    "        \n",
    "    def training_data(self):\n",
    "    # The training_data creates the Dataloader to be used for training in the general class TorchTrainingPlan of fedbiomed\n",
    "        dataset = self.csv_Dataset(self.dataset_path, self.model_args()[\"in_features\"])\n",
    "        train_kwargs = {'shuffle': True}\n",
    "        return DataManager(dataset=dataset , **train_kwargs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model parameters \n",
    "model_args = {\n",
    "    'in_features': 6, \n",
    "    'out_features': 1\n",
    "}\n",
    "\n",
    "# training parameters \n",
    "training_args = {\n",
    "    'loader_args': { 'batch_size': 40, }, \n",
    "    'optimizer_args': {\n",
    "          'lr': 1e-3\n",
    "    },\n",
    "    'epochs': 2, \n",
    "    'dry_run': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an experiment\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `round_limit` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "# Calling the training data with specified tags. Change the following tag accordingly\n",
    "tags =  ['UsedCars']\n",
    "rounds = 3\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 model_args=model_args,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `round_limit` rounds are done for all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local training results for each round and each node are available via `exp.training_replies()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the training results for the last round below.\n",
    "\n",
    "Different timings (in seconds) are reported for each dataset of a node participating in a round :\n",
    "- `rtime_training` real time (clock time) spent in the training function on the node\n",
    "- `ptime_training` process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total` real time (clock time) spent in the researcher between sending the request and handling the response, at the `Job()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())\n",
    "\n",
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "round_data = exp.training_replies()[rounds - 1].data()\n",
    "for c in range(len(round_data)):\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n",
    "        rtraining = round_data[c]['timing']['rtime_training'],\n",
    "        ptraining = round_data[c]['timing']['ptime_training'],\n",
    "        rtotal = round_data[c]['timing']['rtime_total']))\n",
    "print('\\n')\n",
    "    \n",
    "exp.training_replies()[rounds - 1].dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated parameters for each round are available via `exp.aggregated_params()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the federated parameters for the last round of the experiment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_model = exp.training_plan().model()\n",
    "fed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " fed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Hold one file for testing the fed model\n",
    "test_dataset_path = os.path.join(data_dir, \"ford_transformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "def cal_rmse(actual, prediction):\n",
    "    return ((actual- prediction)**2).mean()**0.5\n",
    "\n",
    "def testing_rmse(model, data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    device = 'cpu'\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            preds.append(output.numpy().flatten())\n",
    "    rmse = cal_rmse(data_loader.dataset.Y_train.numpy(),np.hstack(preds))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class csv_Dataset(Dataset):\n",
    "        def __init__(self, dataset_path):\n",
    "            self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)\n",
    "            x_train = self.input_file.loc[:,('year','transmission','mileage','tax','mpg','engineSize')].values\n",
    "            y_train = self.input_file.loc[:,'price'].values\n",
    "            self.X_train = torch.from_numpy(x_train).float()\n",
    "            self.Y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "        def __len__(self):            \n",
    "            return len(self.Y_train)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            return (self.X_train[idx], self.Y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = csv_Dataset(test_dataset_path)\n",
    "train_kwargs = { 'shuffle': True}\n",
    "data_loader = DataLoader(dataset, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = testing_rmse(fed_model, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e416aa0399b226346633f35c0f9bb77d7e7cf1619eb46cae5c1dd017cab61cfc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
