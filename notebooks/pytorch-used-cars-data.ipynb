{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing : Download Used Cars Dataset \n",
    "https://www.kaggle.com/adityadesai13/used-car-dataset-ford-and-mercedes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change here with the directory where you downloaded the dataset\n",
    "data_dir = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Details\n",
    "The data consists of used cars listings. 100,000 listings, which have been separated into files corresponding to each car manufacturer. Each file will simulate data for each node.\n",
    "\n",
    "# Goal\n",
    "\n",
    "The goal of this tutorial is to build a federated regression model on Non-IID dataset and generate the best model by performing validation on hold out dataset and tuning hyperparameters.The metric used to decide best model is RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use audi and bmw for training on 2 nodes\n",
    "audi = pd.read_csv(os.path.join(data_dir, \"audi.csv\"))\n",
    "bmw = pd.read_csv(os.path.join(data_dir, \"bmw.csv\"))\n",
    "\n",
    "# Use Ford for final validation at central researcher (test dataset)\n",
    "ford = pd.read_csv(os.path.join(data_dir, \"ford.csv\"))\n",
    "\n",
    "# Use the following csvs if you want to run more than 2 nodes. Uncomment Corresponding lines in the following cell blocks\n",
    "# cclass = pd.read_csv(os.path.join(data_dir, \"cclass.csv\"))\n",
    "# focus = pd.read_csv(os.path.join(data_dir, \"focus.csv\"))\n",
    "# hyundai = pd.read_csv(os.path.join(\"data_dir, \"huyndai.csv\"))\n",
    "# merc = pd.read_csv(os.path.join(data_dir, \"merc.csv\"))\n",
    "# skoda = pd.read_csv(os.path.join(data_dir, \"skoda.csv\"))\n",
    "# toyota = pd.read_csv(os.path.join(data_dir, \"toyota.csv\"))\n",
    "# vauxhall = pd.read_csv(os.path.join(data_dir, \"vauxhall.csv\"))\n",
    "# vw = pd.read_csv(os.path.join(data_dir, \"vw.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns model & fuelType as labels are not consistent across files. A better solution could be vertical federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audi.drop(columns = ['model','fuelType'],inplace = True)\n",
    "bmw.drop(columns = ['model','fuelType'],inplace = True)\n",
    "ford.drop(columns = ['model','fuelType'],inplace = True)\n",
    "\n",
    "# cclass.drop(columns = ['model','fuelType'],inplace = True)\n",
    "# focus.drop(columns = ['model','fuelType'],inplace = True)\n",
    "# hyundai.drop(columns = ['model','fuelType'],inplace = True)\n",
    "# merc.drop(columns = ['model','fuelType'],inplace = True)\n",
    "# skoda.drop(columns = ['model','fuelType'],inplace = True)\n",
    "# toyata.drop(columns = ['model','fuelType'],inplace = True)\n",
    "# vauxhall.drop(columns = ['model','fuelType'],inplace = True)\n",
    "# vw.drop(columns = ['model','fuelType'],inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encode transmission column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audi['transmission'] = audi['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "bmw['transmission'] = bmw['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "ford['transmission'] = ford['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "\n",
    "# cclass['transmission'] = cclass['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "# focus['transmission'] = focus['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "# hyundai['transmission'] = hyundai['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "# merc['transmission'] = merc['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "# skoda['transmission'] = skoda['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "# toyata['transmission'] = toyata['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "# vauxhall['transmission'] = vauxhall['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "# vw['transmission'] = vw['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audi.to_csv(os.path.join(data_dir, 'audi_transformed.csv'),header = True,index= False)\n",
    "bmw.to_csv(os.path.join(data_dir, 'bmw_transformed.csv'),header = True,index= False)\n",
    "ford.to_csv(os.path.join(data_dir, 'ford_transformed.csv'),header = True,index= False)\n",
    "\n",
    "# cclass.to_csv('cclass_transformed.csv',header = True,index= False)\n",
    "# focus.to_csv('focus_transformed.csv',header = True,index= False)\n",
    "# hyundai.to_csv('huydai_transformed.csv',header = True,index= False)\n",
    "# merc.to_csv('merc_transformed.csv',header = True,index= False)\n",
    "# skoda.to_csv('skoda_transformed.csv',header = True,index= False)\n",
    "# toyata.to_csv('toyata_transformed.csv',header = True,index= False)\n",
    "# vauxhall.to_csv('vaxhall_transformed.csv',header = True,index= False)\n",
    "# vw.to_csv('vw_transformed.csv',header = True,index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fedbiomed Researcher to train a model on a Used Cars dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use for developing (autoreloads changes made across packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the network\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the nodes up\n",
    "It is necessary to previously configure 2 nodes:\n",
    "1. `./scripts/fedbiomed_run node config config1.ini add` (node1) and `./scripts/fedbiomed_run node config config2.ini add` (node2)\n",
    "  * Select option 1 to add a csv file to the node\n",
    "    * use the `audi_transformed.csv` file (node 1) and `bmw_transformed.csv` file (node 2)\n",
    "  * Choose the name, tags and description of the dataset\n",
    "    * choose tag `UsedCars` (or modify the used tag in this notebook)\n",
    "  * Spin as many nodes as you want(max nodes 11 for 11 csv files in used cars dataset). Hold out one file for testing.\n",
    "  * Load the .csv file generated using above mentioned notebook to individual nodes\n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node config config1.ini list` (node1) and `./scripts/fedbiomed_run node config config2.ini list` (node2)\n",
    "3. Run the node using `./scripts/fedbiomed_run node config config1.ini start` (node1) and `./scripts/fedbiomed_run node config config2.ini start` (node2). Wait until you get `Starting task manager`. it means you are online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment to train a model on the data found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a torch.nn MyTrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import environ\n",
    "import tempfile\n",
    "tmp_dir_model = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+'/')\n",
    "model_file = tmp_dir_model.name + '/class_export_csv.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : write **only** the code to export in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"$model_file\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.torchnn import TorchTrainingPlan\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'MyTrainingPlan')\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self, model_args: dict = {}):\n",
    "        super(MyTrainingPlan, self).__init__(model_args)\n",
    "        # should match the model arguments passed below to the experiment class\n",
    "        self.in_features = model_args['in_features']\n",
    "        self.out_features = model_args['out_features']\n",
    "        self.fc1 = nn.Linear(self.in_features, 5)\n",
    "        self.fc2 = nn.Linear(5, self.out_features)\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        # In this case, we need the torch Dataset and DataLoader classes\n",
    "        # We need pandas to read the local .csv file at the node side\n",
    "        deps = [\"from torch.utils.data import Dataset, DataLoader\",\n",
    "                \"import pandas as pd\"]\n",
    "        self.add_dependency(deps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, data, target):\n",
    "        output = self.forward(data).float()\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss   = torch.sqrt(criterion(output, target.unsqueeze(1)))\n",
    "        return loss\n",
    "\n",
    "    class csv_Dataset(Dataset):\n",
    "    # Here we define a custom Dataset class inherited from the general torch Dataset class\n",
    "    # This class takes as argument a .csv file path and creates a torch Dataset \n",
    "        def __init__(self, dataset_path, x_dim):\n",
    "            self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)\n",
    "            x_train = self.input_file.loc[:,('year','transmission','mileage','tax','mpg','engineSize')].values\n",
    "            y_train = self.input_file.loc[:,'price'].values\n",
    "            self.X_train = torch.from_numpy(x_train).float()\n",
    "            self.Y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "        def __len__(self):            \n",
    "            return len(self.Y_train)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            return (self.X_train[idx], self.Y_train[idx])\n",
    "        \n",
    "    def training_data(self,  batch_size = 48):\n",
    "    # The training_data creates the Dataloader to be used for training in the general class TorchTrainingPlan of fedbiomed\n",
    "        dataset = self.csv_Dataset(self.dataset_path, self.in_features)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        data_loader = DataLoader(dataset, **train_kwargs)\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model parameters \n",
    "model_args = {\n",
    "    'in_features': 6, \n",
    "    'out_features': 1\n",
    "}\n",
    "\n",
    "# training parameters \n",
    "training_args = {\n",
    "    'batch_size': 40, \n",
    "    'lr': 1e-3, \n",
    "    'epochs': 2, \n",
    "    'dry_run': False,  \n",
    "    #'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an experiment\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `rounds` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "# Calling the training data with specified tags. Change the following tag accordingly\n",
    "tags =  ['UsedCars']\n",
    "rounds = 3\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 #nodes=None,\n",
    "                 model_path=model_file,\n",
    "                 model_class='MyTrainingPlan',\n",
    "                 model_args=model_args,\n",
    "                 training_args=training_args,\n",
    "                 rounds=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `rounds` are done for all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local training results for each round and each node are available in `exp.training_replies` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the training results for the last round below.\n",
    "\n",
    "Different timings (in seconds) are reported for each dataset of a node participating in a round :\n",
    "- `rtime_training` real time (clock time) spent in the training function on the node\n",
    "- `ptime_training` process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total` real time (clock time) spent in the researcher between sending the request and handling the response, at the `Job()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies.keys())\n",
    "\n",
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "round_data = exp.training_replies[rounds - 1].data()\n",
    "for c in range(len(round_data)):\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n",
    "        rtraining = round_data[c]['timing']['rtime_training'],\n",
    "        ptraining = round_data[c]['timing']['ptime_training'],\n",
    "        rtotal = round_data[c]['timing']['rtime_total']))\n",
    "print('\\n')\n",
    "    ()\n",
    "exp.training_replies[rounds - 1].dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated parameters for each round are available in `exp.aggregated_params` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the federated parameters for the last round of the experiment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params.keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params[rounds - 1]['params'].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_model = exp.model_instance\n",
    "fed_model.load_state_dict(exp.aggregated_params[rounds - 1]['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " fed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold one file for testing the fed model\n",
    "test_dataset_path = os.path.join(data_dir, \"ford_transformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "def cal_rmse(actual, prediction):\n",
    "    return ((actual- prediction)**2).mean()**0.5\n",
    "\n",
    "def testing_rmse(model, data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    device = 'cpu'\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            preds.append(output.numpy().flatten())\n",
    "    rmse = cal_rmse(data_loader.dataset.Y_train.numpy(),np.hstack(preds))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class csv_Dataset(Dataset):\n",
    "        def __init__(self, dataset_path):\n",
    "            self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)\n",
    "            x_train = self.input_file.loc[:,('year','transmission','mileage','tax','mpg','engineSize')].values\n",
    "            y_train = self.input_file.loc[:,'price'].values\n",
    "            self.X_train = torch.from_numpy(x_train).float()\n",
    "            self.Y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "        def __len__(self):            \n",
    "            return len(self.Y_train)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            return (self.X_train[idx], self.Y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = csv_Dataset(test_dataset_path)\n",
    "train_kwargs = {'batch_size': 64, 'shuffle': True}\n",
    "data_loader = DataLoader(dataset, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = testing_rmse(fed_model, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e416aa0399b226346633f35c0f9bb77d7e7cf1619eb46cae5c1dd017cab61cfc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
