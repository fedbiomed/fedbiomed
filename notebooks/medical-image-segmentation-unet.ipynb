{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3852a3c8-eeb1-4799-9e5c-6269ced855d3",
   "metadata": {},
   "source": [
    "# Brain Segmentation\n",
    "\n",
    "This tutorial will show how to use Fed-BioMed to perform image segmentation on 3D medical MRI images of brains, using the publicly available [IXI dataset](https://brain-development.org/ixi-dataset/). \n",
    "It uses a [3D U-Net](https://link.springer.com/chapter/10.1007/978-3-319-46723-8_49) model for the segmentation, trained on data from 3 separate centers. \n",
    "\n",
    "Here we display a very complex case, using advanced Fed-BioMed functionalities such as:\n",
    "- loading a MedicalImageDataset\n",
    "- implementing a custom Node Selection Strategy\n",
    "- setting a non-default Optimizer\n",
    "- monitoring training loss with Tensorboard\n",
    "\n",
    "This tutorial is based on [TorchIO's tutorial](https://colab.research.google.com/github/fepegar/torchio-notebooks/blob/main/notebooks/TorchIO_tutorial.ipynb#scrollTo=OoHXr1a4_9Ll)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd7d60-f737-4ddd-b179-85922e9d0371",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "### Donwload  the data\n",
    "Download the IXI dataset from the [Mendeley archive](https://data.mendeley.com/datasets/7kd5wj7v7p). \n",
    "Then separate your data in different folders for each center, in order to achieve the following structure:\n",
    "```\n",
    "_ Guys\n",
    " |_ train\n",
    " | |_ IXI002-Guys-0828\n",
    " | |_ ...\n",
    " | |_ participants.csv\n",
    " |_ holdout\n",
    "    |_ IXI022-Guys-0701\n",
    "     |_ ...\n",
    "     |_ participants.csv\n",
    "_ HH ....\n",
    "```\n",
    "The following python code achieves the intended result. \n",
    "```python\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_path = '<path to your dataset root folder>'\n",
    "src_folder = os.path.join(base_path, '7kd5wj7v7p-3', 'IXI_sample')\n",
    "dst_folder_base = os.path.join(base_path, 'UniCancer-Centers')\n",
    "\n",
    "allcenters = pd.read_csv('<path to the global participants.csv file>')\n",
    "\n",
    "center_names = ['Guys', 'HH', 'IOP']\n",
    "center_dfs = list()\n",
    "\n",
    "for center_name in center_names:\n",
    "    df = allcenters[allcenters.SITE_NAME == center_name]\n",
    "    center_dfs.append(df)\n",
    "\n",
    "    train, test = train_test_split(df, test_size=0.1)\n",
    "\n",
    "    train_folder = os.path.join(dst_folder_base, center_name, 'train')\n",
    "    holdout_folder = os.path.join(dst_folder_base, center_name, 'holdout')\n",
    "    if not os.path.exists(train_folder):\n",
    "        os.makedirs(train_folder)\n",
    "    if not os.path.exists(holdout_folder):\n",
    "        os.makedirs(holdout_folder)\n",
    "\n",
    "    for subject_folder in train.FOLDER_NAME.values:\n",
    "        shutil.copytree(\n",
    "          src=os.path.join(src_folder, subject_folder),\n",
    "          dst=os.path.join(train_folder, subject_folder),\n",
    "          dirs_exist_ok=True\n",
    "       )\n",
    "\n",
    "    train.to_csv(os.path.join(train_folder, 'participants.csv'))\n",
    "\n",
    "    for subject_folder in test.FOLDER_NAME.values:\n",
    "        shutil.copytree(\n",
    "          src=os.path.join(src_folder, subject_folder),\n",
    "          dst=os.path.join(holdout_folder, subject_folder),\n",
    "          dirs_exist_ok=True\n",
    "       )\n",
    "    test.to_csv(os.path.join(holdout_folder, 'participants.csv'))\n",
    "```\n",
    "\n",
    "### Add the dataset to a different node for each center\n",
    "\n",
    "```bash\n",
    "$ ./scripts/fedbiomed_run node config guys.ini add\n",
    "```\n",
    "\n",
    "Then follow the instructions to add a bids dataset.\n",
    "Repeat for the `hh` and `iop` center, taking care to change the `config guys.ini` parameter accordingly \n",
    "\n",
    "### Launch each node\n",
    "\n",
    "```bash\n",
    "$ ./scripts/fedbiomed_run node config guys.ini start\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a477236a-1a6c-4c37-83c3-3bb3d99fcd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb9e4e-0f4c-430d-8887-f98343f652c1",
   "metadata": {},
   "source": [
    "## Define a new Strategy\n",
    "\n",
    "Fed-BioMed's default strategy reads the number of samples per node through the `shape` parameter that is computed when the data is uploaded. \n",
    "For technical reasons, we need to change this to account for the fact that different modalities may have been used during the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751760e6-f191-499f-99a6-255df4735c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n",
    "\n",
    "class BIDSStrategy(DefaultStrategy):\n",
    "    def __init__(self, data, modalities = ['T1']):\n",
    "        super().__init__(data)\n",
    "        self._modalities = modalities\n",
    "        \n",
    "    def refine(self, training_replies, round_i):\n",
    "        models_params = []\n",
    "        weights = []\n",
    "\n",
    "        # check that all nodes answered\n",
    "        cl_answered = [val['node_id'] for val in training_replies.data()]\n",
    "\n",
    "        answers_count = 0\n",
    "        for cl in self.sample_nodes(round_i):\n",
    "            if cl in cl_answered:\n",
    "                answers_count += 1\n",
    "            else:\n",
    "                # this node did not answer\n",
    "                logger.error(f'{ErrorNumbers.FB408.value} (node = {cl})')\n",
    "\n",
    "        if len(self.sample_nodes(round_i)) != answers_count:\n",
    "            if answers_count == 0:\n",
    "                # none of the nodes answered\n",
    "                msg = ErrorNumbers.FB407.value\n",
    "\n",
    "            else:\n",
    "                msg = ErrorNumbers.FB408.value\n",
    "\n",
    "            logger.critical(msg)\n",
    "            raise FedbiomedStrategyError(msg)\n",
    "\n",
    "        # check that all nodes that answer could successfully train\n",
    "        self._success_node_history[round_i] = []\n",
    "        all_success = True\n",
    "        for tr in training_replies:\n",
    "            if tr['success'] is True:\n",
    "                model_params = tr['params']\n",
    "                models_params.append(model_params)\n",
    "                self._success_node_history[round_i].append(tr['node_id'])\n",
    "            else:\n",
    "                # node did not succeed\n",
    "                all_success = False\n",
    "                logger.error(f'{ErrorNumbers.FB409.value} (node = {tr[\"node_id\"]})')\n",
    "\n",
    "        if not all_success:\n",
    "            raise FedbiomedStrategyError(ErrorNumbers.FB402.value)\n",
    "\n",
    "        # so far, everything is OK\n",
    "        shapes = [sum(val[0][\"shape\"][modality][0] for modality in self._modalities) for (key, val) in self._fds.data().items()]\n",
    "        totalrows = sum(shapes)\n",
    "        weights = [x / totalrows for x in shapes]\n",
    "        logger.info('Nodes that successfully reply in round ' +\n",
    "                    str(round_i) + ' ' +\n",
    "                    str(self._success_node_history[round_i]))\n",
    "        return models_params, weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb49d3-00f7-4c75-b8ac-5588410fb351",
   "metadata": {},
   "source": [
    "## Create a Training Plan\n",
    "\n",
    "We create a training plan that incorporates the UNet model. We choose to write the code for the UNet model instead of using the [unet](https://github.com/fepegar/unet) to avoid forcing the \n",
    "\n",
    "\n",
    "\n",
    "The code for UNet is taken from \n",
    "_Pérez-García, Fernando. (2020). fepegar/unet: PyTorch implementation of 2D and 3D U-Net (v0.7.5). Zenodo. https://doi.org/10.5281/zenodo.3697931 _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcaa6eb-28e2-4260-9698-2966fdf0d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.logger import logger\n",
    "from fedbiomed.common.data import DataManager, BIDSDataset\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "class UNetTrainingPlan(TorchTrainingPlan):\n",
    "    #### Conv ####\n",
    "    class ConvolutionalBlock(nn.Module):\n",
    "        def __init__(\n",
    "                self,\n",
    "                dimensions: int,\n",
    "                in_channels: int,\n",
    "                out_channels: int,\n",
    "                normalization = None,\n",
    "                kernel_size: int = 3,\n",
    "                activation = 'ReLU',\n",
    "                preactivation: bool = False,\n",
    "                padding: int = 0,\n",
    "                padding_mode: str = 'zeros',\n",
    "                dilation = None,\n",
    "                dropout: float = 0,\n",
    "                ):\n",
    "            super().__init__()\n",
    "\n",
    "            block = nn.ModuleList()\n",
    "\n",
    "            dilation = 1 if dilation is None else dilation\n",
    "            if padding:\n",
    "                total_padding = kernel_size + 2 * (dilation - 1) - 1\n",
    "                padding = total_padding // 2\n",
    "\n",
    "            class_name = 'Conv{}d'.format(dimensions)\n",
    "            conv_class = getattr(nn, class_name)\n",
    "            no_bias = not preactivation and (normalization is not None)\n",
    "            conv_layer = conv_class(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                padding=padding,\n",
    "                padding_mode=padding_mode,\n",
    "                dilation=dilation,\n",
    "                bias=not no_bias,\n",
    "            )\n",
    "\n",
    "            norm_layer = None\n",
    "            if normalization is not None:\n",
    "                class_name = '{}Norm{}d'.format(\n",
    "                    normalization.capitalize(), dimensions)\n",
    "                norm_class = getattr(nn, class_name)\n",
    "                num_features = in_channels if preactivation else out_channels\n",
    "                norm_layer = norm_class(num_features)\n",
    "\n",
    "            activation_layer = None\n",
    "            if activation is not None:\n",
    "                activation_layer = getattr(nn, activation)()\n",
    "\n",
    "            if preactivation:\n",
    "                self.add_if_not_none(block, norm_layer)\n",
    "                self.add_if_not_none(block, activation_layer)\n",
    "                self.add_if_not_none(block, conv_layer)\n",
    "            else:\n",
    "                self.add_if_not_none(block, conv_layer)\n",
    "                self.add_if_not_none(block, norm_layer)\n",
    "                self.add_if_not_none(block, activation_layer)\n",
    "\n",
    "            dropout_layer = None\n",
    "            if dropout:\n",
    "                class_name = 'Dropout{}d'.format(dimensions)\n",
    "                dropout_class = getattr(nn, class_name)\n",
    "                dropout_layer = dropout_class(p=dropout)\n",
    "                self.add_if_not_none(block, dropout_layer)\n",
    "\n",
    "            self.conv_layer = conv_layer\n",
    "            self.norm_layer = norm_layer\n",
    "            self.activation_layer = activation_layer\n",
    "            self.dropout_layer = dropout_layer\n",
    "\n",
    "            self.block = nn.Sequential(*block)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.block(x)\n",
    "\n",
    "        @staticmethod\n",
    "        def add_if_not_none(module_list, module):\n",
    "            if module is not None:\n",
    "                module_list.append(module)\n",
    "\n",
    "    #### Decoding ####\n",
    "\n",
    "    CHANNELS_DIMENSION = 1\n",
    "    UPSAMPLING_MODES = (\n",
    "        'nearest',\n",
    "        'linear',\n",
    "        'bilinear',\n",
    "        'bicubic',\n",
    "        'trilinear',\n",
    "    )\n",
    "\n",
    "\n",
    "    class Decoder(nn.Module):\n",
    "        def __init__(\n",
    "                self,\n",
    "                in_channels_skip_connection: int,\n",
    "                dimensions: int,\n",
    "                upsampling_type: str,\n",
    "                num_decoding_blocks: int,\n",
    "                normalization,\n",
    "                preactivation: bool = False,\n",
    "                residual: bool = False,\n",
    "                padding: int = 0,\n",
    "                padding_mode: str = 'zeros',\n",
    "                activation = 'ReLU',\n",
    "                initial_dilation = None,\n",
    "                dropout: float = 0,\n",
    "                ):\n",
    "            super().__init__()\n",
    "            upsampling_type = UNetTrainingPlan.fix_upsampling_type(upsampling_type, dimensions)\n",
    "            self.decoding_blocks = nn.ModuleList()\n",
    "            self.dilation = initial_dilation\n",
    "            for _ in range(num_decoding_blocks):\n",
    "                decoding_block = UNetTrainingPlan.DecodingBlock(\n",
    "                    in_channels_skip_connection,\n",
    "                    dimensions,\n",
    "                    upsampling_type,\n",
    "                    normalization=normalization,\n",
    "                    preactivation=preactivation,\n",
    "                    residual=residual,\n",
    "                    padding=padding,\n",
    "                    padding_mode=padding_mode,\n",
    "                    activation=activation,\n",
    "                    dilation=self.dilation,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "                self.decoding_blocks.append(decoding_block)\n",
    "                in_channels_skip_connection //= 2\n",
    "                if self.dilation is not None:\n",
    "                    self.dilation //= 2\n",
    "\n",
    "        def forward(self, skip_connections, x):\n",
    "            zipped = zip(reversed(skip_connections), self.decoding_blocks)\n",
    "            for skip_connection, decoding_block in zipped:\n",
    "                x = decoding_block(skip_connection, x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    class DecodingBlock(nn.Module):\n",
    "        def __init__(\n",
    "                self,\n",
    "                in_channels_skip_connection: int,\n",
    "                dimensions: int,\n",
    "                upsampling_type: str,\n",
    "                normalization,\n",
    "                preactivation: bool = True,\n",
    "                residual: bool = False,\n",
    "                padding: int = 0,\n",
    "                padding_mode: str = 'zeros',\n",
    "                activation = 'ReLU',\n",
    "                dilation = None,\n",
    "                dropout: float = 0,\n",
    "                ):\n",
    "            super().__init__()\n",
    "\n",
    "            self.residual = residual\n",
    "\n",
    "            if upsampling_type == 'conv':\n",
    "                in_channels = out_channels = 2 * in_channels_skip_connection\n",
    "                self.upsample = UNetTrainingPlan.get_conv_transpose_layer(\n",
    "                    dimensions, in_channels, out_channels)\n",
    "            else:\n",
    "                self.upsample = UNetTrainingPlan.get_upsampling_layer(upsampling_type)\n",
    "            in_channels_first = in_channels_skip_connection * (1 + 2)\n",
    "            out_channels = in_channels_skip_connection\n",
    "            self.conv1 = UNetTrainingPlan.ConvolutionalBlock(\n",
    "                dimensions,\n",
    "                in_channels_first,\n",
    "                out_channels,\n",
    "                normalization=normalization,\n",
    "                preactivation=preactivation,\n",
    "                padding=padding,\n",
    "                padding_mode=padding_mode,\n",
    "                activation=activation,\n",
    "                dilation=dilation,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            in_channels_second = out_channels\n",
    "            self.conv2 = UNetTrainingPlan.ConvolutionalBlock(\n",
    "                dimensions,\n",
    "                in_channels_second,\n",
    "                out_channels,\n",
    "                normalization=normalization,\n",
    "                preactivation=preactivation,\n",
    "                padding=padding,\n",
    "                padding_mode=padding_mode,\n",
    "                activation=activation,\n",
    "                dilation=dilation,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "\n",
    "            if residual:\n",
    "                self.conv_residual = UNetTrainingPlan.ConvolutionalBlock(\n",
    "                    dimensions,\n",
    "                    in_channels_first,\n",
    "                    out_channels,\n",
    "                    kernel_size=1,\n",
    "                    normalization=None,\n",
    "                    activation=None,\n",
    "                )\n",
    "\n",
    "        def forward(self, skip_connection, x):\n",
    "            x = self.upsample(x)\n",
    "            skip_connection = self.center_crop(skip_connection, x)\n",
    "            x = torch.cat((skip_connection, x), \n",
    "                          dim=UNetTrainingPlan.CHANNELS_DIMENSION)\n",
    "            if self.residual:\n",
    "                connection = self.conv_residual(x)\n",
    "                x = self.conv1(x)\n",
    "                x = self.conv2(x)\n",
    "                x += connection\n",
    "            else:\n",
    "                x = self.conv1(x)\n",
    "                x = self.conv2(x)\n",
    "            return x\n",
    "\n",
    "        def center_crop(self, skip_connection, x):\n",
    "            skip_shape = torch.tensor(skip_connection.shape)\n",
    "            x_shape = torch.tensor(x.shape)\n",
    "            crop = skip_shape[2:] - x_shape[2:]\n",
    "            half_crop = crop // 2\n",
    "            # If skip_connection is 10, 20, 30 and x is (6, 14, 12)\n",
    "            # Then pad will be (-2, -2, -3, -3, -9, -9)\n",
    "            pad = -torch.stack((half_crop, half_crop)).t().flatten()\n",
    "            skip_connection = F.pad(skip_connection, pad.tolist())\n",
    "            return skip_connection\n",
    "\n",
    "    @staticmethod\n",
    "    def get_upsampling_layer(upsampling_type: str) -> nn.Upsample:\n",
    "        if upsampling_type not in UNetTrainingPlan.UPSAMPLING_MODES:\n",
    "            message = (\n",
    "                'Upsampling type is \"{}\"'\n",
    "                ' but should be one of the following: {}'\n",
    "            )\n",
    "            message = message.format(upsampling_type, UPSAMPLING_MODES)\n",
    "            raise ValueError(message)\n",
    "        upsample = nn.Upsample(\n",
    "            scale_factor=2,\n",
    "            mode=upsampling_type,\n",
    "            align_corners=False,\n",
    "        )\n",
    "        return upsample\n",
    "\n",
    "    @staticmethod\n",
    "    def get_conv_transpose_layer(dimensions, in_channels, out_channels):\n",
    "        class_name = 'ConvTranspose{}d'.format(dimensions)\n",
    "        conv_class = getattr(nn, class_name)\n",
    "        conv_layer = conv_class(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        return conv_layer\n",
    "\n",
    "    @staticmethod\n",
    "    def fix_upsampling_type(upsampling_type: str, dimensions: int):\n",
    "        if upsampling_type == 'linear':\n",
    "            if dimensions == 2:\n",
    "                upsampling_type = 'bilinear'\n",
    "            elif dimensions == 3:\n",
    "                upsampling_type = 'trilinear'\n",
    "        return upsampling_type\n",
    "\n",
    "    #### Encoding ####\n",
    "\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(\n",
    "                self,\n",
    "                in_channels: int,\n",
    "                out_channels_first: int,\n",
    "                dimensions: int,\n",
    "                pooling_type: str,\n",
    "                num_encoding_blocks: int,\n",
    "                normalization,\n",
    "                preactivation: bool = False,\n",
    "                residual: bool = False,\n",
    "                padding: int = 0,\n",
    "                padding_mode: str = 'zeros',\n",
    "                activation = 'ReLU',\n",
    "                initial_dilation = None,\n",
    "                dropout: float = 0,\n",
    "                ):\n",
    "            super().__init__()\n",
    "\n",
    "            self.encoding_blocks = nn.ModuleList()\n",
    "            self.dilation = initial_dilation\n",
    "            is_first_block = True\n",
    "            for _ in range(num_encoding_blocks):\n",
    "                encoding_block = UNetTrainingPlan.EncodingBlock(\n",
    "                    in_channels,\n",
    "                    out_channels_first,\n",
    "                    dimensions,\n",
    "                    normalization,\n",
    "                    pooling_type,\n",
    "                    preactivation,\n",
    "                    is_first_block=is_first_block,\n",
    "                    residual=residual,\n",
    "                    padding=padding,\n",
    "                    padding_mode=padding_mode,\n",
    "                    activation=activation,\n",
    "                    dilation=self.dilation,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "                is_first_block = False\n",
    "                self.encoding_blocks.append(encoding_block)\n",
    "                if dimensions == 2:\n",
    "                    in_channels = out_channels_first\n",
    "                    out_channels_first = in_channels * 2\n",
    "                elif dimensions == 3:\n",
    "                    in_channels = 2 * out_channels_first\n",
    "                    out_channels_first = in_channels\n",
    "                if self.dilation is not None:\n",
    "                    self.dilation *= 2\n",
    "\n",
    "        def forward(self, x):\n",
    "            skip_connections = []\n",
    "            for encoding_block in self.encoding_blocks:\n",
    "                x, skip_connnection = encoding_block(x)\n",
    "                skip_connections.append(skip_connnection)\n",
    "            return skip_connections, x\n",
    "\n",
    "        @property\n",
    "        def out_channels(self):\n",
    "            return self.encoding_blocks[-1].out_channels\n",
    "\n",
    "\n",
    "    class EncodingBlock(nn.Module):\n",
    "        def __init__(\n",
    "                self,\n",
    "                in_channels: int,\n",
    "                out_channels_first: int,\n",
    "                dimensions: int,\n",
    "                normalization,\n",
    "                pooling_type,\n",
    "                preactivation: bool = False,\n",
    "                is_first_block: bool = False,\n",
    "                residual: bool = False,\n",
    "                padding: int = 0,\n",
    "                padding_mode: str = 'zeros',\n",
    "                activation = 'ReLU',\n",
    "                dilation= None,\n",
    "                dropout: float = 0,\n",
    "                ):\n",
    "            super().__init__()\n",
    "\n",
    "            self.preactivation = preactivation\n",
    "            self.normalization = normalization\n",
    "\n",
    "            self.residual = residual\n",
    "\n",
    "            if is_first_block:\n",
    "                normalization = None\n",
    "                preactivation = None\n",
    "            else:\n",
    "                normalization = self.normalization\n",
    "                preactivation = self.preactivation\n",
    "\n",
    "            self.conv1 = UNetTrainingPlan.ConvolutionalBlock(\n",
    "                dimensions,\n",
    "                in_channels,\n",
    "                out_channels_first,\n",
    "                normalization=normalization,\n",
    "                preactivation=preactivation,\n",
    "                padding=padding,\n",
    "                padding_mode=padding_mode,\n",
    "                activation=activation,\n",
    "                dilation=dilation,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "\n",
    "            if dimensions == 2:\n",
    "                out_channels_second = out_channels_first\n",
    "            elif dimensions == 3:\n",
    "                out_channels_second = 2 * out_channels_first\n",
    "            self.conv2 = UNetTrainingPlan.ConvolutionalBlock(\n",
    "                dimensions,\n",
    "                out_channels_first,\n",
    "                out_channels_second,\n",
    "                normalization=self.normalization,\n",
    "                preactivation=self.preactivation,\n",
    "                padding=padding,\n",
    "                activation=activation,\n",
    "                dilation=dilation,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "\n",
    "            if residual:\n",
    "                self.conv_residual = UNetTrainingPlan.ConvolutionalBlock(\n",
    "                    dimensions,\n",
    "                    in_channels,\n",
    "                    out_channels_second,\n",
    "                    kernel_size=1,\n",
    "                    normalization=None,\n",
    "                    activation=None,\n",
    "                )\n",
    "\n",
    "            self.downsample = None\n",
    "            if pooling_type is not None:\n",
    "                self.downsample = UNetTrainingPlan.get_downsampling_layer(dimensions, pooling_type)\n",
    "\n",
    "        def forward(self, x):\n",
    "            if self.residual:\n",
    "                connection = self.conv_residual(x)\n",
    "                x = self.conv1(x)\n",
    "                x = self.conv2(x)\n",
    "                x += connection\n",
    "            else:\n",
    "                x = self.conv1(x)\n",
    "                x = self.conv2(x)\n",
    "            if self.downsample is None:\n",
    "                return x\n",
    "            else:\n",
    "                skip_connection = x\n",
    "                x = self.downsample(x)\n",
    "                return x, skip_connection\n",
    "\n",
    "        @property\n",
    "        def out_channels(self):\n",
    "            return self.conv2.conv_layer.out_channels\n",
    "\n",
    "    @staticmethod\n",
    "    def get_downsampling_layer(\n",
    "            dimensions: int,\n",
    "            pooling_type: str,\n",
    "            kernel_size: int = 2,\n",
    "            ) -> nn.Module:\n",
    "        class_name = '{}Pool{}d'.format(pooling_type.capitalize(), dimensions)\n",
    "        class_ = getattr(nn, class_name)\n",
    "        return class_(kernel_size)\n",
    "\n",
    "    \n",
    "    # Init of UNetTrainingPlan\n",
    "    def __init__(self, model_args: dict):\n",
    "        super().__init__()\n",
    "        self.CHANNELS_DIMENSION = 1\n",
    "        \n",
    "        in_channels = model_args.get('in_channels',1)\n",
    "        out_classes = model_args.get('out_classes',2)\n",
    "        dimensions = model_args.get('dimensions',2)\n",
    "        num_encoding_blocks = model_args.get('num_encoding_blocks',5)\n",
    "        out_channels_first_layer = model_args.get('out_channels_first_layer',64)\n",
    "        normalization = model_args.get('normalization', None)\n",
    "        pooling_type = model_args.get('pooling_type', 'max')\n",
    "        upsampling_type = model_args.get('upsampling_type','conv')\n",
    "        preactivation = model_args.get('preactivation',False)\n",
    "        residual = model_args.get('residual',False)\n",
    "        padding = model_args.get('padding',0)\n",
    "        padding_mode = model_args.get('padding_mode','zeros')\n",
    "        activation = model_args.get('activation','ReLU')\n",
    "        initial_dilation = model_args.get('initial_dilation',None)\n",
    "        dropout = model_args.get('dropout',0)\n",
    "        monte_carlo_dropout = model_args.get('monte_carlo_dropout',0)\n",
    "        \n",
    "        depth = num_encoding_blocks - 1\n",
    "        \n",
    "        # Force padding if residual blocks\n",
    "        if residual:\n",
    "            padding = 1\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = UNetTrainingPlan.Encoder(\n",
    "            in_channels,\n",
    "            out_channels_first_layer,\n",
    "            dimensions,\n",
    "            pooling_type,\n",
    "            depth,\n",
    "            normalization,\n",
    "            preactivation=preactivation,\n",
    "            residual=residual,\n",
    "            padding=padding,\n",
    "            padding_mode=padding_mode,\n",
    "            activation=activation,\n",
    "            initial_dilation=initial_dilation,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Bottom (last encoding block)\n",
    "        in_channels = self.encoder.out_channels\n",
    "        if dimensions == 2:\n",
    "            out_channels_first = 2 * in_channels\n",
    "        else:\n",
    "            out_channels_first = in_channels\n",
    "\n",
    "        self.bottom_block = UNetTrainingPlan.EncodingBlock(\n",
    "            in_channels,\n",
    "            out_channels_first,\n",
    "            dimensions,\n",
    "            normalization,\n",
    "            pooling_type=None,\n",
    "            preactivation=preactivation,\n",
    "            residual=residual,\n",
    "            padding=padding,\n",
    "            padding_mode=padding_mode,\n",
    "            activation=activation,\n",
    "            dilation=self.encoder.dilation,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        if dimensions == 2:\n",
    "            power = depth - 1\n",
    "        elif dimensions == 3:\n",
    "            power = depth\n",
    "        in_channels = self.bottom_block.out_channels\n",
    "        in_channels_skip_connection = out_channels_first_layer * 2**power\n",
    "        num_decoding_blocks = depth\n",
    "        self.decoder = UNetTrainingPlan.Decoder(\n",
    "            in_channels_skip_connection,\n",
    "            dimensions,\n",
    "            upsampling_type,\n",
    "            num_decoding_blocks,\n",
    "            normalization=normalization,\n",
    "            preactivation=preactivation,\n",
    "            residual=residual,\n",
    "            padding=padding,\n",
    "            padding_mode=padding_mode,\n",
    "            activation=activation,\n",
    "            initial_dilation=self.encoder.dilation,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Monte Carlo dropout\n",
    "        self.monte_carlo_layer = None\n",
    "        if monte_carlo_dropout:\n",
    "            dropout_class = getattr(nn, 'Dropout{}d'.format(dimensions))\n",
    "            self.monte_carlo_layer = dropout_class(p=monte_carlo_dropout)\n",
    "\n",
    "        # Classifier\n",
    "        if dimensions == 2:\n",
    "            in_channels = out_channels_first_layer\n",
    "        elif dimensions == 3:\n",
    "            in_channels = 2 * out_channels_first_layer\n",
    "        self.classifier = UNetTrainingPlan.ConvolutionalBlock(\n",
    "            dimensions, in_channels, out_classes,\n",
    "            kernel_size=1, activation=None,\n",
    "        )\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        deps = [\"from monai.transforms import (Compose, NormalizeIntensity, AddChannel, Resize, AsDiscrete)\",\n",
    "               \"import torch.nn as nn\",\n",
    "               \"from fedbiomed.common.data import BIDSDataset\",\n",
    "               'import numpy as np',\n",
    "               'from torch.optim import AdamW']\n",
    "        self.add_dependency(deps)\n",
    "        \n",
    "        self.optimizer = AdamW(self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections, encoding = self.encoder(x)\n",
    "        encoding = self.bottom_block(encoding)\n",
    "        x = self.decoder(skip_connections, encoding)\n",
    "        if self.monte_carlo_layer is not None:\n",
    "            x = self.monte_carlo_layer(x)\n",
    "        x = self.classifier(x)\n",
    "        x = F.softmax(x, dim=self.CHANNELS_DIMENSION)\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_dice_loss(output, target, epsilon=1e-9):\n",
    "        CHANNELS_DIMENSION = 1\n",
    "        SPATIAL_DIMENSIONS = 2, 3, 4\n",
    "        p0 = output\n",
    "        g0 = target\n",
    "        p1 = 1 - p0\n",
    "        g1 = 1 - g0\n",
    "        tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
    "        fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n",
    "        fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
    "        num = 2 * tp\n",
    "        denom = 2 * tp + fp + fn + epsilon\n",
    "        dice_score = num / denom\n",
    "        return 1. - dice_score\n",
    "    \n",
    "    @staticmethod\n",
    "    def demographics_transform(demographics):\n",
    "        \"\"\"Transforms dict of demographics into data type for ML.\n",
    "        \n",
    "        Must return either a torch Tensor or something Tensor-like\n",
    "        that can be easily converted through the torch.as_tensor()\n",
    "        function.\"\"\"\n",
    "        \n",
    "        # simple example: keep only some keys\n",
    "        keys_to_keep = ['HEIGHT', 'WEIGHT']\n",
    "        out = np.array([float(val) for key, val in demographics.items() if key in keys_to_keep])\n",
    "        \n",
    "        # more complex: generate dummy variables for site name\n",
    "        # not ideal as it requires knowing the site names in advance\n",
    "        # could be better implemented with some preprocess\n",
    "        site_names = ['Guys', 'IOP', 'HH']\n",
    "        len_dummy_vars = len(site_names) + 1\n",
    "        dummy_vars = np.zeros(shape=(len_dummy_vars,))\n",
    "        site_name = demographics['SITE_NAME']\n",
    "        if site_name in site_names:\n",
    "            site_idx = site_names.index(site_name)\n",
    "        else:\n",
    "            site_idx = len_dummy_vars - 1\n",
    "        dummy_vars[site_idx] = 1.\n",
    "        \n",
    "        return np.concatenate((out, dummy_vars))\n",
    "        \n",
    "\n",
    "    def training_data(self,  batch_size = 4):\n",
    "    # The training_data creates the Dataloader to be used for training in the general class Torchnn of fedbiomed\n",
    "        common_shape = (48, 60, 48)\n",
    "        training_transform = Compose([AddChannel(), Resize(common_shape), NormalizeIntensity(),])\n",
    "        target_transform = Compose([AddChannel(), Resize(common_shape), AsDiscrete(to_onehot=2)])\n",
    "        \n",
    "        dataset = BIDSDataset(\n",
    "            root=self.dataset_path,\n",
    "            data_modalities='T1',\n",
    "            target_modalities='label',\n",
    "            transform=training_transform,\n",
    "            target_transform=target_transform,\n",
    "            demographics_transform=UNetTrainingPlan.demographics_transform)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset, **train_kwargs)\n",
    "    \n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        #this function must return the loss to backward it \n",
    "        img = data[0]['T1']\n",
    "        demographics = data[1]\n",
    "        output = self.forward(img)\n",
    "        loss = UNetTrainingPlan.get_dice_loss(output, target['label'])\n",
    "        avg_loss = loss.mean()\n",
    "        return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a593b-c0aa-4813-bb3f-7dfbe04007c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    # Model wants to use GPU if available on node and proposed by node (default: False)\n",
    "    #'use_gpu': True\n",
    "    'in_channels': 1,\n",
    "    'out_classes': 2,\n",
    "    'dimensions': 3,\n",
    "    'num_encoding_blocks': 3,\n",
    "    'out_channels_first_layer': 8,\n",
    "    'normalization': 'batch',\n",
    "    'upsampling_type': 'linear',\n",
    "    'padding': True,\n",
    "    'activation': 'PReLU',\n",
    "}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 16, \n",
    "    'lr': 0.001, \n",
    "    'epochs': 5, \n",
    "    'dry_run': False,  \n",
    "    #'batch_maxnum': 2 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16725efa-5498-4c4b-9055-559fb8c1a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['bids-train']\n",
    "rounds = 1\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=UNetTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 tensorboard=True\n",
    "                )\n",
    "bids_strategy = BIDSStrategy(exp._fds, modalities=['T1'])\n",
    "_ = exp.set_strategy(node_selection_strategy=bids_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86aca90-54f6-4267-9154-73c46da7add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75914fe1-d3ec-499f-a883-82acdcea4dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b47274-8f2b-4476-bb73-19c95a4a5126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a122332-4939-43a4-9058-802d4615950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import environ\n",
    "tensorboard_dir = environ['TENSORBOARD_RESULTS_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3371fd-175f-408b-9de5-9b86716a7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir \"$tensorboard_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9c255-c98d-4c4e-87c2-3f1bf359c467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedbiomed-researcher",
   "language": "python",
   "name": "fedbiomed-researcher"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
