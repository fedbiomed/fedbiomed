{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data imputation with Fedbiomed using MIWAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how to impute missing not at random (MAR) data in a federated setting using MIWAE (https://arxiv.org/abs/2006.12871). We will compare results of federated training using FedAvg, FedProx and [FedCos](https://arxiv.org/abs/2204.03174) with local results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment we will use the breast cancer data from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data, target, test_size=0.20, random_state=42)\n",
    "df_data_train = pd.DataFrame(data_train)\n",
    "N_train = len(df_data_train)\n",
    "client_1, client_2, client_3 = np.split(df_data_train.sample(frac=1,random_state=42), \\\n",
    "                                        [int(.33*N_train), int(.66*len(df_data_train))])\n",
    "\n",
    "Clients_data=[client_1, client_2, client_3]\n",
    "\n",
    "# from each dataset we will remove randomly 50% of data\n",
    "np.random.seed(1234)\n",
    "\n",
    "# 50% of missing data for client 1, 30% for client 2, 60% for client 3\n",
    "perc_miss_list = [0.5,0.3,0.6] \n",
    "\n",
    "Clients_missing = []\n",
    "for perc,c in enumerate(Clients_data):\n",
    "    perc_miss=perc_miss_list[perc]\n",
    "    n = c.shape[0] # number of observations\n",
    "    p = c.shape[1] # number of features\n",
    "    xmiss = np.copy(c)\n",
    "    xmiss = (xmiss - np.mean(xmiss,0))/np.std(xmiss,0)\n",
    "    xmiss_flat = xmiss.flatten()\n",
    "    miss_pattern = np.random.choice(n*p, np.floor(n*p*perc_miss).astype(np.int_),\\\n",
    "                                    replace=False)\n",
    "    xmiss_flat[miss_pattern] = np.nan \n",
    "    xmiss = xmiss_flat.reshape([n,p]) # in xmiss, the missing values are represented by nans\n",
    "    mask = np.isfinite(xmiss) # binary mask that indicates which values are missing\n",
    "    Clients_missing.append(xmiss)\n",
    "\n",
    "import os \n",
    "os.makedirs('clients_data', exist_ok=True) \n",
    "for i in range(len(Clients_missing)):\n",
    "    pd.DataFrame(Clients_missing[i]).to_csv('clients_data/client_'+str(i+1)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the network\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the nodes up\n",
    "It is necessary to previously configure a node:\n",
    "1. `./scripts/fedbiomed_run node add`\n",
    "  * Select option 1 (csv) to add client_1 dataset to the first node\n",
    "  * Provide the correct tag by entering:  breast_cancer\n",
    "  * Pick the folder where client_1 dataset has been saved\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "3. Run the node using `./scripts/fedbiomed_run node start`. Wait until you get `Starting task manager`. it means you are online.\n",
    "4. Following the same procedure, you can create additional nodes for clients 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 10:03:32,228 fedbiomed INFO - Component environment:\n",
      "2022-05-20 10:03:32,229 fedbiomed INFO - type = ComponentType.RESEARCHER\n",
      "2022-05-20 10:03:32,270 fedbiomed INFO - Messaging researcher_46feb887-a218-4d51-adea-4c996b331186 successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x13850f190>\n",
      "2022-05-20 10:03:32,371 fedbiomed INFO - Listing available datasets in all nodes... \n",
      "2022-05-20 10:03:42,389 fedbiomed INFO - \n",
      " Node: node_249fa875-5b53-49c9-aebd-c53009d774a5 | Number of Datasets: 1 \n",
      "+---------------+-------------+-------------------+---------------+-----------+\n",
      "| name          | data_type   | tags              | description   | shape     |\n",
      "+===============+=============+===================+===============+===========+\n",
      "| breast_cancer | csv         | ['breast_cancer'] | breast_cancer | [134, 13] |\n",
      "+---------------+-------------+-------------------+---------------+-----------+\n",
      "\n",
      "2022-05-20 10:03:42,391 fedbiomed INFO - \n",
      " Node: node_e27fee9e-415f-4927-8f52-41ac937784cf | Number of Datasets: 1 \n",
      "+---------------+-------------+-------------------+---------------+-----------+\n",
      "| name          | data_type   | tags              | description   | shape     |\n",
      "+===============+=============+===================+===============+===========+\n",
      "| breast_cancer | csv         | ['breast_cancer'] | breast_cancer | [134, 13] |\n",
      "+---------------+-------------+-------------------+---------------+-----------+\n",
      "\n",
      "2022-05-20 10:03:42,393 fedbiomed INFO - \n",
      " Node: node_efa8cc39-825f-438a-940c-fce941364667 | Number of Datasets: 1 \n",
      "+---------------+-------------+-------------------+---------------+-----------+\n",
      "| name          | data_type   | tags              | description   | shape     |\n",
      "+===============+=============+===================+===============+===========+\n",
      "| breast_cancer | csv         | ['breast_cancer'] | breast_cancer | [139, 13] |\n",
      "+---------------+-------------+-------------------+---------------+-----------+\n",
      "\n",
      "2022-05-20 10:03:42,396 fedbiomed INFO - Listing available datasets in all nodes... \n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.requests import Requests\n",
    "req = Requests()\n",
    "req.list(verbose=True)\n",
    "xx = req.list()\n",
    "dataset_size = [xx[i][0]['shape'][1] for i in xx]\n",
    "assert min(dataset_size)==max(dataset_size)\n",
    "data_size = dataset_size[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an experiment model and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a torch.nn MIWAETrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : write **only** the code to export in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.distributions as td\n",
    "import pandas as pd\n",
    "\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from fedbiomed.common.constants import ProcessTypes\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class MIWAETrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self, model_args: dict = {}):\n",
    "        super(MIWAETrainingPlan, self).__init__(model_args)\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "               \"import torch.distributions as td\",\n",
    "               \"import pandas as pd\",\n",
    "               \"import numpy as np\"]\n",
    "        \n",
    "        self.n_features=model_args['n_features']\n",
    "        self.n_latent=model_args['n_latent']\n",
    "        self.n_hidden=model_args['n_hidden']\n",
    "        self.n_samples=model_args['n_samples']\n",
    "        \n",
    "        self.add_dependency(deps)\n",
    "        \n",
    "        # the encoder will output both the mean and the diagonal covariance\n",
    "        self.encoder=nn.Sequential(\n",
    "                        torch.nn.Linear(self.n_features, self.n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(self.n_hidden, self.n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(self.n_hidden, 2*self.n_latent),  \n",
    "                        )\n",
    "        # the decoder will output both the mean, the scale, \n",
    "        # and the number of degrees of freedoms (hence the 3*p)\n",
    "        self.decoder = nn.Sequential(\n",
    "                        torch.nn.Linear(self.n_latent, self.n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(self.n_hidden, self.n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(self.n_hidden, 3*self.n_features),  \n",
    "                        )\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(list(self.encoder.parameters()) \\\n",
    "                                    + list(self.decoder.parameters()),lr=1e-3)\n",
    "              \n",
    "        self.encoder.apply(self.weights_init)\n",
    "        self.decoder.apply(self.weights_init)\n",
    "    \n",
    "    def weights_init(self,layer):\n",
    "        if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)\n",
    "    \n",
    "    def miwae_loss(self,iota_x,mask):\n",
    "        batch_size = iota_x.shape[0]\n",
    "        out_encoder = self.encoder(iota_x)\n",
    "        # prior\n",
    "        p_z = td.Independent(td.Normal(loc=torch.zeros(self.n_latent).to(self.device)\\\n",
    "                                       ,scale=torch.ones(self.n_latent).to(self.device)),1)\n",
    "        \n",
    "        q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :self.n_latent],\\\n",
    "                                                scale=torch.nn.Softplus()\\\n",
    "                                                (out_encoder[..., self.n_latent:\\\n",
    "                                                             (2*self.n_latent)])),1)\n",
    "\n",
    "        zgivenx = q_zgivenxobs.rsample([self.n_samples])\n",
    "        zgivenx_flat = zgivenx.reshape([self.n_samples*batch_size,self.n_latent])\n",
    "\n",
    "        out_decoder = self.decoder(zgivenx_flat)\n",
    "        all_means_obs_model = out_decoder[..., :self.n_features]\n",
    "        all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., self.n_features:\\\n",
    "                                                               (2*self.n_features)]) + 0.001\n",
    "        all_degfreedom_obs_model = torch.nn.Softplus()\\\n",
    "        (out_decoder[..., (2*self.n_features):(3*self.n_features)]) + 3\n",
    "\n",
    "        data_flat = torch.Tensor.repeat(iota_x,[self.n_samples,1]).reshape([-1,1])\n",
    "        tiledmask = torch.Tensor.repeat(mask,[self.n_samples,1])\n",
    "\n",
    "        all_log_pxgivenz_flat = torch.distributions.StudentT\\\n",
    "        (loc=all_means_obs_model.reshape([-1,1]),\\\n",
    "         scale=all_scales_obs_model.reshape([-1,1]),\\\n",
    "         df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
    "        all_log_pxgivenz = all_log_pxgivenz_flat.reshape([self.n_samples*batch_size,self.n_features])\n",
    "\n",
    "        logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([self.n_samples,batch_size])\n",
    "        logpz = p_z.log_prob(zgivenx)\n",
    "        logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "        neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq,0))\n",
    "\n",
    "        return neg_bound\n",
    "\n",
    "    def training_data(self,  batch_size = 48):\n",
    "        \n",
    "        df = pd.read_csv(self.dataset_path, sep=',', index_col=False)\n",
    "        x_train = df.values\n",
    "        x_mask = np.isfinite(x_train)\n",
    "        # xhat_0: missing values are replaced by zeros. \n",
    "        #This x_hat0 is what will be fed to our encoder.\n",
    "        xhat_0 = np.copy(x_train)\n",
    "        xhat_0[np.isnan(x_train)] = 0\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        \n",
    "        data_manager = DataManager(dataset=xhat_0 , target=x_mask , **train_kwargs)\n",
    "        \n",
    "        return data_manager\n",
    "    \n",
    "    def training_step(self, data, mask):\n",
    "        self.encoder.zero_grad()\n",
    "        self.decoder.zero_grad()\n",
    "        loss = self.miwae_loss(iota_x = data,mask = mask)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side. \n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "* data `tags` to search nodes for training.\n",
    "* total number of `rounds`.\n",
    "If FedProx optimisation is requested, `fedprox_mu` parameter must be defined here. It also must be a float between XX and YY.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "h = 128 # number of hidden units in (same for all MLPs)\n",
    "d = 10 # dimension of the latent space, we choose d=1 for visualisation purposes\n",
    "K = 20 # number of IS during training\n",
    "\n",
    "n_epochs=5\n",
    "\n",
    "model_args = {'n_features':data_size, 'n_latent':d,'n_hidden':h,'n_samples':K}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'lr': 1e-3, \n",
    "    'log_interval' : 1,\n",
    "    'epochs': n_epochs, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "tags =  ['breast_cancer']\n",
    "rounds = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and run the experiment\n",
    "\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `round_limit` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 10:03:55,794 fedbiomed INFO - Searching dataset with data tags: ['breast_cancer'] for all nodes\n",
      "2022-05-20 10:04:05,812 fedbiomed INFO - Node selected for training -> node_249fa875-5b53-49c9-aebd-c53009d774a5\n",
      "2022-05-20 10:04:05,813 fedbiomed INFO - Node selected for training -> node_e27fee9e-415f-4927-8f52-41ac937784cf\n",
      "2022-05-20 10:04:05,814 fedbiomed INFO - Node selected for training -> node_efa8cc39-825f-438a-940c-fce941364667\n",
      "2022-05-20 10:04:05,817 fedbiomed INFO - Checking data quality of federated datasets...\n",
      "2022-05-20 10:04:05,976 fedbiomed DEBUG - Model file has been saved: /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/var/experiments/Experiment_0000/my_model_d017b264-e90c-4c8b-a879-a2a02e3157c8.py\n",
      "2022-05-20 10:04:06,295 fedbiomed DEBUG - upload (HTTP POST request) of file /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/var/experiments/Experiment_0000/my_model_d017b264-e90c-4c8b-a879-a2a02e3157c8.py successful, with status code 201\n",
      "2022-05-20 10:04:06,798 fedbiomed DEBUG - upload (HTTP POST request) of file /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/var/experiments/Experiment_0000/aggregated_params_init_b9fba4c1-6f4f-4e3a-b7de-4994a0347c98.pt successful, with status code 201\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MIWAETrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `round_limit` rounds are done for all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local training results for each round and each node are available via `exp.training_replies()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the training results for the last round below.\n",
    "\n",
    "Different timings (in seconds) are reported for each dataset of a node participating in a round :\n",
    "- `rtime_training` real time (clock time) spent in the training function on the node\n",
    "- `ptime_training` process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total` real time (clock time) spent in the researcher between sending the request and handling the response, at the `Job()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List the training rounds :  dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\n",
      "\n",
      "List the nodes for the last training round and their timings : \n",
      "\t- node_249fa875-5b53-49c9-aebd-c53009d774a5 :    \n",
      "\t\trtime_training=0.67 seconds    \n",
      "\t\tptime_training=0.33 seconds    \n",
      "\t\trtime_total=10.03 seconds\n",
      "\t- node_efa8cc39-825f-438a-940c-fce941364667 :    \n",
      "\t\trtime_training=0.70 seconds    \n",
      "\t\tptime_training=0.35 seconds    \n",
      "\t\trtime_total=10.07 seconds\n",
      "\t- node_e27fee9e-415f-4927-8f52-41ac937784cf :    \n",
      "\t\trtime_training=0.70 seconds    \n",
      "\t\tptime_training=0.35 seconds    \n",
      "\t\trtime_total=10.12 seconds\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>success</th>\n",
       "      <th>msg</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>params_path</th>\n",
       "      <th>params</th>\n",
       "      <th>timing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>dataset_fca98937-6cb8-4587-bc69-7f0fdad09abd</td>\n",
       "      <td>node_249fa875-5b53-49c9-aebd-c53009d774a5</td>\n",
       "      <td>/Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed...</td>\n",
       "      <td>{'encoder.0.weight': [[tensor(-0.1305), tensor...</td>\n",
       "      <td>{'rtime_training': 0.6657301810000149, 'ptime_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>dataset_b231ccd0-b513-4a32-b4d6-af5412e8c679</td>\n",
       "      <td>node_efa8cc39-825f-438a-940c-fce941364667</td>\n",
       "      <td>/Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed...</td>\n",
       "      <td>{'encoder.0.weight': [[tensor(-0.1253), tensor...</td>\n",
       "      <td>{'rtime_training': 0.6970942529999888, 'ptime_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>dataset_60c3d82d-8590-4d12-815d-ba11aae8af13</td>\n",
       "      <td>node_e27fee9e-415f-4927-8f52-41ac937784cf</td>\n",
       "      <td>/Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed...</td>\n",
       "      <td>{'encoder.0.weight': [[tensor(-0.1385), tensor...</td>\n",
       "      <td>{'rtime_training': 0.7043897529999867, 'ptime_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   success msg                                    dataset_id  \\\n",
       "0     True      dataset_fca98937-6cb8-4587-bc69-7f0fdad09abd   \n",
       "1     True      dataset_b231ccd0-b513-4a32-b4d6-af5412e8c679   \n",
       "2     True      dataset_60c3d82d-8590-4d12-815d-ba11aae8af13   \n",
       "\n",
       "                                     node_id  \\\n",
       "0  node_249fa875-5b53-49c9-aebd-c53009d774a5   \n",
       "1  node_efa8cc39-825f-438a-940c-fce941364667   \n",
       "2  node_e27fee9e-415f-4927-8f52-41ac937784cf   \n",
       "\n",
       "                                         params_path  \\\n",
       "0  /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed...   \n",
       "1  /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed...   \n",
       "2  /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed...   \n",
       "\n",
       "                                              params  \\\n",
       "0  {'encoder.0.weight': [[tensor(-0.1305), tensor...   \n",
       "1  {'encoder.0.weight': [[tensor(-0.1253), tensor...   \n",
       "2  {'encoder.0.weight': [[tensor(-0.1385), tensor...   \n",
       "\n",
       "                                              timing  \n",
       "0  {'rtime_training': 0.6657301810000149, 'ptime_...  \n",
       "1  {'rtime_training': 0.6970942529999888, 'ptime_...  \n",
       "2  {'rtime_training': 0.7043897529999867, 'ptime_...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())\n",
    "\n",
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "round_data = exp.training_replies()[rounds - 1].data()\n",
    "for c in range(len(round_data)):\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n",
    "        rtraining = round_data[c]['timing']['rtime_training'],\n",
    "        ptraining = round_data[c]['timing']['ptime_training'],\n",
    "        rtotal = round_data[c]['timing']['rtime_total']))\n",
    "print('\\n')\n",
    "    \n",
    "exp.training_replies()[rounds - 1].dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated parameters for each round are available via `exp.aggregated_params()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the federated parameters for the last round of the experiment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List the training rounds :  dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\n",
      "\n",
      "Access the federated params for the last training round :\n",
      "\t- params_path:  /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/var/experiments/Experiment_0000/aggregated_params_6e1939d0-890e-430c-b695-945de713f0a4.pt\n",
      "\t- parameter data:  odict_keys(['encoder.0.weight', 'encoder.0.bias', 'encoder.2.weight', 'encoder.2.bias', 'encoder.4.weight', 'encoder.4.bias', 'decoder.0.weight', 'decoder.0.bias', 'decoder.2.weight', 'decoder.2.bias', 'decoder.4.weight', 'decoder.4.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment with FedProx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the federated training but using FedProx as aggregation scheme (starting from the second iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 10:09:53,302 fedbiomed INFO - Searching dataset with data tags: ['breast_cancer'] for all nodes\n",
      "2022-05-20 10:10:03,324 fedbiomed INFO - Node selected for training -> node_249fa875-5b53-49c9-aebd-c53009d774a5\n",
      "2022-05-20 10:10:03,325 fedbiomed INFO - Node selected for training -> node_efa8cc39-825f-438a-940c-fce941364667\n",
      "2022-05-20 10:10:03,326 fedbiomed INFO - Node selected for training -> node_e27fee9e-415f-4927-8f52-41ac937784cf\n",
      "2022-05-20 10:10:03,328 fedbiomed INFO - Checking data quality of federated datasets...\n",
      "2022-05-20 10:10:03,356 fedbiomed DEBUG - Model file has been saved: /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/var/experiments/Experiment_0001/my_model_803a2080-cce2-415f-a88b-82b003c0643c.py\n",
      "2022-05-20 10:10:03,448 fedbiomed DEBUG - upload (HTTP POST request) of file /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/var/experiments/Experiment_0001/my_model_803a2080-cce2-415f-a88b-82b003c0643c.py successful, with status code 201\n",
      "2022-05-20 10:10:03,562 fedbiomed DEBUG - upload (HTTP POST request) of file /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/var/experiments/Experiment_0001/aggregated_params_init_97e6a5cd-839c-4f25-9512-dae64f1b4265.pt successful, with status code 201\n"
     ]
    }
   ],
   "source": [
    "# To make the method fairly comparable with FedCos, during the first round\n",
    "# we will simply use FedAvg with standard optimization scheme: the FedProx penalization\n",
    "# term will be introduced exclusively from the second round.\n",
    "# training_args.update(fedprox_mu = 0.)\n",
    "\n",
    "exp_fedprox = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MIWAETrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_fedprox.run_once()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from the second round, FedProx is used with mu=0.1\n",
    "# We first update the training args\n",
    "training_args.update(fedprox_mu = 0.1)\n",
    "# Then update training args in the experiment\n",
    "exp_fedprox.set_training_args(training_args)\n",
    "exp_fedprox.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment with FedCos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we propose to use FedCos as well, which introduce an alternative penalization term with cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 10:15:15,530 fedbiomed INFO - Searching dataset with data tags: ['breast_cancer'] for all nodes\n",
      "2022-05-20 10:15:25,552 fedbiomed INFO - Node selected for training -> node_e27fee9e-415f-4927-8f52-41ac937784cf\n",
      "2022-05-20 10:15:25,554 fedbiomed INFO - Node selected for training -> node_249fa875-5b53-49c9-aebd-c53009d774a5\n",
      "2022-05-20 10:15:25,554 fedbiomed INFO - Node selected for training -> node_efa8cc39-825f-438a-940c-fce941364667\n",
      "2022-05-20 10:15:25,560 fedbiomed INFO - Checking data quality of federated datasets...\n",
      "2022-05-20 10:15:25,589 fedbiomed DEBUG - Model file has been saved: /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/var/experiments/Experiment_0002/my_model_dcd0983c-ca68-4eb5-b395-85361d347271.py\n",
      "2022-05-20 10:15:25,742 fedbiomed DEBUG - upload (HTTP POST request) of file /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/var/experiments/Experiment_0002/my_model_dcd0983c-ca68-4eb5-b395-85361d347271.py successful, with status code 201\n",
      "2022-05-20 10:15:25,862 fedbiomed DEBUG - upload (HTTP POST request) of file /Users/balelli/ownCloud/INRIA_EPIONE/FedBioMed/fedbiomed/var/experiments/Experiment_0002/aggregated_params_init_44aad1d0-e698-40cc-bba4-3e8f20a6e3a1.pt successful, with status code 201\n"
     ]
    }
   ],
   "source": [
    "from fedbiomed.researcher.aggregators.fedcos import FedCos\n",
    "\n",
    "# Warning: we can not perform simultaneously FedProx and FedCos, so only one (at most) \n",
    "# between 'fedprox_mu' and 'fedcos_mu' should be provided in 'training_args'.\n",
    "# Therefore we will remove 'fedprox_mu' from 'training_args' before defining 'fedcos_mu'\n",
    "del training_args['fedprox_mu'] \n",
    "\n",
    "training_args.update(fedcos_mu = 0.01)\n",
    "\n",
    "exp_fedcos = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MIWAETrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedCos(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_fedcos.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and comparison to local training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Testing on an external dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we are going to test the performance of the final federated model to impute missing data on a test dataset. To this extent we are going to remove randomly 50% of samples from the test dataset, `data_test`, defined at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the test dataset, we will remove randomly 50% of data\n",
    "np.random.seed(1234)\n",
    "\n",
    "perc_miss = 0.5 # 50% of missing data\n",
    "\n",
    "n = data_test.shape[0] # number of observations\n",
    "p = data_test.shape[1] # number of features\n",
    "xfull = np.copy(data_test)\n",
    "xfull = (xfull - np.mean(xfull,0))/np.std(xfull,0)\n",
    "xmiss = np.copy(xfull)\n",
    "xmiss_flat = xmiss.flatten()\n",
    "miss_pattern = np.random.choice(n*p, np.floor(n*p*perc_miss).astype(np.int_),\\\n",
    "                                replace=False)\n",
    "xmiss_flat[miss_pattern] = np.nan \n",
    "xmiss = xmiss_flat.reshape([n,p]) # in xmiss, the missing values are represented by nans\n",
    "mask = np.isfinite(xmiss) # binary mask that indicates which values are missing\n",
    "xhat_0 = np.copy(xmiss)\n",
    "xhat_0[np.isnan(xmiss)] = 0\n",
    "xhat = np.copy(xhat_0) # This will be out imputed data matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the MIWAE imputation routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miwae_impute(encoder,decoder,iota_x,mask,d,L):\n",
    "    \n",
    "    p_z = td.Independent(td.Normal(loc=torch.zeros(d),scale=torch.ones(d)),1)\n",
    "    \n",
    "    batch_size = iota_x.shape[0]\n",
    "    out_encoder = encoder(iota_x)\n",
    "    q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d],scale=torch.nn.Softplus()(out_encoder[..., d:(2*d)])),1)\n",
    "\n",
    "    zgivenx = q_zgivenxobs.rsample([L])\n",
    "    zgivenx_flat = zgivenx.reshape([L*batch_size,d])\n",
    "\n",
    "    out_decoder = decoder(zgivenx_flat)\n",
    "    all_means_obs_model = out_decoder[..., :p]\n",
    "    all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., p:(2*p)]) + 0.001\n",
    "    all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*p):(3*p)]) + 3\n",
    "\n",
    "    data_flat = torch.Tensor.repeat(iota_x,[L,1]).reshape([-1,1])\n",
    "    tiledmask = torch.Tensor.repeat(mask,[L,1])\n",
    "\n",
    "    all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1,1]),scale=all_scales_obs_model.reshape([-1,1]),df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
    "    all_log_pxgivenz = all_log_pxgivenz_flat.reshape([L*batch_size,p])\n",
    "\n",
    "    logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([L,batch_size])\n",
    "    logpz = p_z.log_prob(zgivenx)\n",
    "    logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "    xgivenz = td.Independent(td.StudentT(loc=all_means_obs_model, scale=all_scales_obs_model, df=all_degfreedom_obs_model),1)\n",
    "\n",
    "    imp_weights = torch.nn.functional.softmax(logpxobsgivenz + logpz - logq,0) # these are w_1,....,w_L for all observations in the batch\n",
    "    xms = xgivenz.mean.reshape([L,batch_size,p])  # that's the only line that changed!\n",
    "    xm=torch.einsum('ki,kij->ij', imp_weights, xms) \n",
    "\n",
    "    return xm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the MSE function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(xhat,xtrue,mask): # MSE function for imputations\n",
    "    xhat = np.array(xhat)\n",
    "    xtrue = np.array(xtrue)\n",
    "    return np.mean(np.power(xhat-xtrue,2)[~mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the model using last updated federated parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract federated model into PyTorch framework\n",
    "model = exp.model_instance()\n",
    "model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "encoder = model.encoder\n",
    "decoder = model.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for the models trained with FedProx and FedCos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fedprox = exp_fedprox.model_instance()\n",
    "model_fedprox.load_state_dict(exp_fedprox.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "encoder_fedprox = model_fedprox.encoder\n",
    "decoder_fedprox = model_fedprox.decoder\n",
    "\n",
    "model_fedcos = exp_fedcos.model_instance()\n",
    "# We should remove the 'disp_global' key from the aggregated paramters since this\n",
    "# is not needed to instantiate the model (just needed for training)\n",
    "aggregated_params_fedcos = exp_fedcos.aggregated_params()[rounds - 1]['params']\n",
    "del aggregated_params_fedcos['disp_global']\n",
    "model_fedcos.load_state_dict(aggregated_params_fedcos)\n",
    "\n",
    "encoder_fedcos = model_fedcos.encoder\n",
    "decoder_fedcos = model_fedcos.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we finally do the imputation and evaluate the corresponding imputation error through MSE for each federated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of fed model on testing data 0.584008\n",
      "-----\n",
      "Imputation MSE of fed model (with fedprox) on testing data  0.56444\n",
      "-----\n",
      "Imputation MSE of fed model (with fedcos) on testing data  0.581398\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "L = 100\n",
    "\n",
    "xhat[~mask] = miwae_impute(encoder = encoder,decoder = decoder,iota_x = torch.from_numpy(xhat_0).float(),mask = torch.from_numpy(mask).float(),d = d,L= L).cpu().data.numpy()[~mask]\n",
    "err_test_data = np.array([mse(xhat,xfull,mask)])\n",
    "print('Imputation MSE of fed model on testing data %g' %err_test_data)\n",
    "print('-----')\n",
    "\n",
    "xhat[~mask] = miwae_impute(encoder = encoder_fedprox,decoder = decoder_fedprox,iota_x = torch.from_numpy(xhat_0).float(),mask = torch.from_numpy(mask).float(),d = d,L= L).cpu().data.numpy()[~mask]\n",
    "err_test_data_fedprox = np.array([mse(xhat,xfull,mask)])\n",
    "print('Imputation MSE of fed model (with fedprox) on testing data  %g' %err_test_data_fedprox)\n",
    "print('-----')\n",
    "\n",
    "xhat[~mask] = miwae_impute(encoder = encoder_fedcos,decoder = decoder_fedcos,iota_x = torch.from_numpy(xhat_0).float(),mask = torch.from_numpy(mask).float(),d = d,L= L).cpu().data.numpy()[~mask]\n",
    "err_test_data_fedcos = np.array([mse(xhat,xfull,mask)])\n",
    "print('Imputation MSE of fed model (with fedcos) on testing data  %g' %err_test_data_fedcos)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing on a client's dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to use the final federated model to impute missing data of client 1, which have been used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of fed model on data from client 1  0.596102\n",
      "-----\n",
      "Imputation MSE of fed model (with fedprox) on data from client 1  0.602942\n",
      "-----\n",
      "Imputation MSE of fed model (with fedcos) on data from client 1  0.591246\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# We first recover data (full and with missing entries) from client 1\n",
    "data_client_1 = np.copy(Clients_data[0])\n",
    "xfull_cl1 = np.copy(data_client_1)\n",
    "xfull_cl1 = (xfull_cl1 - np.mean(xfull_cl1,0))/np.std(xfull_cl1,0)\n",
    "xmiss_cl1 = np.copy(Clients_missing[0])\n",
    "\n",
    "mask_cl1 = np.isfinite(xmiss_cl1) # binary mask that indicates which values are missing\n",
    "xhat_0_cl1 = np.copy(xmiss_cl1)\n",
    "xhat_0_cl1[np.isnan(xmiss_cl1)] = 0\n",
    "xhat_cl1 = np.copy(xhat_0_cl1) # This will be out imputed data matrix\n",
    "\n",
    "### Now we do the imputation\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder,decoder = decoder, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_cl1_data = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of fed model on data from client 1  %g' %err_cl1_data)\n",
    "print('-----')\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder_fedprox,decoder = decoder_fedprox, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_cl1_data_fedprox = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of fed model (with fedprox) on data from client 1  %g' %err_cl1_data_fedprox)\n",
    "print('-----')\n",
    "\n",
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder_fedcos,decoder = decoder_fedcos, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_cl1_data_fedcos = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of fed model (with fedcos) on data from client 1  %g' %err_cl1_data_fedcos)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Local training and testing on a client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test the performance of the same model trained locally and tested on the dataset from client 1. We will use a total of `epochs`x`rounds` local epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miwae_loss(iota_x,mask,d):\n",
    "    \n",
    "    p_z = td.Independent(td.Normal(loc=torch.zeros(d),scale=torch.ones(d)),1)\n",
    "    \n",
    "    batch_size = iota_x.shape[0]\n",
    "    out_encoder = encoder(iota_x)\n",
    "    q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d],scale=torch.nn.Softplus()(out_encoder[..., d:(2*d)])),1)\n",
    "\n",
    "    zgivenx = q_zgivenxobs.rsample([K])\n",
    "    zgivenx_flat = zgivenx.reshape([K*batch_size,d])\n",
    "\n",
    "    out_decoder = decoder(zgivenx_flat)\n",
    "    all_means_obs_model = out_decoder[..., :p]\n",
    "    all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., p:(2*p)]) + 0.001\n",
    "    all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*p):(3*p)]) + 3\n",
    "\n",
    "    data_flat = torch.Tensor.repeat(iota_x,[K,1]).reshape([-1,1])\n",
    "    tiledmask = torch.Tensor.repeat(mask,[K,1])\n",
    "\n",
    "    all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1,1]),scale=all_scales_obs_model.reshape([-1,1]),df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
    "    all_log_pxgivenz = all_log_pxgivenz_flat.reshape([K*batch_size,p])\n",
    "\n",
    "    logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([K,batch_size])\n",
    "    logpz = p_z.log_prob(zgivenx)\n",
    "    logq = q_zgivenxobs.log_prob(zgivenx)\n",
    "\n",
    "    neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq,0))\n",
    "\n",
    "    return neg_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the local training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "MIWAE likelihood bound  -2.8003\n",
      "Epoch 31\n",
      "MIWAE likelihood bound  -2.86463\n",
      "Epoch 61\n",
      "MIWAE likelihood bound  -2.7242\n",
      "Epoch 91\n",
      "MIWAE likelihood bound  -2.89334\n",
      "Epoch 121\n",
      "MIWAE likelihood bound  -2.81006\n"
     ]
    }
   ],
   "source": [
    "# We recover again data (full and with missing entries) from client 1\n",
    "data_client_1 = np.copy(Clients_data[0])\n",
    "xfull_cl1 = np.copy(data_client_1)\n",
    "xfull_cl1 = (xfull_cl1 - np.mean(xfull_cl1,0))/np.std(xfull_cl1,0)\n",
    "xmiss_cl1 = np.copy(Clients_missing[0])\n",
    "\n",
    "mask_cl1 = np.isfinite(xmiss_cl1) # binary mask that indicates which values are missing\n",
    "xhat_0_cl1 = np.copy(xmiss_cl1)\n",
    "xhat_0_cl1[np.isnan(xmiss_cl1)] = 0\n",
    "xhat_cl1 = np.copy(xhat_0_cl1) # This will be out imputed data matrix\n",
    "\n",
    "n_epochs_local = n_epochs*rounds\n",
    "bs = 48 # batch size\n",
    "\n",
    "encoder_cl1 = nn.Sequential(\n",
    "    torch.nn.Linear(p, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 2*d),  # the encoder will output both the mean and the diagonal covariance\n",
    ")\n",
    "\n",
    "decoder_cl1 = nn.Sequential(\n",
    "    torch.nn.Linear(d, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, h),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(h, 3*p),  # the decoder will output both the mean, the scale, and the number of degrees of freedoms (hence the 3*p)\n",
    ")\n",
    "\n",
    "optimizer_cl1 = torch.optim.Adam(list(encoder_cl1.parameters()) + list(decoder_cl1.parameters()),lr=1e-3)\n",
    "\n",
    "def weights_init(layer):\n",
    "    if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)\n",
    "        \n",
    "encoder_cl1.apply(weights_init)\n",
    "decoder_cl1.apply(weights_init)\n",
    "\n",
    "for ep in range(1,n_epochs_local):\n",
    "    perm = np.random.permutation(n) # We use the \"random reshuffling\" version of SGD\n",
    "    batches_data = np.array_split(xhat_0_cl1[perm,], n/bs)\n",
    "    batches_mask = np.array_split(mask_cl1[perm,], n/bs)\n",
    "    for it in range(len(batches_data)):\n",
    "        optimizer_cl1.zero_grad()\n",
    "        encoder_cl1.zero_grad()\n",
    "        decoder_cl1.zero_grad()\n",
    "        b_data = torch.from_numpy(batches_data[it]).float()\n",
    "        b_mask = torch.from_numpy(batches_mask[it]).float()\n",
    "        loss = miwae_loss(iota_x = b_data,mask = b_mask, d = d)\n",
    "        loss.backward()\n",
    "        optimizer_cl1.step()\n",
    "    if ep % rounds == 1:\n",
    "        print('Epoch %g' %ep)\n",
    "        print('MIWAE likelihood bound  %g' %(-np.log(K)-miwae_loss(iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(), d = d).cpu().data.numpy())) # Gradient step      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we do the imputation on the same dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of local model on data from same client (cl 1)  1.04139\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "xhat_cl1[~mask_cl1] = miwae_impute(encoder = encoder_cl1, decoder = decoder_cl1, iota_x = torch.from_numpy(xhat_0_cl1).float(),mask = torch.from_numpy(mask_cl1).float(),d = d,L= L).cpu().data.numpy()[~mask_cl1]\n",
    "err_local_cl1_data = np.array([mse(xhat_cl1,xfull_cl1,mask_cl1)])\n",
    "print('Imputation MSE of local model on data from same client (cl 1)  %g' %err_local_cl1_data)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the imputation on the external test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE of local model on testing data 1.03618\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "xhat[~mask] = miwae_impute(encoder = encoder_cl1,decoder = decoder_cl1,iota_x = torch.from_numpy(xhat_0).float(),mask = torch.from_numpy(mask).float(),d = d,L= L).cpu().data.numpy()[~mask]\n",
    "err_local_cl1_test_data = np.array([mse(xhat,xfull,mask)])\n",
    "print('Imputation MSE of local model on testing data %g' %err_local_cl1_test_data)\n",
    "print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of obtained results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation MSE on testing data\n",
      "-----\n",
      "Model          Mean Squared Error (â†“)\n",
      "-----------  ------------------------\n",
      "FedAvg                       0.584008\n",
      "FedProx                      0.56444\n",
      "FedCos                       0.581398\n",
      "Local (cl1)                  1.03618\n",
      "-----\n",
      "-----\n",
      "Imputation MSE on local data from client 1\n",
      "-----\n",
      "Model          Mean Squared Error (â†“)\n",
      "-----------  ------------------------\n",
      "FedAvg                       0.596102\n",
      "FedProx                      0.602942\n",
      "FedCos                       0.591246\n",
      "Local (cl1)                  1.04139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 10:23:18,146 fedbiomed INFO - \u001b[1mCRITICAL\u001b[0m\n",
      "\t\t\t\t\t\u001b[1m NODE\u001b[0m node_efa8cc39-825f-438a-940c-fce941364667\n",
      "\t\t\t\t\t\u001b[1m MESSAGE:\u001b[0m Node stopped in signal_handler, probably by user decision (Ctrl C)\u001b[0m\n",
      "-----------------------------------------------------------------\n",
      "2022-05-20 10:23:19,730 fedbiomed INFO - \u001b[1mCRITICAL\u001b[0m\n",
      "\t\t\t\t\t\u001b[1m NODE\u001b[0m node_e27fee9e-415f-4927-8f52-41ac937784cf\n",
      "\t\t\t\t\t\u001b[1m MESSAGE:\u001b[0m Node stopped in signal_handler, probably by user decision (Ctrl C)\u001b[0m\n",
      "-----------------------------------------------------------------\n",
      "2022-05-20 10:23:20,980 fedbiomed INFO - \u001b[1mCRITICAL\u001b[0m\n",
      "\t\t\t\t\t\u001b[1m NODE\u001b[0m node_249fa875-5b53-49c9-aebd-c53009d774a5\n",
      "\t\t\t\t\t\u001b[1m MESSAGE:\u001b[0m Node stopped in signal_handler, probably by user decision (Ctrl C)\u001b[0m\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print('Imputation MSE on testing data')\n",
    "print('-----')\n",
    "data = [['FedAvg', err_test_data],\n",
    "['FedProx', err_test_data_fedprox],\n",
    "['FedCos', err_test_data_fedcos],\n",
    "['Local (cl1)', err_local_cl1_test_data]]\n",
    "print (tabulate(data, headers=[\"Model\", \"Mean Squared Error (\\u2193)\"]))\n",
    "print('-----')\n",
    "print('-----')\n",
    "print('Imputation MSE on local data from client 1')\n",
    "print('-----')\n",
    "data = [['FedAvg', err_cl1_data],\n",
    "['FedProx', err_cl1_data_fedprox],\n",
    "['FedCos', err_cl1_data_fedcos],\n",
    "['Local (cl1)', err_local_cl1_data]]\n",
    "print (tabulate(data, headers=[\"Model\", \"Mean Squared Error (\\u2193)\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the federated model performs better than the local one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
