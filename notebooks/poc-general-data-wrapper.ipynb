{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee99bdb4",
   "metadata": {},
   "source": [
    "Related to user story: [SP11-Item04: General Data Wrapper PoC](https://gitlab.inria.fr/fedbiomed/fedbiomed/-/issues/164)\n",
    "\n",
    "## Tabular dataset\n",
    "\n",
    "Workflow of data pre processing:\n",
    "\n",
    "1. Columns name should be shared with the researcher\n",
    "2. Data format file to be filled by clinicians.\n",
    "3. Specify if missing data are allowed for a given columns (Exception). The file will be used for data verification during FL pre-processing,\n",
    "4. Outlier verification for quantitative data, continuous and discrete, and for dates (Critical warning),\n",
    "5. Missing data imputation by local mean (or optional NN), or majority voting for discrete labels. Give warnings when missing data are found (for verification a posteriori).\n",
    "6. Give critical warning when too many missing are found (>50%),\n",
    "7. Verify that number of available data is greater then minimum required (Error)\n",
    "\n",
    "Critical warnings have different levels of disclosure to the researcher (1) only the warning, 2) type of warning, 3) type of warning and column affected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b334c9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytable\n",
      "  Downloading prettytable-2.4.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: wcwidth in /home/ybouilla/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages (from prettytable) (0.2.5)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ae4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. load  a single view dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import csv\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union, Dict, Any, Iterator, Optional\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17059269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "\n",
    "class ExcelSignatures(Enum):\n",
    "    XLSX = (b'\\x50\\x4B\\x05\\x06', 2, -22, 4)\n",
    "    LSX1 = (b'\\x09\\x08\\x10\\x00\\x00\\x06\\x05\\x00', 0, 512, 8)\n",
    "    LSX2 = (b'\\x09\\x08\\x10\\x00\\x00\\x06\\x05\\x00', 0, 1536, 8)\n",
    "    LSX3 = (b'\\x09\\x08\\x10\\x00\\x00\\x06\\x05\\x00', 0, 2048, 8)\n",
    "    \n",
    "    def __init__(self, sig, whence, offset, size):\n",
    "        self._sig = sig\n",
    "        self._whence = whence\n",
    "        self._offset = offset\n",
    "        self._size = size\n",
    "\n",
    "    @property \n",
    "    def signature(self) -> bytes:\n",
    "        return self._sig\n",
    "    \n",
    "    @property\n",
    "    def whence(self) -> int:\n",
    "        return self._whence\n",
    "    \n",
    "    @property\n",
    "    def offset(self) -> int:\n",
    "        return self._offset\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return self._size\n",
    "\n",
    "\n",
    "\n",
    "def load_tabular_datasets(path:str) -> Dict[str, pd.DataFrame]:\n",
    "    tabular_datasets = {}\n",
    "\n",
    "    if os.path.isdir(path):\n",
    "        print('directory found')\n",
    "        _is_folder = True\n",
    "        \n",
    "        _tabular_data_files = os.listdir(path)\n",
    "    else:\n",
    "        print('file found')\n",
    "        _is_folder = False\n",
    "        _tabular_data_files = (path,)\n",
    "        \n",
    "    for tabular_data_file in _tabular_data_files:\n",
    "        if _is_folder:\n",
    "            tabular_data_file = os.path.join(path, tabular_data_file)\n",
    "        \n",
    "        _is_excel = excel_sniffer(tabular_data_file)\n",
    "        _csv_delimiter, _csv_header = csv_sniffer(tabular_data_file)\n",
    "        _view_name = os.path.basename(tabular_data_file)\n",
    "        if _is_excel:\n",
    "            tabular_datasets[_view_name] = load_excel_file(tabular_data_file)\n",
    "        elif _csv_delimiter is not None:\n",
    "            tabular_datasets[_view_name] = load_csv_file(tabular_data_file,\n",
    "                                                               _csv_delimiter, \n",
    "                                                               _csv_header)\n",
    "        else:\n",
    "            print(f'warning: cannot parse {tabular_data_file}: not a tabular data file')\n",
    "        \n",
    "    return tabular_datasets\n",
    "\n",
    "def load_csv_file(path:str, delimiter:str, header:int) -> pd.DataFrame:\n",
    "    try:\n",
    "        dataframe = pd.read_csv(path, delimiter=delimiter, header=header)\n",
    "    except csv.Error as err:\n",
    "        print('err', err, 'in file', path)\n",
    "            \n",
    "    return dataframe\n",
    "\n",
    "#https://stackoverflow.com/questions/23515791/how-to-check-the-uploaded-file-is-csv-or-xls-in-python/23515973\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_excel_file(path:str, sheet_name: Union[str, int]=0) -> pd.DataFrame:\n",
    "    \"\"\"May rely on openpyxl package\"\"\"\n",
    "    #with open(path, 'r') as excl:\n",
    "    #    _c = csv.DictReader(excl, dialect=csv.excel_tab)\n",
    "    #    _delimiter = _c.dialect.delimiter\n",
    "    \n",
    "    dataframe = pd.read_excel(path, sheet_name=sheet_name)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def excel_sniffer(path: str) -> bool:\n",
    "    \n",
    "    for excel_sig in ExcelSignatures:\n",
    "        with open(path, 'rb') as f:\n",
    "            f.seek(excel_sig.offset, excel_sig.whence)\n",
    "            bytes = f.read(excel_sig.size)\n",
    "\n",
    "            if bytes == excel_sig.signature:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "\n",
    "def csv_sniffer(path:str) :\n",
    "        \n",
    "    with open(path, 'r') as csvfile:\n",
    "        try:\n",
    "            # do some operation on file using sniffer to make sure considered file\n",
    "            # is a CSV file\n",
    "            dialect = csv.Sniffer().sniff(csvfile.readline())\n",
    "            delimiter = dialect.delimiter\n",
    "            dialect.lineterminator\n",
    "            has_header = csv.Sniffer().has_header(csvfile.readline())\n",
    "            if has_header:\n",
    "                header = 0\n",
    "            else:\n",
    "                header = None\n",
    "        except (csv.Error, UnicodeDecodeError) as err:\n",
    "            delimiter, header = None, None\n",
    "            print('err', err, 'in file', path)\n",
    "    return delimiter, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "750c4052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /home/ybouilla/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages (3.0.9)\r\n",
      "Requirement already satisfied: et-xmlfile in /home/ybouilla/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages (from openpyxl) (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d6750dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file found\n",
      "err 'utf-8' codec can't decode byte 0x8c in position 15: invalid start byte in file ../../Exceltest.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Exceltest.xlsx':    ID   Age Eligibility\n",
       " 0    1   45           Y\n",
       " 1    2   45           Y\n",
       " 2    3   33           N\n",
       " 3    4   54           Y\n",
       " 4    5   45           Y\n",
       " 5    6   54         NaN\n",
       " 6    7   34           N\n",
       " 7    8   54         NaN\n",
       " 8    9   45         NaN\n",
       " 9   10   44           Y}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_tabular_datasets('../../Exceltest.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470a1d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file found\n"
     ]
    }
   ],
   "source": [
    "single_view_dataset = load_tabular_datasets(r'/user/ybouilla/home/Documents/data/pseudo_adni_mod/pseudo_adni_mod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed673dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file1':      a   e   i   o      0      1      2      3                 time  pressure  \\\n",
       " 0   48  98  65   5  False   True  False  False  2018-01-01 00:00:00  0.088082   \n",
       " 1   87  83  13  70   True  False   True  False  2018-01-01 01:00:00  0.774788   \n",
       " 2   46  73  81  96  False  False  False   True  2018-01-01 02:00:00  0.514092   \n",
       " 3   84  45  81  39  False   True   True   True  2018-01-01 03:00:00  0.832881   \n",
       " 4   94  84   0  15  False   True  False  False  2018-01-01 04:00:00  0.696152   \n",
       " ..  ..  ..  ..  ..    ...    ...    ...    ...                  ...       ...   \n",
       " 95  14  66  25  64   True   True   True  False  2018-01-04 23:00:00  0.295578   \n",
       " 96  91  81  48  53  False  False   True   True  2018-01-05 00:00:00  0.474322   \n",
       " 97  15  82  12  51   True   True   True   True  2018-01-05 01:00:00  0.927511   \n",
       " 98  51  18   4  52  False  False   True   True  2018-01-05 02:00:00  0.494798   \n",
       " 99  51  70  63  77  False  False   True  False  2018-01-05 03:00:00  0.316395   \n",
       " \n",
       "         sp02  a.1  e.1  i.1  o.1 gender blood type                  pkey  \n",
       " 0   0.360430   90   63   60    8    MAN          A  zmixzrgvxrjqxoe sluk  \n",
       " 1   0.072426   15   20   45   10    MAN          O  vrzahnpfluspdcbfnaqt  \n",
       " 2   0.538551   30    2    6   22  WOMAN          A  pnrepvmrxqabdlvisclv  \n",
       " 3   0.761962   42   70    7   79  WOMAN         AB  gwj luzejwdxzsiljxzd  \n",
       " 4   0.389385   65   90   12   90    MAN          B  jjdvcnofivbqhirxzdyo  \n",
       " ..       ...  ...  ...  ...  ...    ...        ...                   ...  \n",
       " 95  0.027930   13   41   93   19  WOMAN          A  hrvepmqjn llgbzplshv  \n",
       " 96  0.545006   91   41   99   71  WOMAN          B  wroevwyuamxibzshlxxh  \n",
       " 97  0.059205   38    7   73   47    MAN          B  ywadcykylymkdtzfctpg  \n",
       " 98  0.125428   38   11   97    3    MAN          O  ruchbfa zwgenxslegrl  \n",
       " 99  0.772293   49   74   36   42  WOMAN         AB  zeapltpxuvuibfybxcll  \n",
       " \n",
       " [100 rows x 18 columns],\n",
       " 'contatct':     discrete       city                  pkey\n",
       " 0       64.0      Lille  qpqorfhylu gmfjy bdj\n",
       " 1       26.0      Lille  kkmjozalfyirgsire ui\n",
       " 2       61.0      Paris  ezfasuuycdda foisjte\n",
       " 3       29.0      Paris  faxiqkt xggzmwzoidbg\n",
       " 4       99.0      Lille  znwhlj rwzdutnagwasy\n",
       " ..       ...        ...                   ...\n",
       " 95       9.0      Paris  zeqhcikzdodus jn qjf\n",
       " 96      98.0  Marseille  iicthcvfmkajbvr gzir\n",
       " 97      21.0      Lille  ztjakcsk bhjoksdz lm\n",
       " 98      42.0  Marseille   sabunaa opt vpulnxj\n",
       " 99       3.0      Paris  qmbexyexvgromrm admu\n",
       " \n",
       " [100 rows x 3 columns],\n",
       " 'file2':         0      1      2      3                 time        pH  \\\n",
       " 0   False   True  False   True  2018-01-01 00:00:00  0.023107   \n",
       " 1    True  False  False  False  2018-01-01 01:00:00       NaN   \n",
       " 2   False   True  False   True  2018-01-01 02:00:00  0.407279   \n",
       " 3    True   True   True   True  2018-01-01 03:00:00  0.536301   \n",
       " 4   False   True   True   True  2018-01-01 04:00:00  0.749443   \n",
       " ..    ...    ...    ...    ...                  ...       ...   \n",
       " 95   True   True   True  False  2018-01-04 23:00:00       NaN   \n",
       " 96   True  False  False  False  2018-01-05 00:00:00  0.388389   \n",
       " 97  False   True   True  False  2018-01-05 01:00:00  0.889067   \n",
       " 98   True   True  False  False  2018-01-05 02:00:00  0.402979   \n",
       " 99   True  False   True  False  2018-01-05 03:00:00  0.667349   \n",
       " \n",
       "                     pkey  \n",
       " 0   kkmjozalfyirgsire ui  \n",
       " 1   xkdawggpnuulcewuoyzz  \n",
       " 2   khuulhwgwnjggrfoefce  \n",
       " 3   xxysdmwwmjsmyhaswfdb  \n",
       " 4   ldejfuij mnbnf wwmms  \n",
       " ..                   ...  \n",
       " 95  wrmdecb s pohtmrcdj   \n",
       " 96  whmwrpvqmerdpwwzxasf  \n",
       " 97  pnrepvmrxqabdlvisclv  \n",
       " 98  iicthcvfmkajbvr gzir  \n",
       " 99  kiejdmbuih awhuifwwd  \n",
       " \n",
       " [100 rows x 7 columns]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_view_dataframe =  load_tabular_datasets('test7')\n",
    "multi_view_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d533e99",
   "metadata": {},
   "source": [
    "Data format file to be filled by clinicians (step 2 int he workflow):\n",
    "\n",
    "Data format file will be a dictionary specifying the type: \n",
    "* for single view datasets:\n",
    "```{<feature_name>: {'data_type': <data_type>, 'type':<values_taken>, 'range': <value_range>}```\n",
    " * for multiview datatset\n",
    "```{{<view_name>: <feature_name>: {'data_type': <data_type>, 'type':<values_taken>, 'range': <value_range>}}```\n",
    "\n",
    "where\n",
    "* `<view_name>` is the name of the view\n",
    "* `<feature_name>` is the name of the feature\n",
    "* `<data_type>` can be categorical or continuous or missing_data or datetime\n",
    "* `<value_taken>` is the type of the value (eg int, char, float, signed, unsigned ...)\n",
    "* `<value_range>` represent either a list of bounds, an upper or a lower bound, or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "134b8aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create data format file\n",
    "\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "import enum\n",
    "import datetime\n",
    "\n",
    "# the use of Enum classes will prevent incorrect combination of values\n",
    "class QuantitativeDataType(Enum):\n",
    "    CONTINUOUS = [float, np.float64]\n",
    "    DISCRETE = [int, np.int64]\n",
    "\n",
    "class CategoricalDataType(Enum):\n",
    "    BOOLEAN = [bool]\n",
    "    NUMERICAL = [float, int, np.float64, np.int64]\n",
    "    CHARACTER = [str, object]\n",
    "    \n",
    "class KeyDataType(Enum):\n",
    "    NUMERICAL = [int, np.int64]\n",
    "    CHARACTER = [str, object]\n",
    "    DATETIME = [pd.Timestamp,\n",
    "                pd.Timedelta,\n",
    "                pd.Period,\n",
    "                datetime.datetime,\n",
    "                np.datetime64]\n",
    "\n",
    "\n",
    "class CustomDataType(Enum):\n",
    "    \"\"\"for demo purpose: here a custom datatype\"\"\"\n",
    "    DISCRETE = [int, np.int64]\n",
    "    CHARACTER = [str, object]\n",
    "    \n",
    "    \n",
    "class DataType(Enum):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # what about KEY = KeyDataType()\n",
    "    KEY = [KeyDataType.NUMERICAL,\n",
    "           KeyDataType.CHARACTER,\n",
    "           KeyDataType.DATETIME]\n",
    "    QUANTITATIVE = [QuantitativeDataType.CONTINUOUS,\n",
    "                   QuantitativeDataType.DISCRETE]\n",
    "    CATEGORICAL = [CategoricalDataType.BOOLEAN,\n",
    "                  CategoricalDataType.NUMERICAL,\n",
    "                  CategoricalDataType.CHARACTER]\n",
    "    #MISSING = 'MISSING'\n",
    "    #DATETIME = 'DATETIME'\n",
    "    DATETIME = [pd.Timestamp,\n",
    "                pd.Timedelta,\n",
    "                pd.Period,\n",
    "                datetime.datetime,\n",
    "                np.datetime64]\n",
    "    CUSTOM = [CustomDataType.DISCRETE,\n",
    "             CustomDataType.CHARACTER]\n",
    "    UNKNOWN = 'UNKNOWN'\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_names():\n",
    "        return tuple(n for n, _ in DataType.__members__.items())\n",
    "\n",
    "class MissingValueAllowedDefault(Enum):\n",
    "    KEY = False\n",
    "    QUANTITATIVE = True\n",
    "    CATEGORICAL = True\n",
    "    DATETIME = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_names():\n",
    "        return tuple(n for n, _ in MissingValueAllowedDefault.__members__.items())\n",
    "    \n",
    "    \n",
    "class DataTypeProperties(Enum):\n",
    "    \"\"\"Data Type possible modification (when editing)\"\"\"\n",
    "    CATEGORICAL = (False, False, True, False)\n",
    "    QUANTITATIVE = (True, True, False, False)\n",
    "    DATETIME = (True, True, False, True)\n",
    "    UNKNOWN = (False, False, False, False)\n",
    "    CUSTOM = (True, True, True, False)\n",
    "    \n",
    "    def __init__(self,\n",
    "                 lower_bound: bool,\n",
    "                 upper_bound: bool,\n",
    "                 set_of_values: bool,\n",
    "                 date_format:bool):\n",
    "        self._lower_bound = lower_bound\n",
    "        self._upper_bound = upper_bound\n",
    "        self._set_of_values = set_of_values\n",
    "        self._date_format = date_format\n",
    "        \n",
    "    @property\n",
    "    def lower_bound(self):\n",
    "        return self._lower_bound\n",
    "    \n",
    "    @property\n",
    "    def upper_bound(self):\n",
    "        return self._upper_bound\n",
    "    \n",
    "    @property\n",
    "    def set_of_values(self):\n",
    "        return self._set_of_values\n",
    "    \n",
    "    @property\n",
    "    def date_format(self):\n",
    "        return self._date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35b2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc4f0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_type(\n",
    "                  #avail_data_types: enum.EnumMeta,\n",
    "                  d_format: Enum,\n",
    "                  d_type: type) ->  Tuple[Enum, List[Union[type, str]]]:\n",
    "    present_d_types = []\n",
    "    sub_d_type_format = None\n",
    "    for avail_data_type in DataType:\n",
    "        if d_format is avail_data_type:\n",
    "            sub_dtypes = avail_data_type.value\n",
    "            if not isinstance(sub_dtypes, str) and hasattr(sub_dtypes, '__getitem__') and isinstance(sub_dtypes[0], Enum):\n",
    "                # check if dtype has subtypes\n",
    "                #(eg if datatype is QUANTITATIVE, subtype will be CONTINOUS or DISCRETE)\n",
    "                for sub_dtype in sub_dtypes:\n",
    "                    if any(d_type == t for t in tuple(sub_dtype.value)):\n",
    "                        present_d_types.append(d_type)\n",
    "                        sub_d_type_format = sub_dtype\n",
    "                        print(sub_dtype, d_type)\n",
    "            else:\n",
    "                # case where datatype doesnot have subtypes, eg DATETIME\n",
    "                if any(d_type == t for t in sub_dtypes):\n",
    "                    present_d_types.append(d_type)\n",
    "                sub_d_type_format = d_format\n",
    "    print(sub_d_type_format, '|', present_d_types)\n",
    "    return  sub_d_type_format, present_d_types\n",
    "\n",
    "\n",
    "def find_data_type(data_format_name: str, data_type_name: str=None) -> List[Enum]:\n",
    "    \"\"\"Retrieves from a given data_format and data_type,\n",
    "    the corresponding Enum class describing data\"\"\"\n",
    "    \n",
    "    ## varible initialisation\n",
    "    data_type = []\n",
    "    _is_data_format_unrecognized = True\n",
    "    _is_data_type_unrecognized = True\n",
    "    \n",
    "    _available_data_types = [t for t in DataType]\n",
    "    \n",
    "    for a_data_type in _available_data_types:\n",
    "        if data_format_name == a_data_type.name:\n",
    "            _is_data_format_unrecognized = False\n",
    "            data_type.append(a_data_type)\n",
    "            \n",
    "            for sub_type in a_data_type.value:\n",
    "                \n",
    "                if data_type_name is not None and  not isinstance(sub_type, str):\n",
    "                    # check if sub data type exist (it shouldnot if variable is UNKNOWN)\n",
    "                    if data_type_name == sub_type.name:\n",
    "                        \n",
    "                        _is_data_type_unrecognized = False\n",
    "                        data_type.append(sub_type)\n",
    "                else:\n",
    "                    _is_data_type_unrecognized = False\n",
    "    # check for data formt file consistancy error\n",
    "    if any((_is_data_format_unrecognized, _is_data_type_unrecognized)):\n",
    "        if _is_data_format_unrecognized:\n",
    "            print(f'error: {data_format_name} not recognized as a valid data type')\n",
    "        else:\n",
    "            print(f'error {data_type_name} not recognized as a valid data type')\n",
    "            \n",
    "    return data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee00bd34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "691fe901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_missing_data(column: pd.Series)->bool:\n",
    "    is_missing_data = column.isna().any()\n",
    "    return is_missing_data\n",
    "df = pd.DataFrame({'w': [1, 2, 3, 4,  'jj', None]})\n",
    "print(check_missing_data(df['w']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f354b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "936ac8f0",
   "metadata": {},
   "source": [
    "CLI details:\n",
    "\n",
    "1. open  csv file\n",
    "2. for each columns in file ask type of variable or if variable should be excluded\n",
    "3. automatically detect the type given values in columns \n",
    "4. ask for each columns if missing data are allowed\n",
    "\n",
    "\n",
    "eg :\n",
    "\n",
    "assume a column is of type discrete with integers\n",
    "\n",
    "\n",
    "1. user select it is quantitative\n",
    "2. then system will label it as quantitative-discrete\n",
    "\n",
    "\n",
    "**Question** : do we want an auto selection parameter choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d328b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yes_no_msg() -> str:\n",
    "    msg_yes_or_no_question = '1) YES\\n2) NO\\n'   \n",
    "    return msg_yes_or_no_question\n",
    "\n",
    "def parse_yes_no_msg(resp: str) -> bool:\n",
    "    \"\"\"implements logic to parse yes or no msg\"\"\"\n",
    "    yes_or_no_question_key = {'1': True,\n",
    "                    '2': False}\n",
    "    return yes_or_no_question_key.get(resp)\n",
    "\n",
    "def get_data_type_selection_msg(available_data_type:List[Enum]) ->Tuple[str, int]:\n",
    "    \n",
    "    n_available_data_type = len(available_data_type)\n",
    "    msg = ''\n",
    "\n",
    "    \n",
    "    for i, dtype in enumerate(available_data_type):\n",
    "        msg += '%d) %s \\n' %  (i+1, dtype.name)\n",
    "    \n",
    "    ignoring_key = i+2  # add ingoring entry\n",
    "    msg += '%d) ignore this column\\n' % (ignoring_key)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return msg, ignoring_key\n",
    "\n",
    "def unique(iterable: Iterator) -> int:\n",
    "    \"\"\"returns number of unique values\"\"\"\n",
    "    return len(set(iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "800e755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI for clinicians for setting up data format file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b6238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_from_user_dataframe_format_file(dataset: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \n",
    "    dataset_columns = dataset.columns\n",
    "    dataset_columns_length = len(dataset_columns)\n",
    "    data_format_file = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    available_data_type = [d_type for d_type in DataType]  # get all available data types\n",
    "    \n",
    "    for n_feature, feature in enumerate(dataset_columns):\n",
    "        print(f'displaying first 10 values of feature {feature} (n_feature: {n_feature+1}/{dataset_columns_length})')\n",
    "        #print(tabulate(dataset[feature].head(10).values()))\n",
    "        pprint.pprint(dataset[feature].head(10))  # print first 10 lines of feature value\n",
    "        print(f'number of differents samples: {unique(dataset[feature])} / total of samples: {dataset[feature].shape[0]}')\n",
    "        \n",
    "        msg_data_type_selection, ignoring_id = get_data_type_selection_msg(available_data_type)\n",
    "        msg_data_type_selection = f'specify data type for {feature}:\\n' + msg_data_type_selection\n",
    "        \n",
    "        # ask user about data type\n",
    "        data_format_id = get_user_input(msg_data_type_selection,\n",
    "                                       \n",
    "                                       n_answers=dataset_columns_length+1)\n",
    "        \n",
    "        if int(data_format_id) > ignoring_id - 1:\n",
    "            # case where user decide to ingore column: go to next iteration (next feature)\n",
    "            print(f\"Ignoring feature {feature}\")\n",
    "            continue\n",
    "        else:\n",
    "            # case where user selected a data type: add data type and info to the format file\n",
    "            data_format = available_data_type[int(data_format_id)-1]\n",
    "            data_type = dataset[feature].dtype\n",
    "            n_data_type, types = get_data_type(data_format, data_type)\n",
    "            \n",
    "        # KEY and DATETIME type \n",
    "        if data_format is DataType.KEY or data_format is DataType.DATETIME:  \n",
    "            is_missing_values_allowed = False\n",
    "        else: \n",
    "            # ask user if missing values are allowed for this specific variable\n",
    "            msg_yes_or_no_question = get_yes_no_msg()\n",
    "            msg_yes_or_no_question = f'Allow {feature} to have missing values:\\n' + msg_yes_or_no_question\n",
    "            missing_values_user_selection = get_user_input(msg_yes_or_no_question,\n",
    "                                                        n_answers=2)\n",
    "            is_missing_values_allowed = parse_yes_no_msg(missing_values_user_selection)\n",
    "            \n",
    "        if hasattr(n_data_type, 'name'):\n",
    "            name_data_type = n_data_type.name\n",
    "        else:\n",
    "            name_data_type = n_data_type[0]\n",
    "        data_format_file[feature] = {'data_format': data_format.name,\n",
    "                                     'data_type': n_data_type.name,\n",
    "                                     'values': str(data_type),\n",
    "                                     'is_missing_values': is_missing_values_allowed}\n",
    "            \n",
    "    return data_format_file\n",
    "            \n",
    "def get_user_input(msg:str,  n_answers:int) -> str:\n",
    "    \"\"\"\"\"\"\n",
    "    is_column_parsed = False\n",
    "    while not is_column_parsed:\n",
    "        #data_format_id = input(f'specify data type for {feature}:\\n' + msg )\n",
    "        resp = input(msg)\n",
    "        if resp.isdigit() and int(resp) <= n_answers and int(resp)>0:\n",
    "            # check if value passed by user is correct (if it is integer,\n",
    "            # and whithin range [1, n_available_data_type])\n",
    "            is_column_parsed = True\n",
    "\n",
    "        else:\n",
    "            print(f'error ! {resp} value not understood')\n",
    "            \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "185958c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLI to use when dataset is available\n",
    "\n",
    "\n",
    "def get_from_user_multi_view_dataset_fromat_file(datasets: Dict[str, pd.DataFrame])-> Dict[str, pd.DataFrame]:\n",
    "    \n",
    "    data_format_files = {}\n",
    "    \n",
    "    for tabular_data_file in datasets.keys():\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        print(f\"+++++++ Now parsing view: {tabular_data_file} +++++++\")\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        data_format_file = get_from_user_dataframe_format_file(datasets[tabular_data_file])\n",
    "        \n",
    "        _file_name = os.path.basename(tabular_data_file)\n",
    "        data_format_files[_file_name] = data_format_file\n",
    "        \n",
    "    return data_format_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8acbe629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "+++++++ Now parsing view: pseudo_adni_mod.csv +++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "displaying first 10 values of feature CDRSB.bl (n_feature: 1/16)\n",
      "0    1\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    1\n",
      "6    4\n",
      "7    0\n",
      "8    3\n",
      "9    2\n",
      "Name: CDRSB.bl, dtype: int64\n",
      "number of differents samples: 8 / total of samples: 1000\n",
      "specify data type for CDRSB.bl:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) CUSTOM \n",
      "6) UNKNOWN \n",
      "7) ignore this column\n",
      "6\n",
      "DataType.UNKNOWN | []\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44649/552618136.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_format_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_from_user_multi_view_dataset_fromat_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_view_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_44649/3231818315.py\u001b[0m in \u001b[0;36mget_from_user_multi_view_dataset_fromat_file\u001b[0;34m(datasets)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"+++++++ Now parsing view: {tabular_data_file} +++++++\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdata_format_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_from_user_dataframe_format_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtabular_data_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0m_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtabular_data_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_44649/930600104.py\u001b[0m in \u001b[0;36mget_from_user_dataframe_format_file\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mmsg_yes_or_no_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_yes_no_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mmsg_yes_or_no_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'Allow {feature} to have missing values:\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmsg_yes_or_no_question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             missing_values_user_selection = get_user_input(msg_yes_or_no_question,\n\u001b[0m\u001b[1;32m     43\u001b[0m                                                         n_answers=2)\n\u001b[1;32m     44\u001b[0m             \u001b[0mis_missing_values_allowed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_yes_no_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values_user_selection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_44649/930600104.py\u001b[0m in \u001b[0;36mget_user_input\u001b[0;34m(msg, n_answers)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_column_parsed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m#data_format_id = input(f'specify data type for {feature}:\\n' + msg )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn_answers\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# check if value passed by user is correct (if it is integer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             )\n\u001b[0;32m-> 1006\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "data_format_file = get_from_user_multi_view_dataset_fromat_file(single_view_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fec5f1",
   "metadata": {},
   "source": [
    "data_fromat_ref (read only)\n",
    "\n",
    "CLI editer data_format_file\n",
    "\n",
    "review : \n",
    "- specify lower / upper bound NUMERICAL\n",
    "- Specify categorical (BOOLEAN, CHARACTER, NUMERICAL)\n",
    "\n",
    "- save different categorical values \n",
    " a posteriori ex SEX -> male or female, NOT FEMALE\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b874ee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multi_view_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44649/3532817470.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmulti_data_format_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_from_user_multi_view_dataset_fromat_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_view_dataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'multi_view_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "multi_data_format_file = get_from_user_multi_view_dataset_fromat_file(multi_view_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0d2102a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1': {'e': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  '1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': False},\n",
       "  '2': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': True},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pressure': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'e.1': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'gender': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'blood type': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': True},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'contatct': {'discrete': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'city': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'file2': {'1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': False},\n",
       "  'time': {'data_format': 'DATETIME',\n",
       "   'data_type': 'DATETIME',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False},\n",
       "  'pH': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'pkey': {'data_format': 'KEY',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_data_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1e57d180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully saved at multi_format_file\n"
     ]
    }
   ],
   "source": [
    "save_format_file_ref(multi_data_format_file, 'multi_format_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76cb0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data format file\n",
    "\n",
    "json_file_name = \"format_file_ref\"\n",
    "\n",
    "with open(json_file_name, \"w\") as format_file:\n",
    "    json.dump(data_format_file, format_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f8fef1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_format_file_ref(format_file_ref: Dict[str, Dict[str, Any]], path: str):\n",
    "    # save `format_file_ref` into a JSON file\n",
    "    with open(path, \"w\") as format_file:\n",
    "        json.dump(format_file_ref, format_file)\n",
    "    print(f\"Model successfully saved at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b2f6451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_format_file_ref(path: str) -> Dict[str, Dict[str, Any]]:\n",
    "    # retrieve data format file\n",
    "    with open(path, \"r\") as format_file:\n",
    "        format_file_ref = json.load(format_file)\n",
    "    return format_file_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c36e3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_file_name = \"format_file_ref\"\n",
    "\n",
    "\n",
    "format_file = load_format_file_ref(json_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "82331c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudo_adni_mod.csv': {'CDRSB.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'ADAS11.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'MMSE.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'RAVLT.immediate.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': True},\n",
       "  'RAVLT.learning.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'FAQ.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'TAU.MEDIAN.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'AGE': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31f18285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1)data_type2)lower bound3)upper bound4) Values taken5) Cancel Operation', 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_msg_action_selection(DataTypeProperties.CUSTOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fca66943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_msg_action_selection(data_type_propreties: Enum) -> Tuple[str, int]:\n",
    "    # create edit selection message for user given data type\n",
    "    msg = \"\"\n",
    "    action_counter = 1\n",
    "    # special case for data_type DATETIME\n",
    "    if data_type_propreties.name != DataType.DATETIME.name:\n",
    "        msg += \"%d)data_type\\n\" % action_counter\n",
    "        action_counter += 1\n",
    "        \n",
    "    if data_type_propreties.lower_bound:\n",
    "        msg += \"%d)lower bound\\n\" % action_counter\n",
    "        action_counter += 1\n",
    "    \n",
    "    if data_type_propreties.upper_bound:\n",
    "        msg += \"%d)upper bound\\n\" % action_counter\n",
    "        action_counter += 1\n",
    "        \n",
    "    if data_type_propreties.set_of_values:\n",
    "        msg += \"%d) Values taken\\n\" % action_counter\n",
    "        action_counter += 1\n",
    "        \n",
    "    if data_type_propreties.date_format:\n",
    "        msg += \"%d) Date format\\n\" % action_counter\n",
    "        action_counter += 1\n",
    "    \n",
    "    msg += \"%d) Cancel Operation\\n\" % action_counter\n",
    "    \n",
    "    return msg, action_counter\n",
    "\n",
    "def select_action(data_format: str,\n",
    "                  action: str,\n",
    "                  available_categorical_data_type: List[Enum],\n",
    "                  msg:str) -> Tuple[Dict[str, Any], bool]:\n",
    "    \n",
    "    # variable initialization\n",
    "    is_cancelled = False\n",
    "    _action_counter = 1  # integer for dynamic cli management\n",
    "    _avail_data_type_properties = [dtype for dtype in DataTypeProperties]\n",
    "    \n",
    "    for data_type_properties in avail_data_type_properties:\n",
    "        if data_format == data_type_properties.name:\n",
    "            # define action\n",
    "            # action for lower bound\n",
    "            if data_type_properties.lower_bound:\n",
    "                pass\n",
    "    if data_format == DataType.CATEGORICAL.name:\n",
    "        \n",
    "        if action == '1':\n",
    "            new_field = ask_for_data_type(available_categorical_data_type,\n",
    "                                          msg)\n",
    "        elif action =='2':\n",
    "            new_field = ask_for_categorical_values()\n",
    "        elif action == '3':\n",
    "            print('operation cancelled')\n",
    "            new_field = None\n",
    "            is_cancelled = True\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            new_field = None\n",
    "    else:\n",
    "        # for QUANTITATIVE, DATETIME and KEY data types\n",
    "        \n",
    "        if action == '1':\n",
    "            new_field =  ask_for_lower_bound()\n",
    "        elif action == '2':\n",
    "            new_field = ask_for_upper_bound()\n",
    "        elif action == '3':\n",
    "            new_field = None\n",
    "            print('operation cancelled')\n",
    "            is_cancelled = True\n",
    "        else:\n",
    "            new_field = None\n",
    "    \n",
    "    return new_field, is_cancelled\n",
    "\n",
    "\n",
    "def isfloat(value:str) ->bool:\n",
    "    \"\"\"checks if string represents a float or int\"\"\"\n",
    "    is_float = True\n",
    "    try:\n",
    "        float(value)\n",
    "    except ValueError as e:\n",
    "        is_float = False\n",
    "    return is_float\n",
    "    \n",
    "        \n",
    "def ask_for_lower_bound() -> Dict[str, float]:\n",
    "    _is_entered_value_correct = False\n",
    "    while not _is_entered_value_correct:\n",
    "        lower_bound = input('enter lower bound')\n",
    "        if isfloat(lower_bound):\n",
    "            # check if entered value is correct (is a numerical value)\n",
    "            _is_entered_value_correct = True\n",
    "    return {'lower_bound': float(lower_bound)}\n",
    "\n",
    "def ask_for_upper_bound() -> Dict[str, float]:\n",
    "    \n",
    "    _is_entered_value_correct = False\n",
    "    while not _is_entered_value_correct:\n",
    "        upper_bound = input('enter upper bound')\n",
    "        if isfloat(upper_bound):\n",
    "            # check if entered value is correct (is a numerical value)\n",
    "            _is_entered_value_correct = True\n",
    "    return {'upper_bound': float(upper_bound)}\n",
    "\n",
    "def ask_for_data_type(\n",
    "                      available_categorical_data_type: List[Enum], \n",
    "                      msg: str) -> Dict[str, Any]:\n",
    "    # edit categorical datatype\n",
    "    data_type_selection = get_user_input(msg, 4)\n",
    "    updates = None\n",
    "    if data_type_selection != '4':\n",
    "        new_data_type = available_categorical_data_type[int(data_type_selection) - 1]\n",
    "        new_values = list(map(lambda x: str(x), new_data_type.value))\n",
    "        updates = {'data_type': new_data_type.name, 'values': new_values}\n",
    "    return updates\n",
    "\n",
    "\n",
    "def ask_for_categorical_values() -> Dict[str, Any]:\n",
    "    possible_values = input('enter possible values (separated by \",\")')\n",
    "    possible_values = possible_values.split(\",\")  # separate values passed by user into a list\n",
    "    return {'categorical_values': possible_values}\n",
    "\n",
    "\n",
    "\n",
    "def edit_feature_format_file_ref(feature_content: Dict[str, Any],\n",
    "                                  feature_name: str,\n",
    "                                  available_categorical_data_type: List[Enum],\n",
    "                                  messages: Dict[str, str],\n",
    "                                  ignore_keystroke: int) -> Dict[str, Any]:\n",
    "    \"\"\"Edits a specific feature that belongs to a specific view within a format file\"\"\"\n",
    "    \n",
    "\n",
    "    _is_feature_unparsed = True  \n",
    "    _is_cancelled = False  # whether parsing of current column has been cancelled or not\n",
    "    _is_first_edit = True\n",
    "    \n",
    "    # iterate over number of feature contained in view, and ask for each feature if changes are needed\n",
    "    while _is_feature_unparsed:\n",
    "        if _is_cancelled or not _is_first_edit:\n",
    "            _f_answer = True\n",
    "        else:\n",
    "            _f_answer = get_user_input(f\"Edit variable: {feature_name}?\\n\" + messages['yes_or_no'], 2)\n",
    "            # ask if user wants to edit feature names\n",
    "            _f_answer = parse_yes_no_msg(_f_answer)\n",
    "            _is_operation_cancelled = False  # for cancelling feature edition\n",
    "            _is_first_edit = False\n",
    "        if _f_answer:\n",
    "            # case where user wants to edit the current feature\n",
    "            \n",
    "            _msg = messages['edit']\n",
    "            if feature_content.get('data_format') == DataType.CATEGORICAL.name:\n",
    "                # case if feature is a categorical variable\n",
    "                 _msg += messages['categorical_edit']\n",
    "            else:\n",
    "                #case if feature is a quantitative variable\n",
    "                _msg += messages['quantitative_edit']\n",
    "                \n",
    "            _msg += messages['ignore']\n",
    "            _edit_selection = get_user_input(_msg, 3)\n",
    "            \n",
    "            _edited_field, _is_cancelled = select_action(feature_content.get('data_format'),\n",
    "                                                          _edit_selection,\n",
    "                                                          available_categorical_data_type,\n",
    "                                                          messages['data_type_select'])\n",
    "            \n",
    "            if not _is_cancelled:\n",
    "                # if user has not cancelled field edition\n",
    "                if _edited_field is not None:\n",
    "                    feature_content.update(_edited_field)\n",
    "             \n",
    "                _c_answer = get_user_input(f\"Continue Editing variable: {feature_name}?\\n\" + messages['yes_or_no'], 2)\n",
    "                _is_feature_unparsed = parse_yes_no_msg(_c_answer)\n",
    "            else:\n",
    "                _is_feature_unparsed = False\n",
    "                \n",
    "        else:\n",
    "            _is_feature_unparsed = False\n",
    "            \n",
    "    return feature_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ed0d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_format_file_ref(format_file_ref: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \n",
    "    # CLI for editing `format_file_ref`, a file containing information about each variable\n",
    "    # in a tabular dataset\n",
    "    print(f'Now editing format file ref')\n",
    "    \n",
    "    ## variables initialization\n",
    "    available_categorical_data_types = [t for t in CategoricalDataType]\n",
    "    _file_names = list(format_file_ref.keys())\n",
    "    _n_tot_files = len(_file_names)\n",
    "    \n",
    "    ## messages definition\n",
    "    _data_type_selection_msg, ign_key = get_data_type_selection_msg(available_categorical_data_types)\n",
    "    \n",
    "    _messages = {\n",
    "        'yes_or_no': get_yes_no_msg(),\n",
    "        'data_type_select':  _data_type_selection_msg,\n",
    "        'edit': 'Which field should be modified?\\n',\n",
    "        'quantitative_edit': '1)lower_bound\\n2)upper_bound\\n',\n",
    "        'categorical_edit': '1)data_type (categorical variable only)\\n2)values taken (categorical variable only)\\n',\n",
    "        'ignore': '3)cancel operation\\n'\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    # iterate over name of files (ie views)\n",
    "    for i_file in range(_n_tot_files):\n",
    "        # ask for each file if user wants to edt it\n",
    "        _answer = get_user_input(f\"Edit file: {_file_names[i_file]}?\\n\" + _messages['yes_or_no'], 2)\n",
    "        _answer = parse_yes_no_msg(_answer)\n",
    "        \n",
    "        if _answer:\n",
    "            # case where user wants to modify current view scheme\n",
    "            _file_content = format_file_ref[_file_names[i_file]]  # get file (ie view) content\n",
    "            \n",
    "            ## variables initialization for parsing current view\n",
    "            _features_names = list(_file_content.keys())\n",
    "            _n_tot_feature = len(_features_names)\n",
    "            \n",
    "            # iterate over features found in view\n",
    "            for i_feature in range(_n_tot_feature):\n",
    "                feature_name = _features_names[i_feature]\n",
    "                feature_content = _file_content[feature_name]\n",
    "                feature_content = edit_feature_format_file_ref(feature_content,\n",
    "                                                               feature_name,\n",
    "                                                               available_categorical_data_types,\n",
    "                                                               _messages,\n",
    "                                                               ign_key)\n",
    "            format_file_ref[_file_names[i_file]].update({feature_name: feature_content})\n",
    "            \n",
    "    return format_file_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ef36608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudo_adni_mod.csv': {'CDRSB.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': [\"<class 'bool'>\"],\n",
       "   'is_missing_values': True,\n",
       "   'categorical_values': ['1', ' 2', ' 4']},\n",
       "  'ADAS11.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'MMSE.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'RAVLT.immediate.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': True},\n",
       "  'RAVLT.learning.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'FAQ.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False,\n",
       "   'lower_bound': 0.0},\n",
       "  'TAU.MEDIAN.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'AGE': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95895e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_format_file_ref(format_file)\n",
    "\n",
    "# TODO: create possiblity for user to update data type when editing format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f519aafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now editing format file ref\n",
      "Edit file: file1?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Edit variable: a?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Which field should be modified?\n",
      "1)lower_bound\n",
      "2)upper_bound\n",
      "3)cancel operation\n",
      "1\n",
      "enter lower bound30\n",
      "Continue Editing variable: a?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Which field should be modified?\n",
      "1)lower_bound\n",
      "2)upper_bound\n",
      "3)cancel operation\n",
      "2\n",
      "enter upper bound2\n",
      "Continue Editing variable: a?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: 0?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: pressure?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Which field should be modified?\n",
      "1)lower_bound\n",
      "2)upper_bound\n",
      "3)cancel operation\n",
      "3\n",
      "operation cancelled\n",
      "Edit variable: sp02?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: i.1?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: gender?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "Which field should be modified?\n",
      "1)data_type (categorical variable only)\n",
      "2)values taken (categorical variable only)\n",
      "3)cancel operation\n",
      "2\n",
      "enter possible values (separated by \",\")MALE, FEMALE\n",
      "Continue Editing variable: gender?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit variable: blood type?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit file: contatct?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "Edit file: file2?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'file1': {'a': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True,\n",
       "   'lower_bound': 30.0,\n",
       "   'upper_bound': 2.0},\n",
       "  '0': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': True},\n",
       "  'pressure': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'sp02': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'i.1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'gender': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': True,\n",
       "   'categorical_values': ['MALE', ' FEMALE']},\n",
       "  'blood type': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': False}},\n",
       " 'contatct': {'discrete': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'city': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'CHARACTER',\n",
       "   'values': 'object',\n",
       "   'is_missing_values': True}},\n",
       " 'file2': {'1': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'BOOLEAN',\n",
       "   'values': 'bool',\n",
       "   'is_missing_values': True},\n",
       "  'pH': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': True}}}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_format_file_ref = load_format_file_ref('multi_format_file')\n",
    "edit_format_file_ref(multi_format_file_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ba372",
   "metadata": {},
   "source": [
    "## tabular data sanity check using file format ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2a9c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions for multi view dataframe\n",
    "def rename_variables_before_joining(multi_view_datasets: Dict[str, pd.DataFrame],\n",
    "                                    views_name: List[Union[str, int]],\n",
    "                                    primary_key:Union[str, int]=None) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Renames variables that have same name but different views using the following naming convention:\n",
    "    if `a` is the name of a feature of `view1` and `a` is the name of a feature of `view2`,\n",
    "    features names will be updated into `view1.a` and `view2.a`\n",
    "    \"\"\"\n",
    "    _features_names = {}\n",
    "    _views_length = len(views_name)\n",
    "    \n",
    "    # parse every combination once\n",
    "    for i_left in range(0, _views_length-1):\n",
    "        _left_view = views_name[i_left]\n",
    "        _left_features_name = multi_view_datasets[_left_view].columns.tolist()\n",
    "        for i_right in range(i_left+1, _views_length):\n",
    "        \n",
    "            _right_view = views_name[i_right]\n",
    "            _right_features_name = multi_view_datasets[_right_view].columns.tolist()\n",
    "            \n",
    "            for _f in _left_features_name:\n",
    "                if primary_key and _f == primary_key:\n",
    "                    # do not affect primary key (if any)\n",
    "                    continue\n",
    "                if _f  in _right_features_name:\n",
    "                    \n",
    "                    if _left_view  not in _features_names:\n",
    "                        _features_names[_left_view] = {}\n",
    "                        \n",
    "                    if _right_view not in _features_names:\n",
    "                        _features_names[_right_view] = {}\n",
    "                        \n",
    "                    _features_names[_left_view].update({_f: _left_view + '.' + str(_f)})\n",
    "                    _features_names[_right_view].update({_f: _right_view + '.' + str(_f)})\n",
    "    \n",
    "    for i in range(_views_length):\n",
    "        _view = views_name[i]\n",
    "        _new_features = _features_names.get(_view)\n",
    "        if _new_features:\n",
    "            multi_view_datasets[_view] = multi_view_datasets[_view].rename(columns=_new_features)\n",
    "        \n",
    "    \n",
    "    return multi_view_datasets\n",
    "\n",
    "\n",
    "def create_multi_view_dataframe(datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    _header_labels = ['views', 'feature_name']\n",
    "    # 1. create multiindex header\n",
    "\n",
    "    _feature_name_array = np.array([])  # store all feature names\n",
    "    _view_name_array = []  # store all views (ie modalities) names\n",
    "\n",
    "    _concatenated_datasets = np.array([])  # store dataframe values\n",
    "\n",
    "    for key in datasets.keys():\n",
    "        _feature_name_array = np.concatenate([_feature_name_array,\n",
    "                                              datasets[key].columns.values])\n",
    "        if len(_concatenated_datasets) <= 0:\n",
    "            # first pass \n",
    "            _concatenated_datasets = datasets[key].values\n",
    "        else:\n",
    "            # next passes\n",
    "            try:\n",
    "                _concatenated_datasets = np.concatenate(\n",
    "                                        [_concatenated_datasets,\n",
    "                                         datasets[key].to_numpy()\n",
    "                                         ], axis=1)\n",
    "            except ValueError as val_err:\n",
    "                # catching case where nb_samples are differents\n",
    "                raise ValueError(\n",
    "                    'Cannot create multi view dataset: different number of samples for each modality have been detected'\\\n",
    "                        + 'Details: ' + str(val_err)\n",
    "                    )\n",
    "        for _ in datasets[key].columns.values:\n",
    "            _view_name_array.append(key)\n",
    "\n",
    "    _header = pd.MultiIndex.from_arrays([_view_name_array,\n",
    "                                         _feature_name_array],\n",
    "                                        names=_header_labels)\n",
    "\n",
    "\n",
    "    # 2. create multi index dataframe\n",
    "\n",
    "    multi_view_df = pd.DataFrame(_concatenated_datasets,\n",
    "                                  columns = _header)\n",
    "    return multi_view_df\n",
    "\n",
    "\n",
    "def join_muti_view_dataset(multi_view_dataset: pd.DataFrame,\n",
    "                           primary_key: str=None) -> pd.DataFrame:\n",
    "    \"\"\"Concatenates a multi view dataset into a plain pandas dataframe,\n",
    "    by doing a join operation along specified primary_key\"\"\"\n",
    "    _views_name = sorted(set(multi_view_dataset.columns.get_level_values(0)))  # get views name\n",
    "    \n",
    "    joined_dataframe = multi_view_dataset[_views_name[0]]  # retrieve the first view\n",
    "    # (as a result of join operation)\n",
    "    for x in range(1, len(_views_name)):\n",
    "        joined_dataframe = joined_dataframe.merge(multi_view_dataset[_views_name[x]],\n",
    "                                                    on=primary_key,\n",
    "                                                    suffixes=('', '.'+_views_name[x]))\n",
    "        \n",
    "        #df['file1'].join(df['file2'].set_index('pkey'), on='pkey', rsuffix='.file2')\n",
    "        \n",
    "    return joined_dataframe\n",
    "\n",
    "\n",
    "def search_primary_key(format_file_ref: Dict[str, Dict[str, Any]]) -> Optional[str]: \n",
    "    \"\"\"\"\"\"\n",
    "    _views_names = list(format_file_ref.keys())\n",
    "    primary_key = None\n",
    "    _c_view = None\n",
    "    for view_name in _views_names:\n",
    "        file_content = format_file_ref[view_name]\n",
    "        _features_names = list(file_content.keys())\n",
    "        for feature_name in _features_names:\n",
    "            feature_content  = file_content[feature_name]\n",
    "            _d_format = feature_content.get('data_format')\n",
    "            \n",
    "            if _d_format == DataType.KEY.name:\n",
    "                if _c_view is None:\n",
    "                    primary_key = feature_name\n",
    "                    _c_view = view_name\n",
    "                    print(f'found primary key {primary_key}')\n",
    "                else:\n",
    "                    print(f'error: found 2 primary keys is same view {view_name}')\n",
    "        _c_view = None\n",
    "    return primary_key\n",
    "\n",
    "\n",
    "\n",
    "def select_data_from_format_file_ref(datasets: Dict[str, Dict[str, Any]],\n",
    "                                     format_file: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"returns an updated dataset containing only the features detailed in format_file\"\"\"\n",
    "    # variables initialisation\n",
    "    \n",
    "    updated_dataset = {}\n",
    "    _views_format_file = list(format_file.keys())\n",
    "    \n",
    "    for view in _views_format_file:\n",
    "        if view in datasets.keys():\n",
    "            # only extract features from format_file\n",
    "            _format_file_features = list(format_file[view].keys())\n",
    "            _current_dataset_feature = datasets[view].columns.tolist()\n",
    "            try:\n",
    "                updated_dataset[view] = datasets[view][_format_file_features]\n",
    "            except KeyError as ke:\n",
    "                # catch error if a column is specified in data format file\n",
    "                # but not found in dataset\n",
    "                _missing_feature = []\n",
    "                for feature in _format_file_features:\n",
    "                    if feature not in _current_dataset_feature:\n",
    "                        _missing_feature.append(feature)\n",
    "                print('Error: th following features', *_missing_feature, f'are not found in view: {view}')\n",
    "        else:\n",
    "            # trigger error\n",
    "            print(f'error!: missing view {view} in dataset')\n",
    "            \n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc0e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68eedfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file found\n"
     ]
    }
   ],
   "source": [
    "dataset_to_check = load_tabular_datasets(r'/user/ybouilla/home/Documents/data/pseudo_adni_mod/pseudo_adni_mod.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd657d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary key None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>feature_name</th>\n",
       "      <th>CDRSB.bl</th>\n",
       "      <th>ADAS11.bl</th>\n",
       "      <th>MMSE.bl</th>\n",
       "      <th>RAVLT.immediate.bl</th>\n",
       "      <th>RAVLT.learning.bl</th>\n",
       "      <th>FAQ.bl</th>\n",
       "      <th>TAU.MEDIAN.bl</th>\n",
       "      <th>AGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>23.739439</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>132.571916</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>64.933800</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.787719</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.987722</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.049924</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>50.314425</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>138.690457</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>57.217830</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-61.573234</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>61.896022</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.065806</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>62.083170</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.985248</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>22.289059</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>157.229102</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.650504</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>103.238647</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.089863</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54.780563</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "feature_name  CDRSB.bl  ADAS11.bl  MMSE.bl  RAVLT.immediate.bl  \\\n",
       "0                  1.0        8.0     27.0           23.739439   \n",
       "1                  0.0        0.0     30.0           64.933800   \n",
       "2                  0.0        8.0     24.0           36.987722   \n",
       "3                  0.0        3.0     29.0           50.314425   \n",
       "4                  0.0        0.0     30.0           57.217830   \n",
       "..                 ...        ...      ...                 ...   \n",
       "995                1.0        2.0     29.0           61.896022   \n",
       "996                0.0        1.0     29.0           62.083170   \n",
       "997                3.0       14.0     24.0           22.289059   \n",
       "998                0.0       13.0     26.0           31.650504   \n",
       "999                0.0       15.0     28.0           29.089863   \n",
       "\n",
       "feature_name  RAVLT.learning.bl  FAQ.bl  TAU.MEDIAN.bl   AGE  \n",
       "0                           4.0     3.0     132.571916  75.0  \n",
       "1                           9.0     0.0      33.787719  67.0  \n",
       "2                           3.0     0.0     110.049924  63.0  \n",
       "3                           5.0     3.0     138.690457  75.0  \n",
       "4                           9.0     0.0     -61.573234  65.0  \n",
       "..                          ...     ...            ...   ...  \n",
       "995                         8.0     0.0      87.065806  76.0  \n",
       "996                         8.0     1.0     121.985248  77.0  \n",
       "997                         2.0     7.0     157.229102  74.0  \n",
       "998                         2.0     4.0     103.238647  64.0  \n",
       "999                         3.0     4.0      54.780563  65.0  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract views names\n",
    "views_names = list(format_file.keys())\n",
    "\n",
    "\n",
    "\n",
    "# look for primary key\n",
    "primary_key = search_primary_key(format_file)\n",
    "print('primary key', primary_key)\n",
    "\n",
    "# select only features in dataset that will be checked\n",
    "pre_parsed_dataset_to_check = select_data_from_format_file_ref(dataset_to_check, format_file)\n",
    "# rename columns names before join operation\n",
    "pre_parsed_dataset_to_check = rename_variables_before_joining(pre_parsed_dataset_to_check, views_names)\n",
    "pre_parsed_dataset_to_check\n",
    "\n",
    "multi_df_to_check = create_multi_view_dataframe(pre_parsed_dataset_to_check)\n",
    "multi_df_to_check\n",
    "\n",
    "#if primary_key is not None:\n",
    "# jointure operation (takesplace only if primary key has been specfied in foramt_file)\n",
    "df_to_check = join_muti_view_dataset(multi_df_to_check)\n",
    "    \n",
    "df_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "15a12eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'CATEGORICAL', 'data_type': 'BOOLEAN', 'values': [\"<class 'bool'>\"], 'is_missing_values': True, 'categorical_values': ['1', ' 2', ' 4']}\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'QUANTITATIVE', 'data_type': 'DISCRETE', 'values': 'int64', 'is_missing_values': True}\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'CATEGORICAL', 'data_type': 'NUMERICAL', 'values': 'float64', 'is_missing_values': False}\n",
      "ok\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': True}\n",
      "ok\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'CATEGORICAL', 'data_type': 'NUMERICAL', 'values': 'float64', 'is_missing_values': False}\n",
      "ok\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'QUANTITATIVE', 'data_type': 'DISCRETE', 'values': 'int64', 'is_missing_values': False, 'lower_bound': 0.0}\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'QUANTITATIVE', 'data_type': 'CONTINUOUS', 'values': 'float64', 'is_missing_values': False}\n",
      "ok\n",
      "[<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>] float64 {'data_format': 'CATEGORICAL', 'data_type': 'NUMERICAL', 'values': 'float64', 'is_missing_values': False}\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# sanity check on columns using format_file\n",
    "\n",
    "\n",
    "\n",
    "_views_format_file = list(format_file.keys())\n",
    "    \n",
    "for view in _views_format_file:\n",
    "    _features_format_file = format_file[view]\n",
    "    \n",
    "    for feature in _features_format_file:\n",
    "        feature_content = df_to_check[feature]\n",
    "        feature_format =  _features_format_file[feature]\n",
    "        # 1. check for datatype consistency\n",
    "        \n",
    "        data_types = find_data_type(feature_format['data_format'], feature_format['data_type'])\n",
    "        \n",
    "        print(dtype[-1].value, feature_content.dtype, feature_format)\n",
    "        if any(t == feature_content.dtype for t in data_types[-1].value ):\n",
    "            print('ok')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf6f61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e87a346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "566d2128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<DataType.CATEGORICAL: [<CategoricalDataType.BOOLEAN: [<class 'bool'>]>, <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>, <CategoricalDataType.CHARACTER: [<class 'str'>, <class 'object'>]>]>,\n",
       " <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ff5d5",
   "metadata": {},
   "source": [
    "## data_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca423d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudo_adni_mod.csv': {'CDRSB.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'ADAS11.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': True},\n",
       "  'MMSE.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'RAVLT.immediate.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': True},\n",
       "  'RAVLT.learning.bl': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'FAQ.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'DISCRETE',\n",
       "   'values': 'int64',\n",
       "   'is_missing_values': False},\n",
       "  'TAU.MEDIAN.bl': {'data_format': 'QUANTITATIVE',\n",
       "   'data_type': 'CONTINUOUS',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False},\n",
       "  'AGE': {'data_format': 'CATEGORICAL',\n",
       "   'data_type': 'NUMERICAL',\n",
       "   'values': 'float64',\n",
       "   'is_missing_values': False}}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "887f090e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1) YES\\n2) NO\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_yes_or_no_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2460bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85714c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d161776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you want to add a new view (file)?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "please add new view name:\n",
      "ll\n",
      "please add new feature name:\n",
      "ll\n",
      "specify data type for 0:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "4\n",
      "do you want to add a new variable (feature) ?1) YES\n",
      "2) NO\n",
      "2\n",
      "process done\n",
      "do you want to add a new view (file)?\n",
      "1) YES\n",
      "2) NO\n",
      "1\n",
      "please add new view name:\n",
      "kk\n",
      "please add new feature name:\n",
      "vkeof\n",
      "specify data type for 0:\n",
      "1) KEY \n",
      "2) QUANTITATIVE \n",
      "3) CATEGORICAL \n",
      "4) DATETIME \n",
      "5) UNKNOWN \n",
      "6) ignore this column\n",
      "3\n",
      "do you want to add a new variable (feature) ?1) YES\n",
      "2) NO\n",
      "2\n",
      "process done\n",
      "do you want to add a new view (file)?\n",
      "1) YES\n",
      "2) NO\n",
      "2\n",
      "process done\n"
     ]
    }
   ],
   "source": [
    "is_views_finished = False\n",
    "\n",
    "\n",
    "views_format_file = {}\n",
    "\n",
    "while not is_views_finished:\n",
    "    is_features_finished = False\n",
    "    resp = input('do you want to add a new view (file)?\\n' + msg_yes_or_no_question)\n",
    "    resp = yes_or_no_question_key.get(resp)\n",
    "    if not resp:\n",
    "        is_views_finished = True\n",
    "        print('process done')\n",
    "        continue\n",
    "    new_view = input('please add new view name:\\n')\n",
    "    while not is_features_finished:\n",
    "        feature_format_file = {}\n",
    "        new_feature = input('please add new feature name:\\n')\n",
    "        feature_format_file[new_feature] = {}\n",
    "        is_column_parsed = False\n",
    "        try:\n",
    "            while not is_column_parsed:\n",
    "                data_format_id = input(f'specify data type for {feature}:\\n' + msg )\n",
    "                if data_format_id.isdigit() and int(data_format_id) <= n_available_data_type+1:\n",
    "                    # check if value passed by user is correct (if it is integer,\n",
    "                    # and whithin range [1, n_available_data_type])\n",
    "                    is_column_parsed = True\n",
    "                \n",
    "                else:\n",
    "                    print(f'error ! {data_format_id} value not understood')\n",
    "                    \n",
    "        except KeyboardInterrupt as e:\n",
    "            print('stopping now' + str(e))\n",
    "        resp = input('do you want to add a new variable (feature) ?' + msg_yes_or_no_question)\n",
    "        resp = yes_or_no_question_key.get(resp)\n",
    "        if not resp:\n",
    "            is_features_finished = True\n",
    "            print('process done')\n",
    "            continue\n",
    "    views_format_file[new_view] = feature_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee0e0cac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'datetime64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9387/2903691535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/fedbiomed-researcher/lib/python3.9/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'datetime64'"
     ]
    }
   ],
   "source": [
    "type(np.datetime64(\"2018-01-01\"))\n",
    "import datetime\n",
    "type(datetime.datetime(2018, 1, 1))\n",
    "\n",
    "pd.datetime64[ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "24be197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = type(pd.to_datetime('13000101', format='%Y%m%d', errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "52c37736",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.Series(pd.date_range(\"1/1/2011\", freq=\"H\", periods=3)).dtype\n",
    "\n",
    "t =type(t).type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8b13acd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(t == t1 for t1 in [pd.Timestamp, pd.Timedelta, pd.Period, datetime.datetime,np.datetime64] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eb165f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.datetime64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9881289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CDRSB.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'ADAS11.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'MMSE.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'RAVLT.immediate.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'RAVLT.learning.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'RAVLT.forgetting.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'FAQ.bl': {'data_type': <CategoricalDataType.NUMERICAL: [<class 'float'>, <class 'int'>, <class 'numpy.float64'>, <class 'numpy.int64'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False},\n",
       " 'WholeBrain.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'Ventricles.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'Hippocampus.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'MidTemp.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'Entorhinal.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'ABETA.MEDIAN.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'PTAU.MEDIAN.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'TAU.MEDIAN.bl': {'data_type': <QuantitativeDataType.CONTINUOUS: [<class 'float'>, <class 'numpy.float64'>]>,\n",
       "  'values': float,\n",
       "  'is_missing_values': False},\n",
       " 'AGE': {'data_type': <QuantitativeDataType.DISCRETE: [<class 'int'>]>,\n",
       "  'values': int,\n",
       "  'is_missing_values': False}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_format_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb7ad297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.dtype[float64]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(dataset[feature].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63dbaad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " '_AXIS_LEN',\n",
       " '_AXIS_ORDERS',\n",
       " '_AXIS_REVERSED',\n",
       " '_AXIS_TO_AXIS_NUMBER',\n",
       " '_HANDLED_TYPES',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__annotations__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_ufunc__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__divmod__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__finalize__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__ifloordiv__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdivmod__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__round__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_accessors',\n",
       " '_accum_func',\n",
       " '_add_numeric_operations',\n",
       " '_agg_by_level',\n",
       " '_agg_examples_doc',\n",
       " '_agg_see_also_doc',\n",
       " '_align_frame',\n",
       " '_align_series',\n",
       " '_arith_method',\n",
       " '_as_manager',\n",
       " '_attrs',\n",
       " '_binop',\n",
       " '_cacher',\n",
       " '_can_hold_na',\n",
       " '_check_inplace_and_allows_duplicate_labels',\n",
       " '_check_inplace_setting',\n",
       " '_check_is_chained_assignment_possible',\n",
       " '_check_label_or_level_ambiguity',\n",
       " '_check_setitem_copy',\n",
       " '_clear_item_cache',\n",
       " '_clip_with_one_bound',\n",
       " '_clip_with_scalar',\n",
       " '_cmp_method',\n",
       " '_consolidate',\n",
       " '_consolidate_inplace',\n",
       " '_construct_axes_dict',\n",
       " '_construct_axes_from_arguments',\n",
       " '_construct_result',\n",
       " '_constructor',\n",
       " '_constructor_expanddim',\n",
       " '_convert',\n",
       " '_convert_dtypes',\n",
       " '_data',\n",
       " '_dir_additions',\n",
       " '_dir_deletions',\n",
       " '_drop_axis',\n",
       " '_drop_labels_or_levels',\n",
       " '_duplicated',\n",
       " '_find_valid_index',\n",
       " '_flags',\n",
       " '_from_mgr',\n",
       " '_get_axis',\n",
       " '_get_axis_name',\n",
       " '_get_axis_number',\n",
       " '_get_axis_resolvers',\n",
       " '_get_block_manager_axis',\n",
       " '_get_bool_data',\n",
       " '_get_cacher',\n",
       " '_get_cleaned_column_resolvers',\n",
       " '_get_index_resolvers',\n",
       " '_get_label_or_level_values',\n",
       " '_get_numeric_data',\n",
       " '_get_value',\n",
       " '_get_values',\n",
       " '_get_values_tuple',\n",
       " '_get_with',\n",
       " '_gotitem',\n",
       " '_hidden_attrs',\n",
       " '_index',\n",
       " '_indexed_same',\n",
       " '_info_axis',\n",
       " '_info_axis_name',\n",
       " '_info_axis_number',\n",
       " '_init_dict',\n",
       " '_init_mgr',\n",
       " '_inplace_method',\n",
       " '_internal_names',\n",
       " '_internal_names_set',\n",
       " '_is_cached',\n",
       " '_is_copy',\n",
       " '_is_label_or_level_reference',\n",
       " '_is_label_reference',\n",
       " '_is_level_reference',\n",
       " '_is_mixed_type',\n",
       " '_is_view',\n",
       " '_item_cache',\n",
       " '_ixs',\n",
       " '_logical_func',\n",
       " '_logical_method',\n",
       " '_map_values',\n",
       " '_maybe_update_cacher',\n",
       " '_memory_usage',\n",
       " '_metadata',\n",
       " '_mgr',\n",
       " '_min_count_stat_function',\n",
       " '_name',\n",
       " '_needs_reindex_multi',\n",
       " '_protect_consolidate',\n",
       " '_reduce',\n",
       " '_reindex_axes',\n",
       " '_reindex_indexer',\n",
       " '_reindex_multi',\n",
       " '_reindex_with_indexers',\n",
       " '_replace_single',\n",
       " '_repr_data_resource_',\n",
       " '_repr_latex_',\n",
       " '_reset_cache',\n",
       " '_reset_cacher',\n",
       " '_set_as_cached',\n",
       " '_set_axis',\n",
       " '_set_axis_name',\n",
       " '_set_axis_nocheck',\n",
       " '_set_is_copy',\n",
       " '_set_labels',\n",
       " '_set_name',\n",
       " '_set_value',\n",
       " '_set_values',\n",
       " '_set_with',\n",
       " '_set_with_engine',\n",
       " '_slice',\n",
       " '_stat_axis',\n",
       " '_stat_axis_name',\n",
       " '_stat_axis_number',\n",
       " '_stat_function',\n",
       " '_stat_function_ddof',\n",
       " '_take_with_is_copy',\n",
       " '_typ',\n",
       " '_update_inplace',\n",
       " '_validate_dtype',\n",
       " '_values',\n",
       " '_where',\n",
       " 'abs',\n",
       " 'add',\n",
       " 'add_prefix',\n",
       " 'add_suffix',\n",
       " 'agg',\n",
       " 'aggregate',\n",
       " 'align',\n",
       " 'all',\n",
       " 'any',\n",
       " 'append',\n",
       " 'apply',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'array',\n",
       " 'asfreq',\n",
       " 'asof',\n",
       " 'astype',\n",
       " 'at',\n",
       " 'at_time',\n",
       " 'attrs',\n",
       " 'autocorr',\n",
       " 'axes',\n",
       " 'backfill',\n",
       " 'between',\n",
       " 'between_time',\n",
       " 'bfill',\n",
       " 'bool',\n",
       " 'clip',\n",
       " 'combine',\n",
       " 'combine_first',\n",
       " 'compare',\n",
       " 'convert_dtypes',\n",
       " 'copy',\n",
       " 'corr',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'describe',\n",
       " 'diff',\n",
       " 'div',\n",
       " 'divide',\n",
       " 'divmod',\n",
       " 'dot',\n",
       " 'drop',\n",
       " 'drop_duplicates',\n",
       " 'droplevel',\n",
       " 'dropna',\n",
       " 'dtype',\n",
       " 'dtypes',\n",
       " 'duplicated',\n",
       " 'empty',\n",
       " 'eq',\n",
       " 'equals',\n",
       " 'ewm',\n",
       " 'expanding',\n",
       " 'explode',\n",
       " 'factorize',\n",
       " 'ffill',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'first_valid_index',\n",
       " 'flags',\n",
       " 'floordiv',\n",
       " 'ge',\n",
       " 'get',\n",
       " 'groupby',\n",
       " 'gt',\n",
       " 'hasnans',\n",
       " 'head',\n",
       " 'hist',\n",
       " 'iat',\n",
       " 'idxmax',\n",
       " 'idxmin',\n",
       " 'iloc',\n",
       " 'index',\n",
       " 'infer_objects',\n",
       " 'interpolate',\n",
       " 'is_monotonic',\n",
       " 'is_monotonic_decreasing',\n",
       " 'is_monotonic_increasing',\n",
       " 'is_unique',\n",
       " 'isin',\n",
       " 'isna',\n",
       " 'isnull',\n",
       " 'item',\n",
       " 'items',\n",
       " 'iteritems',\n",
       " 'keys',\n",
       " 'kurt',\n",
       " 'kurtosis',\n",
       " 'last',\n",
       " 'last_valid_index',\n",
       " 'le',\n",
       " 'loc',\n",
       " 'lt',\n",
       " 'mad',\n",
       " 'map',\n",
       " 'mask',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'memory_usage',\n",
       " 'min',\n",
       " 'mod',\n",
       " 'mode',\n",
       " 'mul',\n",
       " 'multiply',\n",
       " 'name',\n",
       " 'nbytes',\n",
       " 'ndim',\n",
       " 'ne',\n",
       " 'nlargest',\n",
       " 'notna',\n",
       " 'notnull',\n",
       " 'nsmallest',\n",
       " 'nunique',\n",
       " 'pad',\n",
       " 'pct_change',\n",
       " 'pipe',\n",
       " 'plot',\n",
       " 'pop',\n",
       " 'pow',\n",
       " 'prod',\n",
       " 'product',\n",
       " 'quantile',\n",
       " 'radd',\n",
       " 'rank',\n",
       " 'ravel',\n",
       " 'rdiv',\n",
       " 'rdivmod',\n",
       " 'reindex',\n",
       " 'reindex_like',\n",
       " 'rename',\n",
       " 'rename_axis',\n",
       " 'reorder_levels',\n",
       " 'repeat',\n",
       " 'replace',\n",
       " 'resample',\n",
       " 'reset_index',\n",
       " 'rfloordiv',\n",
       " 'rmod',\n",
       " 'rmul',\n",
       " 'rolling',\n",
       " 'round',\n",
       " 'rpow',\n",
       " 'rsub',\n",
       " 'rtruediv',\n",
       " 'sample',\n",
       " 'searchsorted',\n",
       " 'sem',\n",
       " 'set_axis',\n",
       " 'set_flags',\n",
       " 'shape',\n",
       " 'shift',\n",
       " 'size',\n",
       " 'skew',\n",
       " 'slice_shift',\n",
       " 'sort_index',\n",
       " 'sort_values',\n",
       " 'squeeze',\n",
       " 'std',\n",
       " 'sub',\n",
       " 'subtract',\n",
       " 'sum',\n",
       " 'swapaxes',\n",
       " 'swaplevel',\n",
       " 'tail',\n",
       " 'take',\n",
       " 'to_clipboard',\n",
       " 'to_csv',\n",
       " 'to_dict',\n",
       " 'to_excel',\n",
       " 'to_frame',\n",
       " 'to_hdf',\n",
       " 'to_json',\n",
       " 'to_latex',\n",
       " 'to_list',\n",
       " 'to_markdown',\n",
       " 'to_numpy',\n",
       " 'to_period',\n",
       " 'to_pickle',\n",
       " 'to_sql',\n",
       " 'to_string',\n",
       " 'to_timestamp',\n",
       " 'to_xarray',\n",
       " 'transform',\n",
       " 'transpose',\n",
       " 'truediv',\n",
       " 'truncate',\n",
       " 'tz_convert',\n",
       " 'tz_localize',\n",
       " 'unique',\n",
       " 'unstack',\n",
       " 'update',\n",
       " 'value_counts',\n",
       " 'values',\n",
       " 'var',\n",
       " 'view',\n",
       " 'where',\n",
       " 'xs']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataset[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae52ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
