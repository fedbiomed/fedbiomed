{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed-BioMed secure aggregation tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example experimentation setup\n",
    "\n",
    "This part contains setup of a basic example for Fed-BioMed. At this point, nothing is specific to secure aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the network\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting nodes up\n",
    "It is necessary to previously configure ** at least two nodes**:\n",
    "1. `./scripts/fedbiomed_run node config config_node1.ini add` (respectively for the second node: `./scripts/fedbiomed_run node config config_node2.ini add`)\n",
    "  * Select option 2 (default) to add MNIST to the node\n",
    "  * Confirm default tags by hitting \"y\" and ENTER\n",
    "  * Pick the folder where MNIST is downloaded (this is due to a pytorch issue https://github.com/pytorch/vision/issues/3549)\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run config config_node1.ini node list`\n",
    "3. Run the node using `./scripts/fedbiomed_run config_node1.ini node run`. Wait until you get `Starting task manager`. it means you are online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an experiment model and parameters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a torch training plan MyTrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    \n",
    "    # Defines and return model \n",
    "    def init_model(self, model_args):\n",
    "        return self.Net(model_args = model_args)\n",
    "    \n",
    "    # Defines and return optimizer\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        return torch.optim.Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n",
    "    \n",
    "    # Declares and return dependencies\n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torchvision import datasets, transforms\"]\n",
    "        return deps\n",
    "    \n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, model_args):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "            self.dropout1 = nn.Dropout(0.25)\n",
    "            self.dropout2 = nn.Dropout(0.5)\n",
    "            self.fc1 = nn.Linear(9216, 128)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = self.dropout1(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "\n",
    "\n",
    "            output = F.log_softmax(x, dim=1)\n",
    "            return output\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        # Custom torch Dataloader for MNIST data\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset=dataset1, **train_kwargs)\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side.\n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'optimizer_args': {\n",
    "        \"lr\" : 1e-3\n",
    "    },\n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare and run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "from fedbiomed.researcher.secagg import SecureAggregation\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 2\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None,\n",
    "                 secagg=True, # or custom SecureAggregation(active=<bool>, clipping_range=<int>, timeout=<int>)\n",
    "                 save_breakpoints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access secure aggregation context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use the attribute `secagg` to verify secure aggregation is set as active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Is using secagg: \", exp.secagg.active)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to check secure aggregation context using `secagg` attribute. Since secure aggregation context negotiation will occur during experiment run, context and id should be `None` at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Secagg Biprime \", exp.secagg.biprime)\n",
    "print(\"Secagg Servkey \", exp.secagg.servkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment, using secure aggregation. Secure aggregation context will be created before the first training round and it is going to be updated before each round when new nodes are added or removed to the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.run(increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display context after running one round of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Secagg Biprime context: \", exp.secagg.biprime.context)\n",
    "print(\"Secagg Servkey context: \", exp.secagg.servkey.context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes in experiment triggers re-creation of secure aggregation context\n",
    "\n",
    "The changes that re-create jobs like adding new node to the experiment will trigger automatic secure aggregation re-setup for the next round.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sends new dataset search request\n",
    "from fedbiomed.researcher.strategies import DefaultStrategy\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "exp.set_training_data(None, True)\n",
    "exp.set_strategy(DefaultStrategy)\n",
    "exp.set_aggregator(FedAverage)\n",
    "exp.set_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.run_once(increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing arguments of secure aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `secagg` argument `True` in `Experiment` creates a default `SecureAggregation` instance. Additionally, It is also possible to create `SecureAggregation` instance and pass it as an argument. Here are the arguments that can be set for the `SecureAggregation`\n",
    "\n",
    "- `active`: `True` if the round will use secure aggregation. Default is `True`\n",
    "- `clipping_range`: Clipping range that is goingto be use for quantization of model parameters. Default clipping range is `3`. However, some models can have model weigths greater than `3`. If clipping range is exceeded during the encryption on the nodes, `Experiment` will log a warning message. In such cases, you can provide a higher clipping range through the argument `clipping_range`.\n",
    "- `timeout`: Timeout is the maximum amount of time, in seconds, that the experiment will wait for responses from all parties during secure aggregation setup. Since secure aggregation context depends on network communication and multi-party computation, this argument allows to set higher timeout for larger context setups, or vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.secagg import SecureAggregation\n",
    "secagg = SecureAggregation(\n",
    "    active=True, \n",
    "    clipping_range=100,\n",
    "    timeout=15\n",
    "    \n",
    ")\n",
    "exp.set_secagg(secagg=secagg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.run_once(increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load experiment from a breakpoint\n",
    "\n",
    "Once a breakpoint is loadded if the context is already exsiting there won't be context setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loaded_exp = Experiment.load_breakpoint()\n",
    "loaded_exp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_exp.run_once(increase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
