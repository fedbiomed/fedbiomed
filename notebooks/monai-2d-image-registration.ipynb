{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated 2d XRay registration with MONAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial shows how to deploy in Fed-BioMed the 2d image registration example provided in the project MONAI (https://monai.io/):\n",
    "\n",
    "https://github.com/Project-MONAI/tutorials/blob/master/2d_registration/registration_mednist.ipynb\n",
    "\n",
    "Being MONAI based on PyTorch, the deployment within Fed-BioMed follows seamlessy the same general structure of general PyTorch models. \n",
    "\n",
    "Following the MONAI example, this tutorial is based on the MedNIST dataset>\n",
    "\n",
    "\n",
    "\n",
    "## Image Registration\n",
    "\n",
    "Image registration is the process of transforming and recalibrating different images into one coordinate system. It makes possible to compare several images captured with the same modality.\n",
    "\n",
    "In this tutorial, we are using a UNet-like registration network ( https://arxiv.org/abs/1711.01666 ).\n",
    "Goal of the notebook is to train a model given moving images and fixed images (recalibrated images). \n",
    "\n",
    "## Creating MedNIST nodes\n",
    "\n",
    "MedNIST provides an artificial 2d classification dataset created by gathering different medical imaging datasets from TCIA, the RSNA Bone Age Challenge, and the NIH Chest X-ray dataset. The dataset is kindly made available by Dr. Bradley J. Erickson M.D., Ph.D. (Department of Radiology, Mayo Clinic) under the Creative Commons CC BY-SA 4.0 license.\n",
    "\n",
    "To proceed with the tutorial, we created an iid partitioning of the MedNIST dataset between 3 clients. Each client has 3000 image samples for each class. The training partitions are availables at the following link:\n",
    "\n",
    "https://drive.google.com/file/d/1vLIcBdtdAhh6K-vrgCFy_0Y55dxOWZwf/view\n",
    "\n",
    "The dataset owned by each client has structure:\n",
    "\n",
    "\n",
    "└── client_*/\n",
    "\n",
    "    ├── AbdomenCT/\n",
    "    \n",
    "    └── BreastMRI/\n",
    "    \n",
    "    └── CXR/\n",
    "    \n",
    "    └── ChestCT/\n",
    "    \n",
    "    └── Hand/\n",
    "    \n",
    "    └── HeadCT/      \n",
    "\n",
    "To create the federated dataset, we follow the standard procedure for node creation/population of Fed-BioMed. \n",
    "After activating the fedbiomed network with the commands\n",
    "\n",
    "`source ./scripts/fedbiomed_environment network`\n",
    "\n",
    "and \n",
    "\n",
    "`./scripts/fedbiomed_run network`\n",
    "\n",
    "we create a first node by using the commands\n",
    "\n",
    "`source ./scripts/fedbiomed_environment node`\n",
    "\n",
    "`./scripts/fedbiomed_run node start`\n",
    "\n",
    "We then poulate the node with the data of first client:\n",
    "\n",
    "`./scripts/fedbiomed_run node add`\n",
    "\n",
    "We select option 3 (images) to add MedNIST partition of client 1, by just picking the folder of client 1. We use `mednist` as tag to save the selected dataset.\n",
    "We can further check that the data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "\n",
    "Following the same procedure, we create the other two nodes with the datasets of client 2 and client 3 respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Fed-BioMed Researcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start the reseracher enviroment with the command `source ./scripts/fedbiomed_environment researcher`, and open the Jupyter notebook with `./scripts/fedbiomed_run researcher`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first quesry the network for the mednist dataset. In this case, the nodes are sharing the respective partitions unsing the same tag `mednist`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.requests import Requests\n",
    "req = Requests()\n",
    "req.list(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment to train a model on the data found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for network and data loader of the MONAI tutorial can now be deployed in Fed-BioMed.\n",
    "We first import the necessary modules from `fedbiomed` and `monai` libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils import set_determinism, first\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstD,\n",
    "    Compose,\n",
    "    LoadImageD,\n",
    "    RandRotateD,\n",
    "    RandZoomD,\n",
    "    ScaleIntensityRanged,\n",
    "    EnsureTypeD,\n",
    ")\n",
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "from monai.config import print_config, USE_COMPILED\n",
    "from monai.networks.nets import GlobalNet\n",
    "from monai.networks.blocks import Warp\n",
    "from monai.apps import MedNISTDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the training plan. Note that we use the standard `TorchTrainingPlan` natively provided in Fed-BioMed. We reuse the `MedNISTDataset` data loader defined in the original MONAI tutorial, which is returned by the method `training_data`, which also implements the data parsing from the nodes `dataset_path`. We should also properly define the `training_routine`, following the MONAI tutorial. According to the MONAI tutorial, the model is the `GlobalNet` and the loss is `MSELoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.logger import logger\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "from typing import Union, List\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "import monai\n",
    "from monai.utils import set_determinism, first\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstD,\n",
    "    Compose,\n",
    "    LoadImageD,\n",
    "    RandRotateD,\n",
    "    RandZoomD,\n",
    "    ScaleIntensityRanged,\n",
    "    EnsureTypeD,\n",
    ")\n",
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "from monai.config import print_config, USE_COMPILED\n",
    "from monai.networks.nets import GlobalNet\n",
    "from monai.networks.blocks import Warp\n",
    "from monai.apps import MedNISTDataset\n",
    "\n",
    "\n",
    "# Here we define the model to be used. \n",
    "class MyMonaiTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self, model_args: dict = {}):\n",
    "        super(MyMonaiTrainingPlan, self).__init__(model_args)\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        # In this case, we need the torch DataLoader classes\n",
    "        # Since we will train on MNIST, we need datasets and transform from torchvision\n",
    "        deps = [\"import numpy as np\",\n",
    "                \"import monai\",\n",
    "                \"from torch.nn import MSELoss\",\n",
    "                \"from monai.utils import set_determinism, first\",\n",
    "                \"from monai.transforms import (EnsureChannelFirstD,Compose,LoadImageD,RandRotateD,RandZoomD,ScaleIntensityRanged,EnsureTypeD,)\",\n",
    "                \"from monai.data import DataLoader, Dataset, CacheDataset\",\n",
    "                \"from monai.networks.nets import GlobalNet\",\n",
    "                \"from monai.config import USE_COMPILED\",\n",
    "                \"from monai.networks.blocks import Warp\",\n",
    "                \"from monai.apps import MedNISTDataset\",]\n",
    "        self.add_dependency(deps)\n",
    "        \n",
    "        self.model = GlobalNet(\n",
    "            image_size=(64, 64),\n",
    "            spatial_dims=2,\n",
    "            in_channels=2,  # moving and fixed\n",
    "            num_channel_initial=16,\n",
    "            depth=3)\n",
    "        self.image_loss = MSELoss()\n",
    "        \n",
    "        if USE_COMPILED:\n",
    "            self.warp_layer = Warp(3, \"border\")\n",
    "        else:\n",
    "            self.warp_layer = Warp(\"bilinear\", \"border\")\n",
    "            \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), 1e-5)\n",
    "        \n",
    "    def training_data(self, batch_size = 20):\n",
    "        # Custom torch Dataloader for MedNIST data\n",
    "        data_path = self.dataset_path\n",
    "        # The following line is needed if client structure does not contain the \"/MedNIST\" folder\n",
    "        MedNISTDataset.dataset_folder_name = \"\"\n",
    "        train_data = MedNISTDataset(root_dir=data_path, section=\"training\", download=False, transform=None)\n",
    "        training_datadict = [\n",
    "            {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}\n",
    "            for item in train_data.data if item[\"label\"] == 4  # label 4 is for xray hands\n",
    "        ]\n",
    "        train_transforms = Compose(\n",
    "            [\n",
    "                LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "                EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "                ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],\n",
    "                                     a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),\n",
    "                RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),\n",
    "                RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1,\n",
    "                          monaiprob=1.0, mode=\"bicubic\", align_corners=False),\n",
    "                EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "            ]\n",
    "        )\n",
    "        train_ds = CacheDataset(data=training_datadict[:1000], transform=train_transforms,\n",
    "                                cache_rate=1.0, num_workers=0)\n",
    "        dl = self.MednistDataLoader(train_ds)\n",
    "        \n",
    "        return DataManager(dl, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, moving, fixed):\n",
    "        ddf = self.forward(torch.cat((moving, fixed), dim=1))\n",
    "        pred_image = self.warp_layer(moving, ddf)\n",
    "        loss = self.image_loss(pred_image, fixed)\n",
    "        return loss\n",
    "    \n",
    "    class MednistDataLoader(monai.data.Dataset):\n",
    "        # Custom DataLoader that inherits from monai's Dataset object\n",
    "        def __init__(self, dataset):\n",
    "            self.dataset = dataset\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return (self.dataset[idx][\"moving_hand\"],\n",
    "                    self.dataset[idx][\"fixed_hand\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set the model and training parameters. Note that in this case, no model argument is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    # Model wants to use GPU if available on node and proposed by node (default: False)\n",
    "    #'use_gpu': True\n",
    "}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 16, \n",
    "    'lr': 1e-5, \n",
    "    'epochs': 3, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum':250 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment can be now defined, by providing the `mednist` tag, and running the local training on nodes with model defined in `model_path`, standard `aggregator` (FedAvg) and `client_selection_strategy` (all nodes used). Federated learning is going to be perfomed through 5 optimization rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['mednist']\n",
    "rounds = 5\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MyMonaiTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `round_limit` rounds are done for all the clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the federated model is obtained, it is possible to test it locally on an independent testing partition.\n",
    "The test dataset is available at this link:\n",
    "\n",
    "https://drive.google.com/file/d/1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD/\n",
    "\n",
    "Following the Monai tutorial, in this section we will create a set of previously unseen pairs of moving vs fixed hands, and use the final federated model to predict the transformation between each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import PIL\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gdown\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print_config()\n",
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the testing dataset on the local temporary folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "from fedbiomed.researcher.environ import environ\n",
    "\n",
    "tmp_dir = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+os.sep)\n",
    "\n",
    "resource = \"https://drive.google.com/uc?id=1YbwA0WitMoucoIa_Qao7IC1haPfDp-XD\"\n",
    "base_dir = tmp_dir.name\n",
    "test_file = os.path.join(base_dir, \"MedNIST_testing.zip\")\n",
    "\n",
    "gdown.download(resource, test_file, quiet=False)\n",
    "\n",
    "zf = zipfile.ZipFile(test_file)\n",
    "\n",
    "for file in zf.infolist():\n",
    "    zf.extract(file, base_dir)\n",
    "    \n",
    "data_dir = os.path.join(base_dir, \"MedNIST_testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redefine our custom dataloader (defined previously in  the `TrainingPlan`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "import monai\n",
    "\n",
    "class MednistDataLoader(monai.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.dataset[idx][\"moving_hand\"],\n",
    "                self.dataset[idx][\"fixed_hand\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the testing data loader and pairs of moving vs fixed hands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a GPU if you have one + enough memory available\n",
    "#\n",
    "#use_cuda = torch.cuda.is_available()\n",
    "#device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "# recreate model\n",
    "model = GlobalNet(\n",
    "    image_size=(64, 64),\n",
    "    spatial_dims=2,\n",
    "    in_channels=2,  # moving and fixed\n",
    "    num_channel_initial=16,\n",
    "    depth=3).to(device)\n",
    "\n",
    "if USE_COMPILED:\n",
    "    warp_layer = Warp(3, \"border\").to(device)\n",
    "else:\n",
    "    warp_layer = Warp(\"bilinear\", \"border\").to(device)\n",
    "\n",
    "MedNISTDataset.dataset_folder_name = \"\"\n",
    "test_data = MedNISTDataset(root_dir=data_dir, section=\"test\", download=False, transform=None)\n",
    "testing_datadict = [\n",
    "    {\"fixed_hand\": item[\"image\"], \"moving_hand\": item[\"image\"]}\n",
    "    for item in test_data.data if item[\"label\"] == 4  # label 4 is for xray hands\n",
    "]\n",
    "test_transforms = Compose(\n",
    "            [\n",
    "                LoadImageD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "                EnsureChannelFirstD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "                ScaleIntensityRanged(keys=[\"fixed_hand\", \"moving_hand\"],\n",
    "                                     a_min=0., a_max=255., b_min=0.0, b_max=1.0, clip=True,),\n",
    "                RandRotateD(keys=[\"moving_hand\"], range_x=np.pi/4, prob=1.0, keep_size=True, mode=\"bicubic\"),\n",
    "                RandZoomD(keys=[\"moving_hand\"], min_zoom=0.9, max_zoom=1.1, prob=1.0, mode=\"bicubic\", align_corners=False),\n",
    "                EnsureTypeD(keys=[\"fixed_hand\", \"moving_hand\"]),\n",
    "            ]\n",
    "        )\n",
    "val_ds = CacheDataset(data=testing_datadict[:1000], transform=test_transforms,\n",
    "                      cache_rate=1.0, num_workers=0)\n",
    "val_dl = MednistDataLoader(val_ds)\n",
    "val_loader = DataLoader(val_dl, batch_size=16, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model instance and assign to it the model parameters estimated at the last federated optimization round.\n",
    "Generate predictions of the transformation between pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract federated model into PyTorch framework\n",
    "model = exp.model_instance()\n",
    "model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])\n",
    "\n",
    "for moving, fixed in val_loader:\n",
    "    ddf = model(torch.cat((moving, fixed), dim=1))\n",
    "    pred_image = warp_layer(moving, ddf)\n",
    "    break\n",
    "\n",
    "fixed_image = fixed.detach().cpu().numpy()[:, 0]\n",
    "moving_image = moving.detach().cpu().numpy()[:, 0]\n",
    "pred_image = pred_image.detach().cpu().numpy()[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally print some example of predictions from the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "batch_size = 10\n",
    "plt.subplots(batch_size, 4, figsize=(12, 20))\n",
    "for b in range(batch_size):\n",
    "    # moving image\n",
    "    plt.subplot(batch_size, 4, b * 4 + 1)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"moving image\")\n",
    "    plt.imshow(moving_image[b], cmap=\"gray\")\n",
    "    # fixed image\n",
    "    plt.subplot(batch_size, 4, b * 4 + 2)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"fixed image\")\n",
    "    plt.imshow(fixed_image[b], cmap=\"gray\")\n",
    "    # warped moving\n",
    "    plt.subplot(batch_size, 4, b * 4 + 3)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"predicted image\")\n",
    "    plt.imshow(pred_image[b], cmap=\"gray\")\n",
    "    \n",
    "    #error\n",
    "    plt.subplot(batch_size, 4, b * 4 + 4)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"error between predicted \\nand fixed image\")\n",
    "    plt.imshow(pred_image[b] - fixed_image[b], cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
