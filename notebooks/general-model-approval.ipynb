{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process for Approved Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fed-BioMed offers a feature to run only the pre-approved models on the nodes. The nodes that you will be sending your model might require approved models. Therefore, if the node accepts on the approved model, the model files that are sent by a researcher with the training request should be approved by the node side in adnvance. The approval process is done by a real user/person who will review the model file. The reviewer make sure the model doesn't contain any code that might cause privacy issues. In this tutorial, we will be creating a node with activated model approval option.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the network\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up a Node\n",
    "\n",
    "\n",
    "Enabling model approval can be done both from config file or Fed-BioMed CLI while starting the node. The process of creating and starting a node with model approval option is not so different than setting up a normal node. By default, if any option is not specified in the CLI, the node disables model approval and security section of config file looks like the snippet below. \n",
    "\n",
    "```shell\n",
    "[security]\n",
    "hashing_algorithm = SHA256\n",
    "allow_default_models = True\n",
    "model_approval = False\n",
    "```\n",
    "The Fed-BioMed CLI gets two extra parameters as `--enable-model-approval` and `--allow-default-models` to activate model approval;\n",
    "\n",
    "* `--enable-model-approval` : This parameter enables model approval for the node. If there isn't a config file for the node while running CLI, it creates a new config file with enabled model approval mode `model_approval = True`. \n",
    "* `--allow-default-models`  : This parameter allows default models for train requests. These are the models that comes for Fed-BioMed tutorilas. For example, the model for MNIST dataset that we will be using for this tutorial. If the default models are enabled, node updates/registers model file which is located in `envs/developments/default_models` directory during starting process of the node. \n",
    "\n",
    "\n",
    "### Adding MNIST Dataset to The Node. \n",
    "\n",
    "In this section we will add MNIST dataset to the node. While adding the dataset through CLi we'll also specify `--enable-model-approval` and `--allow-default-models` options. This will create new `config-n1.ini` file with following configuration. \n",
    "\n",
    "```\n",
    "[security]\n",
    "hashing_algorithm = SHA256\n",
    "allow_default_models = True\n",
    "model_approval = True\n",
    "\n",
    "```\n",
    "Now, let's run the following command. \n",
    "\n",
    "```shell\n",
    "$ {FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini --enable-model-approval --allow-default-models add \n",
    "```\n",
    "\n",
    "The CLI will ask you to select the dataset type. Since we will be working on MNIST dataset, please select `2` (default) and continue by typing `y` for the next prompt and select folder that you want to store MNIST dataset. Afterward, if you go to `etc` directory of fedbiomed, you can see `config-n1.ini` file. \n",
    "\n",
    "### Starting the Node\n",
    "\n",
    "Now you can start your node by running following command; \n",
    "\n",
    "```\n",
    "$ {FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini start\n",
    "```\n",
    "\n",
    "Since, config file has been configured to enable model approval mode, you do not need to specifiy any extra parameter while starting the node. But it is also possible to start node with `--enable-model-approval`, `--allow-default-models` or `--disable-model-approval`, `--disable-default-models`. If you start your node with `--disable-model-approval` it will disable model approval even it is enabled in the config file.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating A Experiment\n",
    "\n",
    "In this section we will be using default MNIST model which has been already registered by the node.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import environ\n",
    "import tempfile\n",
    "tmp_dir_model = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+'/')\n",
    "model_file = tmp_dir_model.name + '/class_export_mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model is the model that will be sent to the node for traning. Since the model files are processed by the Experiment to configure dependencies, import part of the final file might be different than this one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"$model_file\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.torchnn import TorchTrainingPlan\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# You can use any class name (here 'Net')\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self):\n",
    "        super(MyTrainingPlan, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        # In this case, we need the torch DataLoader classes\n",
    "        # Since we will train on MNIST, we need datasets and transform from torchvision\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "               \"from torch.utils.data import DataLoader\"]\n",
    "        self.add_dependency(deps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        # Custom torch Dataloader for MNIST data\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        data_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "        return data_loader\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to get/see the final model file we need to initialize the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 2\n",
    "\n",
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'lr': 1e-3, \n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 #nodes=None,\n",
    "                 model_path=model_file,\n",
    "                 model_args=model_args,\n",
    "                 model_class='MyTrainingPlan',\n",
    "                 training_args=training_args,\n",
    "                 rounds=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Final Model File From Experiment\n",
    "\n",
    "`model_file()` displays the model file that will be send to the nodes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.model_file(display = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `exp.check_model_status()` sends request to the nodes to check whether the model is approved or not. The nodes that will receive the reqeusts are the nodes that have been found after searching datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = exp.check_model_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logs should indicate that the model is approved. You can also get status object from the result of the `check_model_status()`. it returns list of status objects each for different node. Since we have only launched single node, it will return only one status object. \n",
    "\n",
    "* `approval_obligation` : Indicates whether the model approval is enabled in the node.  \n",
    "* `is_approved`         : Indicates whether the models is approved or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Model And Testing Model Approval Status\n",
    "\n",
    "Let's change the model codes and test whether it is approved or not. We will be changing the network structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import environ\n",
    "import tempfile\n",
    "tmp_dir_model = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+'/')\n",
    "model_file_2 = tmp_dir_model.name + '/class_export_mnist_2.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"$model_file_2\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.torchnn import TorchTrainingPlan\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self):\n",
    "        super(MyTrainingPlan, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, 1, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 10)\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "               \"from torch.utils.data import DataLoader\"]\n",
    "        \n",
    "        self.add_dependency(deps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        # Custom torch Dataloader for MNIST data\n",
    "        \n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        data_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)        \n",
    "        return data_loader\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 2\n",
    "\n",
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'lr': 1e-3, \n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "exp2 = Experiment(tags=tags,\n",
    "                 #nodes=None,\n",
    "                 model_path=model_file_2,\n",
    "                 model_args=model_args,\n",
    "                 model_class='MyTrainingPlan',\n",
    "                 training_args=training_args,\n",
    "                 rounds=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we changed the codes, the output of  the following method should say that the model is not approved by the node and `is_approved` key of the result object should be equal to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = exp2.check_model_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2.model_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is not approved, you won't be able to train your model in the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering/Approving the Model \n",
    "\n",
    "To register/approve the model that has been created in the previous section, we can use Fed-BioMed CLI. You do not need to stop your node to register new models you can perfom registeritation process in a different terminal window. However, first we need to get final model from `exp2` object \n",
    "\n",
    "\n",
    "--------\n",
    "\n",
    "<div class=\"note\">\n",
    "<p>\n",
    "    In the previous notebook cells, we tried to run a model which is not approved by the node. Therefore, your notebook kernel should have been killed. You might need to restart your kernel to be able to run your expirement. After restarting, please follow tutorial directly starting from this section.  \n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.environ import environ\n",
    "import tempfile\n",
    "tmp_dir_model = tempfile.TemporaryDirectory(dir=environ['TMP_DIR']+'/')\n",
    "model_file_3 = tmp_dir_model.name + '/class_export_mnist_3.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"$model_file_3\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.torchnn import TorchTrainingPlan\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self):\n",
    "        super(MyTrainingPlan, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, 1, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 10)\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "               \"from torch.utils.data import DataLoader\"]\n",
    "        \n",
    "        self.add_dependency(deps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        # Custom torch Dataloader for MNIST data\n",
    "        \n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        data_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)        \n",
    "        return data_loader\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new experiment as `exp3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 2\n",
    "\n",
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'lr': 1e-3, \n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "exp3 = Experiment(tags=tags,\n",
    "                 model_path=model_file_3,\n",
    "                 model_args=model_args,\n",
    "                 model_class='MyTrainingPlan',\n",
    "                 training_args=training_args,\n",
    "                 rounds=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3.model_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `exp3.model_file` is a file a path that show where the final model is saved. It also prints the content of the model file. You can either get the content of model from the output cell or the path where it is save. Anyway, you need to create a new `txt` file and copy the model content in it. You can create new directory in Fedi-BioMed call `models` and inside it you can create new `my-model.txt` file and copy the model content into it. \n",
    "\n",
    "Afterward, please run following command in other terminal to register model file. \n",
    "\n",
    "```shell\n",
    "$ {FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n1.ini --register-model\n",
    "```\n",
    "\n",
    "You should type a unique name for your model e.g. 'MyTestModel-1' and a description. The CLI will ask you select model file you want to register. Select the file that you saved and continue. \n",
    "\n",
    "Now, you should be able to train your model. \n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3.check_model_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
