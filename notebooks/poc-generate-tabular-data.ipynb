{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f67fab2",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to store script for generating tabular random data\n",
    "\n",
    "\n",
    "- single view data\n",
    "- multi view (folder of csv files)\n",
    "- multi view data (all contained in a single csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a0e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 create random data\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from typing import Iterator, Union, List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "659a9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataGenerator:\n",
    "    def __init__(self, n_samples:int, \n",
    "                 feature_names:Iterator[str]=None,\n",
    "                 is_multi_view:bool=False,\n",
    "                 as_multi_index:bool=False):\n",
    "        self._array = None\n",
    "        \n",
    "        self._n_samples = n_samples\n",
    "        self._views = {}\n",
    "        self._features_names = []\n",
    "        self._primary_key = None  # either None or a pandas serie\n",
    "        #self._is_view_set = False\n",
    "        self._available_char = 'qwertyuiopasdfghjklzxcvbnm '  \n",
    "        #all characters available for generating primary keys (eg name_surname)\n",
    "        self._shuffle_primary_key = False\n",
    "    \n",
    "    def add_primary_key(self, col_name: str,\n",
    "                        dtype:str,\n",
    "                        is_shuffled: bool=False,\n",
    "                        char_len:int=20):\n",
    "        if dtype == 'int':\n",
    "            # primary key is an index\n",
    "            _idx = np.arange(self._n_samples)\n",
    "        elif dtype == 'str':\n",
    "            # primary key is a string (ie patient name)\n",
    "            n_char = len(self._available_char)  #\n",
    "            \n",
    "            assert char_len ** n_char > self._n_samples, f'char_len **n_char is supposed to be greater than n_samples'\n",
    "            \n",
    "            _idx = []\n",
    "            for sample in range(self._n_samples):\n",
    "                _is_unique = False\n",
    "                while not _is_unique:\n",
    "                    _order = np.random.randint(0, n_char, size=(char_len,))\n",
    "                    _new_char_id = ''.join([self._available_char[x] for x in _order])\n",
    "                    if _new_char_id not in _idx:\n",
    "                        _is_unique = True\n",
    "                _idx.append(_new_char_id)\n",
    "        else:\n",
    "            raise ValueError(f\"dtype: {dtype} is not valid\")\n",
    "        if is_shuffled:\n",
    "            np.random.shuffle(_idx)\n",
    "        _idx = pd.DataFrame(_idx)\n",
    "        if col_name is not None:\n",
    "            _idx.columns = [col_name]\n",
    "        self._concatenate(_idx)\n",
    "        self._primary_key = _idx\n",
    "        \n",
    "    def set_primary_key(self, col_name: Union[str, int]=None):\n",
    "        \n",
    "        col_indx = self._get_index(col_name)\n",
    "        _primary_key = self._array.iloc[:,col_indx]\n",
    "        assert len(np.unique(_primary_key)) == self._n_samples, 'value in primary key must be unique: aborting'\n",
    "        \n",
    "        self._primary_key = _primary_key\n",
    "\n",
    "    def shuffle_primary_key(self):\n",
    "        self._shuffle_primary_key = True\n",
    "        \n",
    "    def set_view(self, view_name:str):\n",
    "        if self._array is not None:\n",
    "            _df = self.get_single_view_dataframe()\n",
    "        self._views[view_name] = _df\n",
    "        self._features_names = []\n",
    "        self._array = None\n",
    "    \n",
    "    def get_single_view_dataframe(self, view_name: Union[str, int]=None):\n",
    "        #_df = pd.DataFrame(self._array, columns=self._features_names)\n",
    "        \n",
    "        print(f'Dataframe for view {view_name}')\n",
    "        if view_name:\n",
    "            _df = self._views[view_name]\n",
    "        else:\n",
    "            _df = self._array\n",
    "        return _df\n",
    "    \n",
    "    def add_integers_values(self, n_col: int=1,\n",
    "                            col_name:Union[str, Iterator[str]] =None,\n",
    "                            l_bound:int=0,\n",
    "                           u_bound:int=100,\n",
    "                           is_unique=False):\n",
    "        if is_unique:\n",
    "            _rand_int = np.arange(self._n_samples)\n",
    "        else:\n",
    "            _rand_int = np.random.randint(l_bound, u_bound, size=(self._n_samples, n_col))\n",
    "        _rand_int = pd.DataFrame(_rand_int)\n",
    "        if col_name is not None:\n",
    "            _rand_int.columns = [col_name]\n",
    "        self._concatenate(_rand_int)\n",
    "        if col_name is not None:\n",
    "            self._features_names.append(col_name)\n",
    "    \n",
    "    def add_float_values(self, n_col:int=1, col_name:str=None):\n",
    "        _rand = np.random.random((self._n_samples, n_col))\n",
    "        _rand = pd.DataFrame(_rand)\n",
    "        if col_name is not None:\n",
    "            _rand.columns = [col_name]\n",
    "        self._concatenate(_rand)\n",
    "        if col_name is not None:\n",
    "            self._features_names.append(col_name)\n",
    "            \n",
    "    def add_discrete_values(self, n_col:int=1,\n",
    "                            col_name:Union[str, Iterator[str]] =None,\n",
    "                            l_bound:int=0,\n",
    "                           u_bound:int=100):\n",
    "        \n",
    "        _rand_int = np.random.randint(l_bound, u_bound, size=(self._n_samples, n_col))\n",
    "        _rand_int = np.array(_rand_int, dtype=np.float64) # changing type from int to float\n",
    "        _rand_int = pd.DataFrame(_rand_int)\n",
    "        if col_name is not None:\n",
    "            _rand_int.columns = [col_name]\n",
    "        self._concatenate(_rand_int)\n",
    "        if col_name is not None:\n",
    "            self._features_names.append(col_name)\n",
    "            \n",
    "    def add_datetime_values(self,  start=None, end=None, col_name:str=None, freq='H', **kwargs):\n",
    "        _dti = pd.date_range(start, end, periods=self._n_samples, freq=freq,**kwargs)\n",
    "        _dti = pd.DataFrame(_dti)\n",
    "        if col_name is not None:\n",
    "            _dti.columns = [col_name]\n",
    "        self._concatenate(_dti)\n",
    "        if col_name is not None:\n",
    "            self._features_names.append(col_name)\n",
    "    \n",
    "    def add_missing_samples_to_column(self, col_name:Union[str, int], n_missing_points:int=1):\n",
    "        \n",
    "        if self._array is None:\n",
    "            raise ValueError(\"please add data before completing it with missig data\")\n",
    "        if n_missing_points >= self._n_samples:\n",
    "            raise ValueError(f\"too much missing points (n_missing_points {n_missing_points} < n_samples {self._n_samples})\")\n",
    "        col_indx = self._get_index(col_name)\n",
    "        _idx = np.arange(self._n_samples)\n",
    "        np.random.shuffle(_idx)\n",
    "        _array = self._array.iloc[:, col_indx]\n",
    "        _shuffled_array = _array.iloc[_idx].values\n",
    "        _shuffled_array[:n_missing_points] = np.nan\n",
    "        self._array.iloc[:, col_indx] = _shuffled_array[_idx]\n",
    "    \n",
    "    def add_boolean_values(self, n_col:int=1, col_name: Union[str, Iterator[str]]=None):\n",
    "        _rand_bool = np.random.randint(0,2,size=(self._n_samples, n_col), dtype=bool)\n",
    "        _rand_bool = pd.DataFrame(_rand_bool)\n",
    "        if col_name is not None:\n",
    "            _rand_bool.columns = [col_name]\n",
    "        self._concatenate(_rand_bool)\n",
    "        if col_name is not None:\n",
    "            self._features_names.append(col_name)\n",
    "            \n",
    "    def add_character_values(self,\n",
    "                             n_col:int=1,\n",
    "                             col_name: Union[str, Iterator[str]]=\"Gender\",\n",
    "                            char_values: List[str]=['MAN', 'WOMAN']):\n",
    "        \n",
    "        n_values = len(char_values)\n",
    "        _rand_values = np.random.randint(0,n_values,size=(self._n_samples, n_col))\n",
    "        _rand_string = {}\n",
    "        \n",
    "        if col_name is None:\n",
    "            col_name = list(range(n_col))\n",
    "        \n",
    "        if isinstance(col_name, str):\n",
    "            col_name = [col_name]  # convert into a list so `col_name` can be iterable\n",
    "        for i, col in enumerate(col_name):\n",
    "            _rand_string[col] = [char_values[x] for x in _rand_values[:, i]]\n",
    "\n",
    "        _rand_string = pd.DataFrame(_rand_string)\n",
    "        self._concatenate(_rand_string)\n",
    "        \n",
    "    def shuffle_columns(self):\n",
    "        pass\n",
    "    \n",
    "    def shuffle_samples(self):\n",
    "        _idx = np.arange(self._n_samples)\n",
    "        np.random.shuffle(_idx)\n",
    "        self._array = self._array.iloc[_idx]\n",
    "        print('dataset samples shuflled')\n",
    "    \n",
    "    def get_multi_index_dataframe(self):\n",
    "        _dataset = self.get_multi_view_dataset()\n",
    "        _multi_index_df = create_multi_view_dataframe(_dataset)\n",
    "        return _multi_index_df\n",
    "    \n",
    "    def get_multi_view_dataset(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"gets tab\"\"\"\n",
    "        if not self._views:\n",
    "            self._views['view_0'] = self._array\n",
    "        if self._primary_key is not None:\n",
    "            # set a primary key to all dataframe\n",
    "            for view_name in self._views.keys():\n",
    "                # iterate over all views\n",
    "                _primary_key = self._primary_key.values\n",
    "                if self._shuffle_primary_key:\n",
    "                        np.random.shuffle(_primary_key)\n",
    "                _primary_key = pd.DataFrame(_primary_key)\n",
    "                \n",
    "                if hasattr(_primary_key, 'name'):\n",
    "                    # case if primary key is a pandas series\n",
    "                    _primary_key_name = self._primary_key.name\n",
    "                else:\n",
    "                    # case where primary key is a dataframe\n",
    "                    _primary_key_name = self._primary_key.columns[0]\n",
    "                _primary_key.columns = [_primary_key_name]\n",
    "                \n",
    "                if _primary_key_name not in self._views[view_name].columns.values:\n",
    "                    # case where primary key is not present\n",
    "                    \n",
    "                    \n",
    "                    self._views[view_name] = pd.concat([self._views[view_name],\n",
    "                                                        _primary_key], axis=1)\n",
    "                else:\n",
    "                    # case where primary key is present: we are updating just in case\n",
    "\n",
    "                    self._views[view_name][_primary_key_name] = self._primary_key\n",
    "        return self._views\n",
    "\n",
    "    def save_multi_view_dataframe(self, path_folder:str):\n",
    "        \"\"\"saves multiview dataframe (folder containing multiple csvs)\"\"\"\n",
    "        _multi_view_dataframe = self.get_multi_view_dataset()\n",
    "        os.mkdir(path_folder)\n",
    "        \n",
    "        if self._primary_key is not None:\n",
    "            _index = False  # if primary key is set, remove indexes\n",
    "        else:\n",
    "            _index = True\n",
    "        for name in _multi_view_dataframe.keys():\n",
    "            file_name = os.path.join(path_folder, name)\n",
    "            _multi_view_dataframe[name].to_csv(file_name, index=_index)\n",
    "        print(f'multi view dataset saved at {file_name}')\n",
    "        \n",
    "    def _concatenate(self, array):\n",
    "        if self._array is None:\n",
    "            self._array = array\n",
    "        else:\n",
    "            _act_col_names = self._array.columns.values.tolist()\n",
    "            #_act_col_names.extend(array.columns.values.tolist())\n",
    "            self._array = pd.concat([self._array, array], axis=1)\n",
    "            _curr_col_name = self._array.columns.values \n",
    "\n",
    "            _act_col_names = reformate_col_name(_act_col_names)\n",
    "            _curr_col_name = reformate_col_name(_curr_col_name)\n",
    "            # needed for formatting columns name accordingly\n",
    "            # (otherwise concatenation happen badly)\n",
    "            \n",
    "            self._array = self._array.rename(columns={k:v for k, v in zip(_curr_col_name, _act_col_names)})\n",
    "            \n",
    "    \n",
    "    def _get_index(self, col_name:Union[str, int])->int:\n",
    "        if isinstance(col_name, str):\n",
    "            col_indx = self._array.columns.values.tolist().index(col_name)\n",
    "        elif isinstance(col_name, int):\n",
    "            col_indx = col_name\n",
    "        return col_indx\n",
    "    \n",
    "    @staticmethod        \n",
    "    def _check_if_valid_args(n_col, col_names):\n",
    "        if isinstance(col_names, (list, tuple)):\n",
    "            if n_col == len(col_names):\n",
    "                raise ValueError(f\"Mismatch: n_col ({n_col}) != len(col_names) ({len(col_names)})\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "cff3a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformate_col_name(col_names:List[Union[Tuple[str], int]]) -> List[Union[str, int]]:\n",
    "    \"\"\"reformates names of columns contained in list\"\"\"\n",
    "    for i, name in enumerate(col_names):\n",
    "        if isinstance(name, tuple):\n",
    "            col_names[i] = col_names[i][0]\n",
    "    return col_names\n",
    "    \n",
    "\n",
    "def create_multi_view_dataframe(datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    _header_labels = ['views', 'feature_name']\n",
    "    # 1. create multiindex header\n",
    "\n",
    "    _feature_name_array = np.array([])  # store all feature names\n",
    "    _view_name_array = []  # store all views (ie modalities) names\n",
    "\n",
    "    _concatenated_datasets = np.array([])  # store dataframe values\n",
    "\n",
    "    for key in datasets.keys():\n",
    "        #_sub_dataframe_header.append(list(datasets[key].columns.values))\n",
    "        _feature_name_array = np.concatenate([_feature_name_array,\n",
    "                                              datasets[key].columns.values])\n",
    "        if len(_concatenated_datasets) <= 0:\n",
    "            # first pass \n",
    "            _concatenated_datasets = datasets[key].values\n",
    "        else:\n",
    "            # next passes\n",
    "            try:\n",
    "                _concatenated_datasets = np.concatenate(\n",
    "                                        [_concatenated_datasets,\n",
    "                                         datasets[key].to_numpy()\n",
    "                                         ], axis=1)\n",
    "            except ValueError as val_err:\n",
    "                # catching case where nb_samples are differents\n",
    "                raise ValueError(\n",
    "                    'Cannot create multi view dataset: different number of samples for each modality have been detected'\\\n",
    "                        + 'Details: ' + str(val_err)\n",
    "                    )\n",
    "        for _ in datasets[key].columns.values:\n",
    "            _view_name_array.append(key)\n",
    "\n",
    "    _header = pd.MultiIndex.from_arrays([_view_name_array,\n",
    "                                         _feature_name_array],\n",
    "                                        names=_header_labels)\n",
    "\n",
    "\n",
    "    # 2. create multi index dataframe\n",
    "\n",
    "    multi_view_df = pd.DataFrame(_concatenated_datasets,\n",
    "                                  columns = _header)\n",
    "    return multi_view_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b8093",
   "metadata": {},
   "source": [
    "## how to use Tabular data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4d504dd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tbg = TabularDataGenerator(100)\n",
    "\n",
    "# first create 4 columns containing only integers\n",
    "tbg.add_integers_values(4, ['a', 'e', 'i', 'o'])\n",
    "\n",
    "tbg.add_boolean_values(4)  # add 4 boolean columns\n",
    "\n",
    "tbg.add_datetime_values(\"2018-01-01\", col_name='time')  # add datetime column\n",
    "tbg.add_float_values(2, col_name=['pressure', 'sp02']) # add float\n",
    "tbg.add_integers_values(4, ['a', 'e', 'i', 'o'])\n",
    "\n",
    "tbg.add_character_values(col_name=\"gender\")  #add character values\n",
    "tbg.add_character_values(col_name=\"blood type\", char_values=[\"A\", \"B\", \"O\", \"AB\"])  # add values whithin [\"A\", \"B\", \"O\", \"AB\"]\n",
    "tbg.add_missing_samples_to_column(\"blood type\")  # add missing values whithin 'blood type'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10a4632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "907e2fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbg._array['blood type'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6a9eb661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe for view None\n",
      "dataset samples shuflled\n",
      "Dataframe for view None\n",
      "Dataframe for view None\n"
     ]
    }
   ],
   "source": [
    "tbg.add_primary_key('pkey', dtype='str')  # add a primary key\n",
    "tbg.set_view('file1')  # define the first view\n",
    "\n",
    "\n",
    "tbg.add_boolean_values(4) # add data into the second view\n",
    "tbg.add_datetime_values(\"2018-01-01\", col_name='time')\n",
    "tbg.add_float_values(col_name='pH')\n",
    "tbg.shuffle_samples()\n",
    "tbg.shuffle_primary_key()  # shuffle primary key\n",
    "tbg.set_view('file2')\n",
    "\n",
    "tbg.add_discrete_values(col_name=\"discrete\")\n",
    "tbg.add_character_values(col_name=\"city\", char_values=['Paris', 'Marseille', 'Lille'])\n",
    "tbg.set_view('contatct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b68c3a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file1':     discrete                  pkey\n",
       " 0       39.0  ofhdbuktd iuwavnstge\n",
       " 1       70.0  q tfysafqbvz mcpy zw\n",
       " 2       28.0  mby avwhugriwmmolmzw\n",
       " 3       50.0  qqkvlrynigercafefifz\n",
       " 4       23.0  skotisyjg gehjomedp \n",
       " ..       ...                   ...\n",
       " 95      25.0  idpqmtzfodngftmzqrbb\n",
       " 96      94.0  jcpgyduxuxh jhlqbmni\n",
       " 97      50.0  jzheeqfylbjnqzrfmvev\n",
       " 98      69.0  vf xvzhjyvhiywjjlacc\n",
       " 99      76.0  dq gisjidyjidvtqwaoa\n",
       " \n",
       " [100 rows x 2 columns],\n",
       " 'file2':         0      1      2      3                time        pH  \\\n",
       " 0   False  False   True  False 2018-01-01 00:00:00  0.946474   \n",
       " 1    True   True  False  False 2018-01-01 01:00:00  0.513771   \n",
       " 2   False   True   True  False 2018-01-01 02:00:00  0.577206   \n",
       " 3    True  False   True  False 2018-01-01 03:00:00  0.179228   \n",
       " 4    True   True  False   True 2018-01-01 04:00:00  0.823091   \n",
       " ..    ...    ...    ...    ...                 ...       ...   \n",
       " 95  False   True   True  False 2018-01-04 23:00:00  0.437368   \n",
       " 96  False  False  False  False 2018-01-05 00:00:00  0.758644   \n",
       " 97  False  False  False  False 2018-01-05 01:00:00  0.293818   \n",
       " 98   True  False  False  False 2018-01-05 02:00:00  0.974204   \n",
       " 99   True  False  False  False 2018-01-05 03:00:00  0.251343   \n",
       " \n",
       "                     pkey  \n",
       " 0   yzcvwedbdyviwpiercnt  \n",
       " 1   wgciwarlqge kwqreogs  \n",
       " 2   jcpgyduxuxh jhlqbmni  \n",
       " 3   idpqmtzfodngftmzqrbb  \n",
       " 4   q tfysafqbvz mcpy zw  \n",
       " ..                   ...  \n",
       " 95  iyamnvmzaghylkay mgv  \n",
       " 96  nbeldsuipvzh eiceqnb  \n",
       " 97  sjzexkmfgjgswvusqwfu  \n",
       " 98  spoazgtfgoqkkgxggiiq  \n",
       " 99  huudufotlsk cpjbufpe  \n",
       " \n",
       " [100 rows x 7 columns],\n",
       " 'contatct':     discrete       city                  pkey\n",
       " 0       78.0  Marseille  jlwdtj clttncwapljrg\n",
       " 1       69.0  Marseille  fyslcjtultnvhnitwedr\n",
       " 2       97.0      Lille  huudufotlsk cpjbufpe\n",
       " 3       74.0      Lille   vtq erckgpuhijpyete\n",
       " 4       98.0      Lille  phitynjgynzjjbeojjmp\n",
       " ..       ...        ...                   ...\n",
       " 95      35.0      Lille  kghflaxftqfxqxymqdbv\n",
       " 96       2.0  Marseille  yptwqim hiqbqiatxcmv\n",
       " 97      93.0      Paris  hfjmqarwpsmzmgbjdgey\n",
       " 98       7.0  Marseille  jcpgyduxuxh jhlqbmni\n",
       " 99      25.0  Marseille  diwjpiewnddpvucimypr\n",
       " \n",
       " [100 rows x 3 columns]}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbg.get_multi_view_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cfda1cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>time</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>0.840792</td>\n",
       "      <td>vajimsztatlzitquscsm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>0.551657</td>\n",
       "      <td>duwkyykyzzqsoyoy jho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>0.787221</td>\n",
       "      <td>q xwmr ngpy igy rwzj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>0.862961</td>\n",
       "      <td>dgfzmbispegnvnmfxtew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>0.471008</td>\n",
       "      <td>yutmqvuc iqdbemdlbhb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2018-01-04 23:00:00</td>\n",
       "      <td>0.616472</td>\n",
       "      <td>nfahgscvorcbhdeyuu c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>0.788425</td>\n",
       "      <td>zwpugiyenckkizvwjpjr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-05 01:00:00</td>\n",
       "      <td>0.140394</td>\n",
       "      <td>mfavhdtoropccdkpdgwb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>0.747228</td>\n",
       "      <td>aztefxvtzrcrfmlhgkgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-01-05 03:00:00</td>\n",
       "      <td>0.268992</td>\n",
       "      <td>prexnfprmxnoqpcsilcf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1      2      3                time         0  \\\n",
       "0   False  False  False   True 2018-01-01 00:00:00  0.840792   \n",
       "1    True   True  False  False 2018-01-01 01:00:00  0.551657   \n",
       "2   False   True   True   True 2018-01-01 02:00:00  0.787221   \n",
       "3    True  False   True   True 2018-01-01 03:00:00  0.862961   \n",
       "4   False  False   True   True 2018-01-01 04:00:00  0.471008   \n",
       "..    ...    ...    ...    ...                 ...       ...   \n",
       "95   True  False   True   True 2018-01-04 23:00:00  0.616472   \n",
       "96  False   True   True  False 2018-01-05 00:00:00  0.788425   \n",
       "97  False  False   True  False 2018-01-05 01:00:00  0.140394   \n",
       "98  False   True  False  False 2018-01-05 02:00:00  0.747228   \n",
       "99  False   True  False  False 2018-01-05 03:00:00  0.268992   \n",
       "\n",
       "                       0  \n",
       "0   vajimsztatlzitquscsm  \n",
       "1   duwkyykyzzqsoyoy jho  \n",
       "2   q xwmr ngpy igy rwzj  \n",
       "3   dgfzmbispegnvnmfxtew  \n",
       "4   yutmqvuc iqdbemdlbhb  \n",
       "..                   ...  \n",
       "95  nfahgscvorcbhdeyuu c  \n",
       "96  zwpugiyenckkizvwjpjr  \n",
       "97  mfavhdtoropccdkpdgwb  \n",
       "98  aztefxvtzrcrfmlhgkgt  \n",
       "99  prexnfprmxnoqpcsilcf  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbg._views['file2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0afd5719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi view dataset saved at test7/file2\n"
     ]
    }
   ],
   "source": [
    "tbg.save_multi_view_dataframe('test7') # save mutliview dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823cb40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
