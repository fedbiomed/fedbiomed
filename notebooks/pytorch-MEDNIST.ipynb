{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fedbiomed Researcher base example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial shows how to do 2d image classification example on MedNIST dataset using pretrained PyTorch model Densnet121 https://pytorch.org/vision/main/generated/torchvision.models.densenet121.html.\n",
    "\n",
    "\n",
    "## Load MedNIST to the node\n",
    "### About MedNIST\n",
    "\n",
    "MedNIST provides an artificial 2d classification dataset created by gathering different medical imaging datasets from TCIA, the RSNA Bone Age Challenge, and the NIH Chest X-ray dataset. The dataset is kindly made available by Dr. Bradley J. Erickson M.D., Ph.D. (Department of Radiology, Mayo Clinic) under the Creative Commons CC BY-SA 4.0 license.\n",
    "\n",
    "MedNIST dataset is downloaded from the resources provided by the project MONAI:\n",
    "https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz\n",
    "\n",
    "The dataset MedNIST has 58954 images of size (3, 64, 64) distributed into 6 classes (10000 images per class except for BreastMRI class which has 8954 images). Classes are `AbdomenCT`, `BreastMRI`, `CXR`, `ChestCT`, `Hand`, `HeadCT`. It has the structure:\n",
    "\n",
    "\n",
    "‚îî‚îÄ‚îÄ MedNIST/\n",
    "\n",
    "    ‚îú‚îÄ‚îÄ AbdomenCT/\n",
    "    \n",
    "    ‚îî‚îÄ‚îÄ BreastMRI/\n",
    "    \n",
    "    ‚îî‚îÄ‚îÄ CXR/\n",
    "    \n",
    "    ‚îî‚îÄ‚îÄ ChestCT/\n",
    "    \n",
    "    ‚îî‚îÄ‚îÄ Hand/\n",
    "    \n",
    "    ‚îî‚îÄ‚îÄ HeadCT/   \n",
    "### About the model\n",
    "The model used is Densenet-121 model(‚ÄúDensely Connected Convolutional Networks‚Äù) pretrained on ImageNet dataset. The Pytorch pretrained Densenet121 is used https://pytorch.org/vision/main/generated/torchvision.models.densenet121.html to perform image classification on the MedNIST dataset. \n",
    "\n",
    "The goal of this Densenet121 model is to predict the class of the image modality given the medical image. For instance, if an image is a abdomen image taken from a CT scan, model should predict the class `AbdomenCT`.\n",
    "\n",
    "### Start the network\n",
    "\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`\n",
    "\n",
    "### Setup the node\n",
    "\n",
    "1. Populate the node with MedNIST dataset`./scripts/fedbiomed_run node add`\n",
    "  * Select option 3 (mednist) to add MedNIST to the node\n",
    "  * Confirm mednist tags ['#MEDNIST', '#dataset'] by hitting \"y\" and ENTER\n",
    "  * Select the folder where MedNIST is downloaded (It will be downloaded if it is not found in the selected path)\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "3. Start the node using `./scripts/fedbiomed_run node run`. Wait until you get `Starting task manager`. it means you are online.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Fed-Biomed Researcher\n",
    "\n",
    "We are now ready to start the researcher environment with the command source ./scripts/fedbiomed_environment researcher, and open the Jupyter notebook.\n",
    "\n",
    "To make sure that MedNIST dataset is loaded in the node we can send a request to the network to list the available dataset in the node. The list command should output an entry for mednist data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 12:37:29,277 fedbiomed INFO - Component environment:\n",
      "2022-04-20 12:37:29,278 fedbiomed INFO - type = ComponentType.RESEARCHER\n",
      "2022-04-20 12:37:29,447 fedbiomed INFO - Messaging researcher_ef98c6d7-a829-46b5-b4ed-3ba4119aaf8d successfully connected to the message broker, object = <fedbiomed.common.messaging.Messaging object at 0x7efda7dd8310>\n",
      "2022-04-20 12:37:29,510 fedbiomed INFO - Listing available datasets in all nodes... \n",
      "2022-04-20 12:37:39,531 fedbiomed INFO - \n",
      " Node: node_fd6cd39e-646a-4f38-a75d-674dc3e877c5 | Number of Datasets: 2 \n",
      "+---------+-------------+--------------------------+-----------------+--------------------+\n",
      "| name    | data_type   | tags                     | description     | shape              |\n",
      "+=========+=============+==========================+=================+====================+\n",
      "| MEDNIST | mednist     | ['#MEDNIST', '#dataset'] | MEDNIST dataset | [58954, 3, 64, 64] |\n",
      "+---------+-------------+--------------------------+-----------------+--------------------+\n",
      "| nn      | csv         | ['data']                 |                 | [1000, 16]         |\n",
      "+---------+-------------+--------------------------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_fd6cd39e-646a-4f38-a75d-674dc3e877c5': [{'name': 'MEDNIST',\n",
       "   'data_type': 'mednist',\n",
       "   'tags': ['#MEDNIST', '#dataset'],\n",
       "   'description': 'MEDNIST dataset',\n",
       "   'shape': [58954, 3, 64, 64]},\n",
       "  {'name': 'nn',\n",
       "   'data_type': 'csv',\n",
       "   'tags': ['data'],\n",
       "   'description': '',\n",
       "   'shape': [1000, 16]}]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fedbiomed.researcher.requests import Requests\n",
    "req = Requests()\n",
    "req.list(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use for developing (autoreloads changes made across packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an experiment to train a model on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a torch.nn MyTrainingPlan class to send for training on the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import densenet121\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# we will use the densnet121 model\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    def __init__(self, model_args: dict = {}):\n",
    "        super(MyTrainingPlan, self).__init__(model_args)\n",
    "        \n",
    "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
    "        # In this case, we need the torch DataLoader classes\n",
    "        # Since we will train on MedNIST, we need datasets and transform from torchvision\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "                \"from torchvision.models import densenet121\"]\n",
    "        \n",
    "        self.add_dependency(deps)\n",
    "        \n",
    "        self.model =  densenet121(pretrained=True)\n",
    "        # We re-define the final fully-connected the layer\n",
    "        self.model.classifier =nn.Sequential(nn.Linear(1024,512), nn.Softmax())\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        # Custom torch Dataloader for MedNIST data\n",
    "        print(\"dataset path\",self.dataset_path)\n",
    "        preprocess = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(\n",
    "                                            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                                        )])\n",
    "        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset=train_data, **train_kwargs)\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.forward(data)\n",
    "        loss   = self.loss_function(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side.\n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. ü§ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48, \n",
    "    'lr': 1e-3, \n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and run the experiment\n",
    "\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `round_limit` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MEDNIST', '#dataset']\n",
    "rounds = 2\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 model_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)\n",
    "\n",
    "exp.set_test_ratio(.1)\n",
    "exp.set_test_on_global_updates(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `round_limit` rounds are done for all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp.run(rounds=rounds, increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local training results for each round and each node are available via `exp.training_replies()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the training results for the last round below.\n",
    "\n",
    "Different timings (in seconds) are reported for each dataset of a node participating in a round :\n",
    "- `rtime_training` real time (clock time) spent in the training function on the node\n",
    "- `ptime_training` process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total` real time (clock time) spent in the researcher between sending the request and handling the response, at the `Job()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())\n",
    "\n",
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "round_data = exp.training_replies()[rounds - 1].data()\n",
    "for c in range(len(round_data)):\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n",
    "        rtraining = round_data[c]['timing']['rtime_training'],\n",
    "        ptraining = round_data[c]['timing']['ptime_training'],\n",
    "        rtotal = round_data[c]['timing']['rtime_total']))\n",
    "print('\\n')\n",
    "    \n",
    "exp.training_replies()[rounds - 1].dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated parameters for each round are available via `exp.aggregated_params()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the federated parameters for the last round of the experiment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feel free to run other sample notebooks or try your own models :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
