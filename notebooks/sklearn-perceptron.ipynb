{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e87007",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fed-BioMed Researcher to train a federated scikit learn model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e17d2f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Purpose of the exercise :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb782b14",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Three datasets `perp-c1.csv` , `perp-c2.csv` and `perp-c3.csv` have been generated with a target column of 3 different classes.\n",
    "We will fit a Perceptron (classifier) using Federated Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf562e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extending this notebook to any incremental learning scikit model:\n",
    "\n",
    "The same federated learning scheme below applies to any sklearn model supporting the method partial_fit():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98bbac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A family of models could be naturally imported in Fed-BioMed, following the same approach. For example: \n",
    "- Naive Bayes.  \n",
    "- Logistic regression,\n",
    "- SVM/SVC (linear and non-linear), \n",
    "- perceptron, \n",
    "- KMeans, \n",
    "- incremental PCA, \n",
    "- mini batch dictionary learning, \n",
    "- latent Dirichlet annotation, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010d38f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get the data \n",
    "\n",
    "We use the make_classification dataset from sklearn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e08d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c026f9e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X,y = datasets.make_classification(n_samples=300, n_features=20,n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0,shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba7b80",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddbce9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691e48c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "C1 = X[:150,:]\n",
    "C2 = X[150:250,:]\n",
    "C3 = X[250:300,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7323c4db",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y1 = y[:150].reshape([150,1])\n",
    "y2 = y[150:250].reshape([100,1])\n",
    "y3 = y[250:300].reshape([50,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c6228",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "C1.shape ,C2.shape , C3.shape , y1.shape, y2.shape, y3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad768917",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "C2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f383af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n1 = np.concatenate((C1, y1), axis=1)\n",
    "np.savetxt('./data/perp-c1.csv',n1,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb72c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n2 = np.concatenate((C2, y2), axis=1)\n",
    "np.savetxt('./data/perp-c2.csv',n2,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2bb9fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n3 = np.concatenate((C3, y3), axis=1)\n",
    "np.savetxt('./data/perp-c3.csv',n3,delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f160e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setting the node up\n",
    "Before running this notebook you need to configure 2 nodes: <br/>\n",
    "* **Node 1 :** `fedbiomed node --path node-1 dataset add`\n",
    "  * Select option 1 to add a csv file to the node\n",
    "  * Choose the name, tags and description of the dataset (you can write 'perp' always and it will be good)\n",
    "  * Pick the perp-c1.csv file in your machine.\n",
    "  * Check that your data has been added in node 1 by executing `fedbiomed node --path node-1 dataset list`\n",
    "  * Run the node using `fedbiomed node --path node-1 start`. <br/>\n",
    "\n",
    "* **Node 2 :** Open a second terminal and run `fedbiomed node -d node-2 dataset add` \n",
    "  * Select option 1 to add a csv file to the node\n",
    "  * Choose the name, tags and description of the dataset (you can write 'perp' always and it will be good)\n",
    "  * Pick the perp-c2.csv file in your machine.\n",
    "  * Check that your data has been added in node 2 by executing `fedbiomed node --path ./node-2 dataset list`\n",
    "  * Run the node using `fedbiomed node --path node-2 start`.\n",
    " \n",
    "\n",
    " Wait until you get `Starting task manager`, it means node is online.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f91e69a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**model_args** is a dictionnary containing your model arguments, in case of SGDRegressor this will be max_iter and tol.\n",
    "\n",
    "**training_args** is a dictionnary with parameters , related to Federated Learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6cc967",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.common.metrics import MetricTypes\n",
    "\n",
    "n_features = 20\n",
    "n_classes = 2\n",
    "\n",
    "model_args = { 'max_iter':1000, 'tol': 1e-3 , \n",
    "               'n_features' : n_features, 'n_classes' : n_classes}\n",
    "\n",
    "training_args = {   \n",
    "    'epochs': 5,\n",
    "    'loader_args': { 'batch_size': 1, },\n",
    "    'test_ratio': 0.2,\n",
    "    'test_metric': MetricTypes.ACCURACY,\n",
    "    'test_on_local_updates': True,\n",
    "    'test_on_global_updates': True,\n",
    "    'test_batch_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d695ffab",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hereafter the template of the class you should provide to Fedbiomed :\n",
    "    \n",
    "**training_data** : you must return here the (X,y) that must be of the same type of \n",
    "your method partial_fit parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6a6ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.common.training_plans import FedPerceptron\n",
    "from fedbiomed.common.data import DataManager\n",
    "class PerceptronTrainingPlan(FedPerceptron):\n",
    "    def training_data(self):\n",
    "        NUMBER_COLS = 20\n",
    "        dataset = pd.read_csv(self.dataset_path,header=None,delimiter=',')\n",
    "        X = dataset.iloc[:,0:NUMBER_COLS].values\n",
    "        y = dataset.iloc[:,NUMBER_COLS]\n",
    "        return DataManager(dataset=X,target=y.values,  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a1341",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.federated_workflows import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['perp']\n",
    "rounds = 5\n",
    "\n",
    "# search for corresponding datasets across nodes datasets\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=PerceptronTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff55da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train experiments\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ed174",
   "metadata": {},
   "source": [
    "Save trained model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c5838",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.training_plan().export_model('./trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2a782",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lets validate the trained model with the test dataset perp-c3.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4439e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b970bd9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/perp-c3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f579d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test = data.iloc[:,:n_features]\n",
    "y_test = data.iloc[:,n_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e6d380",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "F1 score computed with federated algorithm :\n",
    "\n",
    "For that, we are exporting `exp.aggregated_params()` containing models parameters collected at the end of each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb32ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "loss_metric = f1_score\n",
    "    \n",
    "testing_error = []\n",
    "\n",
    "for i in range(rounds):\n",
    "    fed_model = exp.training_plan().model()\n",
    "    fed_model.coef_ = exp.aggregated_params()[i]['params']['coef_']\n",
    "    fed_model.intercept_ = exp.aggregated_params()[i]['params']['intercept_']\n",
    "    metric = loss_metric(fed_model.predict(X_test),y_test.ravel())\n",
    "    print('F1 score metric: ', metric, )\n",
    "    testing_error.append(metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab76e3e624c52aae5e80807d730e4eaa4ecc8ddffbfd9d62b69327b02ed88a35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
