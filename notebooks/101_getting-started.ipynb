{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed-BioMed Researcher base example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use for developing (autoreloads changes made across packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the network\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the node up\n",
    "It is necessary to previously configure a node:\n",
    "1. `./scripts/fedbiomed_run node add`\n",
    "  * Select option 2 (default) to add MNIST to the node\n",
    "  * Confirm default tags by hitting \"y\" and ENTER\n",
    "  * Pick the folder where MNIST is downloaded (this is due to a pytorch issue https://github.com/pytorch/vision/issues/3549)\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "3. Run the node using `./scripts/fedbiomed_run node start`. Wait until you get `Starting task manager`. it means you are online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an experiment model and parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a TorchTrainingPlan subclass to send for training on the node.\n",
    "\n",
    "* This subclass implements the API-mandatory methods:\n",
    "  - `init_model`: define the torch model's architecture\n",
    "  - `init_loss`: define the loss function to use, as a torch Module\n",
    "  - `training_data`: define how to load the local dataset\n",
    "* It also implements:\n",
    "  - `init_optim`: define a default Adam optimizer (or use input config)\n",
    "  - automated call to `add_dependency` in the `__init__`, to enable model\n",
    "    saving and reloading through a python file storing its source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom TorchTrainingPlan subclass implementing a CNN on the MNIST dataset.\n",
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from fedbiomed.common.data import DataManager\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "\n",
    "\n",
    "class MnistTorchTrainingPlan(TorchTrainingPlan):\n",
    "    \"\"\"Custom torch training plan, implementing MNIST dataset loading.\n",
    "\n",
    "    This class also overrides model-creation behaviour and training\n",
    "    plan saving behavior to lighten dump files, at the cost of being\n",
    "    way more verbose than its `MnistTorchTrainingPlan` counterpart.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Instantiate the training plan.\"\"\"\n",
    "        super().__init__()\n",
    "        # Record all dependencies of this class's source code.\n",
    "        self.add_dependency([\n",
    "            \"from typing import Any, Dict\",\n",
    "            \"import torch\",\n",
    "            \"from torchvision import datasets, transforms\",\n",
    "            \"from fedbiomed.common.data import DataManager\",\n",
    "            \"from fedbiomed.common.training_plans import TorchTrainingPlan\",\n",
    "        ])\n",
    "\n",
    "    @staticmethod\n",
    "    def init_model(model_args: Dict[str, Any]) -> torch.nn.Module:\n",
    "        \"\"\"Set up the neural network that needs training.\"\"\"\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, 3, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 3, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Dropout(0.25),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(9216, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(128, 10),\n",
    "            torch.nn.LogSoftmax(),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def init_loss() -> torch.nn.Module:\n",
    "        \"\"\"Set up the model's loss-computing module.\"\"\"\n",
    "        return torch.nn.NLLLoss()\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_optim(optim_args: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Set up the optimizer's configuration.\n",
    "        \n",
    "        Use the input configuration, or a default Adam optimizer\n",
    "        if no parameters are actually input.\n",
    "        \"\"\"\n",
    "        if not optim_args:\n",
    "            return {\"lrate\": 0.001, \"modules\": [\"adam\"]}\n",
    "        return optim_args\n",
    "\n",
    "    def training_data(\n",
    "            self,\n",
    "            dataset_path: str,\n",
    "            batch_size: int = 48\n",
    "        ) -> DataManager:\n",
    "        \"\"\"Return a DataManager wrapping the MNIST dataset.\"\"\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        dataset = datasets.MNIST(\n",
    "            dataset_path, train=True, download=False, transform=transform\n",
    "        )\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset=dataset, **train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side.\n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48,\n",
    "    'epochs': 1,\n",
    "    'num_updates': 100  # fast pass for development\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and run the experiment\n",
    "\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `round_limit` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 2\n",
    "\n",
    "exp = Experiment(\n",
    "    tags=tags,\n",
    "    model_args=model_args,\n",
    "    training_plan=MnistTorchTrainingPlan(),\n",
    "    training_args=training_args,\n",
    "    round_limit=rounds,\n",
    "    aggregator=FedAverage(),\n",
    "    node_selection_strategy=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `round_limit` rounds are done for all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local training results for each round and each node are available via `exp.training_replies()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the training results for the last round below.\n",
    "\n",
    "Different timings (in seconds) are reported for each dataset of a node participating in a round :\n",
    "- `rtime_training` real time (clock time) spent in the training function on the node\n",
    "- `ptime_training` process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total` real time (clock time) spent in the researcher between sending the request and handling the response, at the `Job()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())\n",
    "\n",
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "round_data = exp.training_replies()[rounds - 1].data()\n",
    "for c in range(len(round_data)):\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n",
    "        rtraining = round_data[c]['timing']['rtime_training'],\n",
    "        ptraining = round_data[c]['timing']['ptime_training'],\n",
    "        rtotal = round_data[c]['timing']['rtime_total']))\n",
    "print('\\n')\n",
    "    \n",
    "exp.training_replies()[rounds - 1].dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated parameters for each round are available via `exp.aggregated_params()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the federated parameters for the last round of the experiment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding fancier optimizer modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here show how adding optmizaton modules can be done using declearn. Note that we use a declearn specific function, wrapping the dictionnary of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the node-side optimizer to use.\n",
    "# Here, Adam optimizer with 0.02 learning rate and default parameters.\n",
    "node_opt = {\"lrate\": 0.02, \"modules\": [\"yogi\"]}\n",
    "\n",
    "# Configure the researcher-side optimizer to use.\n",
    "# Here, apply momentum to the updates and apply them (as lr=1.0).\n",
    "researcher_opt = {\"lrate\": 1.0, \"modules\": [\"momentum\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifications:\n",
    "# * pass the optimizer strategy to the `FedAverage` aggregator \n",
    "#   (overriding the default which simply applies the averaged updates\n",
    "#   with lr=1.0)\n",
    "# * pass the node-side optimizer strategy as part of training args\n",
    "\n",
    "training_args[\"optimizer_args\"] = node_opt\n",
    "\n",
    "exp = Experiment(\n",
    "    tags=tags,\n",
    "    model_args=model_args,\n",
    "    training_plan=MnistTorchTrainingPlan(),\n",
    "    training_args=training_args,  # yogi version\n",
    "    round_limit=rounds,\n",
    "    aggregator=FedAverage(researcher_opt),  # pass the opt\n",
    "    node_selection_strategy=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scaffold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaffold is implement via a pair of optmizaton modules, that are to be added using the same syntax as before. It will in fact trigger additional behaviour by using a system of auxiliary variables that are exchanged in parallel of model parameters throughout training - but this all happens in the backend and does not change things from the user's perspective.\n",
    "\n",
    "Note that Scaffold can be combined with other node-side and/or researcher-side modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the node-side optimizer to use.\n",
    "# Here, use the ScaffoldClientModule, and simple SGD with 0.001 lr.\n",
    "node_opt = {\"lrate\": 0.001, \"modules\": [\"scaffold-client\"]}\n",
    "\n",
    "# Configure the researcher-side optimizer to use.\n",
    "# Here, use the ScaffoldServerModule to update shared weights,\n",
    "# but also apply momentum to the aggregated updates.\n",
    "researcher_opt = {\"lrate\": 1.0, \"modules\": [\"scaffold-server\", \"momentum\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And on the researcher side: same code as before as well.\n",
    "\n",
    "training_args[\"optimizer_args\"] = node_opt\n",
    "\n",
    "exp = Experiment(\n",
    "    tags=tags,\n",
    "    model_args=model_args,\n",
    "    training_plan=MnistTorchTrainingPlan(),\n",
    "    training_args=training_args,  # scaffold version\n",
    "    round_limit=rounds,\n",
    "    aggregator=FedAverage(researcher_opt),\n",
    "    node_selection_strategy=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feel free to run other sample notebooks or try your own models :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Oct 17 2022, 15:04:54) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "071a098b3622cdb998eb8f21015f9c99f68c7dae20cc45eaeec0783837b16779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
