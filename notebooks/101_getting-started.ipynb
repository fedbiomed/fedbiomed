{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed-BioMed Researcher base example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use for developing (autoreloads changes made across packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the network\n",
    "Before running this notebook, start the network with `./scripts/fedbiomed_run network`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the node up\n",
    "It is necessary to previously configure a node:\n",
    "1. `./scripts/fedbiomed_run node add`\n",
    "  * Select option 2 (default) to add MNIST to the node\n",
    "  * Confirm default tags by hitting \"y\" and ENTER\n",
    "  * Pick the folder where MNIST is downloaded (this is due to a pytorch issue https://github.com/pytorch/vision/issues/3549)\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it's been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `./scripts/fedbiomed_run node list`\n",
    "3. Run the node using `./scripts/fedbiomed_run node run`. Wait until you get `Starting task manager`. it means you are online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an experiment model and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a TorchTrainingPlan subclass to send for training on the node.\n",
    "\n",
    "* This subclass implements `training_data` (mandatory as per the API).\n",
    "* Additionnally, in the `MyTorchTrainingPlan` version, we package\n",
    "  the network's architecture as part of the python code rather than\n",
    "  using a pickle dump so that network communication costs remain light.\n",
    "* The `MnistTorchTrainingPlan` version uses the revised serialization API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication-lighter version of the training plan.\n",
    "# This is a hybrid between the new API and the old one.\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from fedbiomed.common.data import DataManager\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "\n",
    "\n",
    "class MyTorchTrainingPlan(TorchTrainingPlan):\n",
    "    \"\"\"Custom torch training plan, implementing MNIST dataset loading.\n",
    "\n",
    "    This class also overrides model-creation behaviour and training\n",
    "    plan saving behavior to lighten dump files, at the cost of being\n",
    "    way more verbose than its `MnistTorchTrainingPlan` counterpart.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def build_module() -> torch.nn.Module:\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, 3, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 3, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Dropout(0.25),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(9216, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(128, 10),\n",
    "            torch.nn.LogSoftmax(),\n",
    "        )\n",
    "\n",
    "    def __init__(self, model=None, optim=None) -> None:\n",
    "        \"\"\"Instantiate the training plan, forcing the model choice.\"\"\"\n",
    "        super().__init__(\n",
    "            model=self.build_module(),\n",
    "            optim=optim or {\"lrate\": 0.001},\n",
    "            loss=torch.nn.NLLLoss(),\n",
    "        )\n",
    "        self.add_dependency([\n",
    "            \"import json\",\n",
    "            \"import torch\",\n",
    "            \"from torchvision import datasets, transforms\",\n",
    "            \"from fedbiomed.common.data import DataManager\",\n",
    "            \"from fedbiomed.common.training_plans import TorchTrainingPlan\",\n",
    "        ])\n",
    "\n",
    "    def save_to_json(self, path) -> None:\n",
    "        # Override parent method to exclude torch module pickle.\n",
    "        super().save_to_json(path)\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            dump = json.load(file)\n",
    "        dump[\"model\"] = None\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(dump, file)\n",
    "\n",
    "    def training_data(self, dataset_path: str, batch_size: int = 48) -> DataManager:\n",
    "        \"\"\"Return a DataManager wrapping the MNIST dataset.\"\"\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        dataset = datasets.MNIST(\n",
    "            dataset_path, train=True, download=False, transform=transform\n",
    "        )\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset=dataset, **train_kwargs)\n",
    "\n",
    "\n",
    "# Instantiation.\n",
    "# Most things are delegated to the class itself (old-style).\n",
    "training_plan_light = MyTorchTrainingPlan(\n",
    "    optim={\"lrate\": 0.001}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API-abiding version of the previous code.\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from fedbiomed.common.data import DataManager\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "\n",
    "\n",
    "class MnistTorchTrainingPlan(TorchTrainingPlan):\n",
    "    \"\"\"Custom torch training plan, implementing MNIST dataset loading.\"\"\"\n",
    "\n",
    "    def training_data(\n",
    "            self,\n",
    "            dataset_path: str,\n",
    "            batch_size: int = 48,\n",
    "        ) -> DataManager:\n",
    "        \"\"\"Return a DataManager wrapping the MNIST dataset.\"\"\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        dataset = datasets.MNIST(\n",
    "            dataset_path, train=True, download=False, transform=transform\n",
    "        )\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset=dataset, **train_kwargs)\n",
    "\n",
    "\n",
    "# Instantiation.\n",
    "# Write the torch module - as one would in a centralized context.\n",
    "module = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 32, 3, 1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(32, 64, 3, 1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(2),\n",
    "    torch.nn.Dropout(0.25),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(9216, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(128, 10),\n",
    "    torch.nn.LogSoftmax(),\n",
    ")\n",
    "# Wrap up the torch module for federated training.\n",
    "training_plan = MnistTorchTrainingPlan(\n",
    "    model=module,\n",
    "    optim={\"lrate\": 0.001},\n",
    "    loss=torch.nn.NLLLoss(),\n",
    ")\n",
    "# Add requirements: due to the `training_data` part only.\n",
    "training_plan.add_dependency([\n",
    "    \"from torchvision import datasets, transforms\",\n",
    "    \"from fedbiomed.common.data import DataManager\",\n",
    "    \"from fedbiomed.common.training_plans import TorchTrainingPlan\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This group of arguments correspond respectively:\n",
    "* `model_args`: a dictionary with the arguments related to the model (e.g. number of layers, features, etc.). This will be passed to the model class on the node side.\n",
    "* `training_args`: a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side.\n",
    "\n",
    "**NOTE:** typos and/or lack of positional (required) arguments will raise error. ðŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48,\n",
    "    'epochs': 1, \n",
    "    'num_updates': 100  # fast pass for development\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and run the experiment\n",
    "\n",
    "- search nodes serving data for these `tags`, optionally filter on a list of node ID with `nodes`\n",
    "- run a round of local training on nodes with model defined in `model_path` + federation with `aggregator`\n",
    "- run for `round_limit` rounds, applying the `node_selection_strategy` between the rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 2\n",
    "\n",
    "exp = Experiment(\n",
    "    tags=tags,\n",
    "    model_args=model_args,\n",
    "    training_plan=training_plan_light,  # light version\n",
    "    training_args=training_args,\n",
    "    round_limit=rounds,\n",
    "    aggregator=FedAverage(),\n",
    "    node_selection_strategy=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start the experiment.\n",
    "\n",
    "By default, this function doesn't stop until all the `round_limit` rounds are done for all the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do it again, with the unlightened version.\n",
    "\n",
    "\n",
    "Runtime of `exp.run()` (without `num_updates=100`):\n",
    "* 1st version: 1min30\n",
    "* 2nd version: 1min30\n",
    "\n",
    "Note: we are on localhost, communication costs should be measured IRL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(\n",
    "    tags=tags,\n",
    "    model_args=model_args,\n",
    "    training_plan=training_plan,  # \"normal\" version\n",
    "    training_args=training_args,\n",
    "    round_limit=rounds,\n",
    "    aggregator=FedAverage(),\n",
    "    node_selection_strategy=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local training results for each round and each node are available via `exp.training_replies()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the training results for the last round below.\n",
    "\n",
    "Different timings (in seconds) are reported for each dataset of a node participating in a round :\n",
    "- `rtime_training` real time (clock time) spent in the training function on the node\n",
    "- `ptime_training` process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total` real time (clock time) spent in the researcher between sending the request and handling the response, at the `Job()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())\n",
    "\n",
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "round_data = exp.training_replies()[rounds - 1].data()\n",
    "for c in range(len(round_data)):\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n",
    "        rtraining = round_data[c]['timing']['rtime_training'],\n",
    "        ptraining = round_data[c]['timing']['ptime_training'],\n",
    "        rtotal = round_data[c]['timing']['rtime_total']))\n",
    "print('\\n')\n",
    "    \n",
    "exp.training_replies()[rounds - 1].dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated parameters for each round are available via `exp.aggregated_params()` (index 0 to (`rounds` - 1) ).\n",
    "\n",
    "For example you can view the federated parameters for the last round of the experiment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path'])\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding fancier optimizer modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here show how adding optmizaton modules can be done using declearn. Note that we use a declearn specific function, wrapping the dictionnary of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from declearn.optimizer import Optimizer\n"
    "from declearn.optimizer.modules import AdamModule, MomentumModule\n",
    "\n",
    "# Configure the node-side optimizer to use.\n",
    "# Here, Adam optimizer with 0.02 learning rate and default parameters.\n",
    "node_opt = Optimizer(\n",
    "    lrate=0.02,\n",
    "    modules=[AdamModule()],\n",
    ")\n",
    "\n",
    "# Configure the researcher-side optimizer to use.\n",
    "# Here, apply momentum to the updates and apply them (as lr=1.0).\n",
    "researcher_opt = Optimizer(\n",
    "    lrate=1.0,\n",
    "    modules=[MomentumModule()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the node side: same code as before, save for the `optim`\n",
    "# parameter of the TrainingPlan.\n",
    "\n",
    "# Same model architecture as before.\n",
    "module = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 32, 3, 1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(32, 64, 3, 1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(2),\n",
    "    torch.nn.Dropout(0.25),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(9216, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(128, 10),\n",
    "    torch.nn.LogSoftmax(),\n",
    ")\n",
    "\n",
    "# Pass along the optimizer at instantiation.\n",
    "training_plan_adam = MnistTorchTrainingPlan(\n",
    "    model=module,\n",
    "    optim=node_opt,\n",
    "    loss=torch.nn.NLLLoss(),\n",
    ")\n",
    "\n",
    "# Add training-data-related requirements, as before.\n",
    "training_plan_adam.add_dependency([\n",
    "    \"from torchvision import datasets, transforms\",\n",
    "    \"from fedbiomed.common.data import DataManager\",\n",
    "    \"from fedbiomed.common.training_plans import TorchTrainingPlan\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the researcher side: pass the optimizer strategy\n",
    "# to the `FedAverage` aggregator (overriding the default\n",
    "# which simply applies the averaged updates with lr=1.0).\n",
    "\n",
    "exp = Experiment(\n",
    "    tags=tags,\n",
    "    model_args=model_args,\n",
    "    training_plan=training_plan_adam,  # version with adam\n",
    "    training_args=training_args,\n",
    "    round_limit=rounds,\n",
    "    aggregator=FedAverage(researcher_opt),  # pass the opt\n",
    "    node_selection_strategy=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feel free to run other sample notebooks or try your own models :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
