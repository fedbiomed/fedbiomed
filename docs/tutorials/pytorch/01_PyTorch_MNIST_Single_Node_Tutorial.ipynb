{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab5b6f6a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PyTorch MNIST Basic Example\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial focuses on how to train a CNN model with Fed-BioMed nodes using the PyTorch framework on the MNIST dataset. You will learn; \n",
    "\n",
    "- How to prepare your environment to be able to train your model;\n",
    "- How to create a training plan class to run it in a single node which works as a different process in your local machine;\n",
    "- How to create a federated learning experiment;\n",
    "- How to load and inspect your model parameters;\n",
    "- How to test your model using test dataset.\n",
    "\n",
    "**Note:** In the following steps, we will be running this example using two nodes. \n",
    "\n",
    "## Before you start\n",
    "\n",
    "Before starting this tutorial please change your directory where you want to keep created Fed-BioMed components. This will allow you to keep Fed-BioMed component in a specific folder, and will ease specifying components using `fedbiomed` command. \n",
    " \n",
    "\n",
    "\n",
    "## 1. Configuring Nodes \n",
    "\n",
    "In this tutorial, you will learn how to train your model with a single Fed-BioMed node. Thus, you need to configure a node and add MNIST dataset into it.\n",
    "Please open a terminal and follow the steps below.    \n",
    "\n",
    "* **Configuration Steps:**\n",
    "    * Navigate to directory where you want to keep Fed-BioMed node and researcher component folders.\n",
    "    * Run `fedbiomed node dataset add` in the terminal. This command will automatically create a default node component folder called 'fbm-node'.\n",
    "    * It will ask you to select the data type that you want to add. The second option (which is the default) has been configured to add the MNIST dataset. Please type `2` and continue. \n",
    "    * Please use default tags which are `#MNIST` and `#dataset`.\n",
    "    * For the next step, please select the directory that you want to download the MNIST dataset. Usually, we may want to save them into the node component folder (here in `fbm-node/data/` folder).\n",
    "    * After the download is completed you will see the details of the MNIST dataset on the screen.\n",
    " \n",
    "Please run the command below in the same terminal to make sure the MNIST dataset is successfully added to the node.  \n",
    "\n",
    "```\n",
    "$ fedbiomed node dataset list\n",
    "```\n",
    "\n",
    "Then launch the `Node` by executing:\n",
    "\n",
    "```\n",
    "$ fedbiomed node start\n",
    "```\n",
    "\n",
    "### Start your notebook\n",
    "\n",
    "You need to start a jupyter notebook and create a new notebook to be able to follow the tutorial. Please open a new terminal window and run the following command to start your notebook.\n",
    "\n",
    "```\n",
    "$ fedbiomed researcher start\n",
    "```\n",
    "\n",
    "The command above will create a component folder for researcher called \"fbm-researcher\" in the directory where this command is executed.**Note:** If you are having a problem understanding the steps above, we recommend you to follow the [installation tutorial](../../installation/0-basic-software-installation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc9b7c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. What is MNIST dataset?\n",
    "\n",
    "[MNIST](http://yann.lecun.com/exdb/mnist/) dataset contains 60000 grayscale images (of size 28 * 28 pixels) of handwritten digits between 0 and 9. MNIST is commonly used for image classification task: the goal is to classify each image by assigning it to the correct digit. \n",
    "\n",
    "For a better visual understanding, we display a few samples from MNIST testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c41c9d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66b023",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from fedbiomed.researcher.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f6624",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "# Get the test dataset\n",
    "test_set = datasets.MNIST(root = os.path.join(config.vars['TMP_DIR'], 'mnist_testing.tmp'),\n",
    "                          download = True, train = False, transform = transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85a5e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# display a few digits from MNIST testing dataset\n",
    "\n",
    "nb_digits_to_display = 10\n",
    "\n",
    "plt.figure(figsize=(10,2)) \n",
    "plt.title(\"Few images of MNIST dataset\")\n",
    "for i in range(nb_digits_to_display):\n",
    "    plt.subplot(1,nb_digits_to_display, i+1)\n",
    "    plt.imshow(test_set.data[i].numpy())\n",
    "    plt.title(f\"label: {test_set.targets[i].numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4877a56",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Training a model \n",
    "\n",
    "In this section, you will learn how to train a model in Fed-BioMed, by creating a training plan class that includes special methods to retrieve mode, optimizer, training data as well as training step to execute at each iteration of training. Then the federated training will be launched by wrapping the training plan in an experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2963c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1 Creating A PyTorch Training Plan\n",
    "A PyTorch training plan is a Python class that inherits from `fedbiomed.common.training_plans.TorchTrainingPlan` which is an abstract class. The abstract methods `init_model`, `training_data` and `training_step` should be provided in the training plan to retrieve/execute model (`nn.Module`), data loader and training actions at each iteration.\n",
    "\n",
    "<div class=\"note\">\n",
    "    <p>\n",
    "        Please visit <a href=\"../../../user-guide/researcher/training-plan/\">training plan user guide</a> for more information and other special methods of training plan.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c3f83",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Here we define the training plan to be used.\n",
    "# You can use any class name (here 'MyTrainingPlan')\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, model_args):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "            self.dropout1 = nn.Dropout(0.25)\n",
    "            self.dropout2 = nn.Dropout(0.5)\n",
    "            self.fc1 = nn.Linear(9216, 128)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = self.dropout1(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "            output = F.log_softmax(x, dim=1)\n",
    "            return output\n",
    "\n",
    "    def init_model(self, model_args):\n",
    "        return self.Net(model_args = model_args)\n",
    "\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        return Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n",
    "\n",
    "    def init_dependencies(self):\n",
    "        return [\"from torchvision import datasets, transforms\",\n",
    "                \"from torch.optim import Adam\"]\n",
    "\n",
    "    def training_data(self):\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        loader_arguments = { 'shuffle': True}\n",
    "        return DataManager(dataset1, **loader_arguments)\n",
    "\n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d08c12d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"note\">\n",
    "    <p>\n",
    "        Fed-BioMed nodes can be configured to accept only approved training plans. Under this configuration, the training plan files that are sent by a researcher must be approved by the node in advance. For more details, you can visit the tutorial for <a href=\"../../security/training-with-approved-training-plans\">working with approved training plan file</a> and <a href=\"../../../user-guide/nodes/training-plan-security-manager\">user guide for managing nodes.</a>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118afa22",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2 Define an Experiment\n",
    " \n",
    " An experiment is a class that orchestrates the training processes that run on different nodes.\n",
    "\n",
    " - `model_arg` includes arguments that will pass into `init_model` method of training plan class. In our case, we don't need to add any arguments.\n",
    " - `training_args` includes arguments related to optimizer, data loader and training step/routine such as learning rate, number of epochs.\n",
    " - `tags` is a list that includes tags that are going to be used for searching related datasets in nodes. In our case, we saved the MNIST dataset with #MNIST and #dataset tags.\n",
    " - `rounds` represents the number of training rounds that will be applied in nodes. In each round every node complete epochs and send model parameters to the experiment.\n",
    "\n",
    "!!! note \"Validation step on testing dataset\"\n",
    "    The training arguments can also include testing parameters to define how the trained model performs for both globally and locally updated model weights after local training. These parameters allow you to reserve a portion of the training dataset for validation. The node retains the initially selected validation subset and uses the same samples for subsequent training rounds. However, if `shuffle_testing_dataset` is set to true for a given round, the dataset will be reshuffled, creating a new validation subset. Additionally, modifying the test ratio will reset the testing dataset, leading to a newly selected validation set that differs from previous training rounds. For more information please visit [testing documentaiton](../../user-guide/researcher/model-testing-during-federated-training.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74e0e1c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.common.metrics import MetricTypes\n",
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'loader_args': { 'batch_size': 48, },\n",
    "    'optimizer_args': {\n",
    "        'lr': 1e-3\n",
    "    },\n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100, # Fast pass for development : only use ( batch_maxnum * batch_size ) samples,\n",
    "    'test_ratio' : 0.25,\n",
    "    'test_metric': MetricTypes.F1_SCORE,\n",
    "    'test_on_global_updates': True,\n",
    "    'test_on_local_updates': True,\n",
    "    'test_batch_size': 0,\n",
    "    'shuffle_testing_dataset': True,\n",
    "}\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d4efe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training plan class should be passed to the experiment. The experiment will be responsible for uploading the training plan file to the file repository. Afterwards, the nodes will receive training request that includes the URL where the training plan class is stored.\n",
    "\n",
    "Finally, you should indicate which method should be chosen to aggregate model parameters after every round. The basic federation scheme is federated averaging, implemented in Fed-BioMed in the class  `FedAverage`.\n",
    "\n",
    "### 3.3 What happens during the initialization of an experiment?\n",
    "\n",
    "1. The experiment searches for `Nodes` whose available datasets have been saved with tags indicated in the `tags` argument. Nodes are selected based on a node selection strategy.\n",
    "2. The experiment sets up according to the provided arguments\n",
    "\n",
    "For more details, you can visit [`Experiment` webpage](../../../user-guide/researcher/experiment).\n",
    "\n",
    "Now, let's create our experiment. Since only one node has been created, the experiment will only find a single node for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9b8d2",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.federated_workflows import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25747d3d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As an output, you should see the log message from the node which dispose of the MNIST dataset. It means that the search request that contains `#MNIST, #dataset` tags has been successfully received by the node. In the example displayed here, we received messages only from one `Node`, which we created before. *Obviously, it is non-sensical to train a `Federated Learning` model with only one `Node`, but this tutorial is just here for the sake of demonstration.* \n",
    "\n",
    "The experiment also receives loss values during training on each node. In Fed-BioMed, it is possible to use a tensorboard to display loss values during training. Please refer to Fed-BioMed's [tensorboard documentation](../../../user-guide/researcher/tensorboard) for how to enable the tensorboard.\n",
    "\n",
    "Now, let's run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8e54d",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfcd5a4",
   "metadata": {},
   "source": [
    "Save trained model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24174e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.training_plan().export_model('./trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45284e9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After running the experiment, according to the provided arguments 4 training rounds should be completed on the node that you created. You can check the node id from the output and compare it with the node id which is defined in the config.ini file. After the process is finished, you are ready to inspect the model parameters. \n",
    "\n",
    "### 3.4 Extracting Training Results\n",
    "\n",
    "#### Timing \n",
    "\n",
    "Training replies for each round are available via `exp.training_replies()` (index 0 to (`rounds` - 1) ). You can display the keys of each round by running the following script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e400c831",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b71f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's see how training details can be accessed from `training_replies()`. The following parameters will be inspected;\n",
    "\n",
    "- `rtime_training` : Real-time (clock time) spent in the training function on the node \n",
    "- `ptime_training`: Process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total`   : Real-time (clock time) spent in the researcher between sending training requests and handling the responses\n",
    "\n",
    "_Note: The following code accesses the training replies of the last round of the experiment._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26f4b1",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())\n",
    "\n",
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "round_data = exp.training_replies()[rounds - 1]\n",
    "for r in round_data.values():\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = r['node_id'],\n",
    "        rtraining = r['timing']['rtime_training'],\n",
    "        ptraining = r['timing']['ptime_training'],\n",
    "        rtotal = r['timing']['rtime_total']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638d0cbc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Federated Parameters\n",
    "Federated model parameters for each round are available via `exp.aggregated_params()` (index 0 to (`rounds` - 1) ).\n",
    "For example, you can easily view the federated parameters for the last round of the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3699adf",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nAccess the federated params for the last training round : \")\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa002f83",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, to access specific parameters of last round:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae944d",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\t- Parameters of CONV1 layer's biases of last round: \\n\", exp.aggregated_params()[rounds - 1]['params']['conv1.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15bfe6b",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# access the TrainingPlan\n",
    "exp.training_plan()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d62c9a9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. PyTorch MNIST Example with Two Nodes\n",
    "\n",
    "In this section, we will be working on two nodes. Following the previous example, the experiment and training routine will remain unchanged.  Therefore, you just need to configure another node, and add the MNIST dataset with the default tags.\n",
    "\n",
    "### 4.1 Configuring Second Node\n",
    "\n",
    "While creating a second node you need to be careful with the node that has already been created. To configure the second node, a different config file has to be defined. Please follow the steps below to configure your second node. \n",
    "\n",
    "1. Please open a new terminal and cd into the base directory of fedbiomed.\n",
    "2. In this step, you need to name a new config file using the `--path` parameter. Instead of downloading a new MNIST dataset, you can use the one that you already downloaded in the previous example. To do so please run `fedbiomed node --path ./my-second-node dataset add --mnist /path/to/your/mnist`. Otherwise you can still do a `fedbiomed node --path ./my-second-node dataset add` as shown before. It will create a component directory called `my-second-node` in the directory where the command is executed.\n",
    "3. You need to start the new node by indicating the newely created config file. Please run the following command to start the second node `fedbiomed node --path ./my-second-node start`\n",
    "\n",
    "### 4.2 Defining an Experiment \n",
    "\n",
    "Since a training plan has already been created and saved in the previous example, you don't need to repeat this step here again: the same training plan with same model will be used for training. However, you can define a new experiment for testing purposes. The experiment will search the MNIST dataset in available nodes. Training arguments are kept the same as in the previous example.\n",
    "\n",
    "_You can also list datasets and select specific nodes to perform traning. You can visit [listing datasets and selecting nodes](../../../user-guide/researcher/listing-datasets-and-selecting-nodes) documentation to get more information._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc4c303",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.federated_workflows import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'loader_args': { 'batch_size': 48, },\n",
    "    'optimizer_args': {\n",
    "        'lr': 1e-3\n",
    "    },\n",
    "    'epochs': 1,\n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 4\n",
    "\n",
    "expN2 = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99b18a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can see from the output that the search request (done when initializing `Experiment`) has been sent to 2 `Nodes` by the experiment.\n",
    "\n",
    "Now, let's run the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047c5aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expN2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5240a0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Testing Federated Model\n",
    "\n",
    "In this section, we will create a test function to obtain accuracy, loss, and confusion matrix using the test MNIST dataset. \n",
    "\n",
    "### 5.1 Aggregated Parameters \n",
    "\n",
    "`training_plan()` returns the training plan and `training_plan().model()` returns the model that is created in the training plan.  It is possible to load specific aggregated parameters which are obtained in every round. Thereafter, it will be ready to make predictions using those parameters. The last round gives the last aggregated model parameters which represents the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c251b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fed_model = expN2.training_plan().model()\n",
    "fed_model.load_state_dict(expN2.aggregated_params()[rounds - 1]['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa271d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.2 Creating A Test Function\n",
    "\n",
    "Let's create a test function that returns loss, accuracy, and confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe10d8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "# Test function \n",
    "def testing_accuracy(model, data_loader,):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    device = 'cpu'\n",
    "\n",
    "    y_pred = []\n",
    "    y_actu = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            y_pred.extend(torch.flatten(pred).tolist()) \n",
    "            y_actu.extend(target.tolist())\n",
    "           \n",
    "    y_pred = pd.Series(y_pred, name='Actual')\n",
    "    y_actu = pd.Series(y_actu, name='Predicted')\n",
    "    cm = pd.crosstab(y_actu, y_pred)\n",
    "    correct = sum([cm.iloc[i,i] for i in range(len(cm))])\n",
    "    \n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100*correct/len(data_loader.dataset)\n",
    "\n",
    "    return(test_loss, accuracy, cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc7747c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will use the MNIST test dataset for testing our federated model. You can download this dataset set from `torchvision.dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ea693",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "# Get the test dataset\n",
    "# here we save the dataset under the `tmp` directory whithin Fed-BioMed folder\n",
    "test_set = datasets.MNIST(root = os.path.join(config.vars['TMP_DIR'], 'mnist_testing.tmp'), download = True, train = False, transform = transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103ee4b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, it is time to get performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f040bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_results = testing_accuracy(fed_model, test_loader)\n",
    "\n",
    "print(\"- Test Loss: \", test_results[0], \"\\n\")\n",
    "print(\"- Accuracy: \", test_results[1], \"\\n\")\n",
    "print(\"- Confusion Matrix: \\n \\n\",  test_results[2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ea7e2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.3 Creating Heatmap for Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55b9c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will use `matplotlib` for plotting results. If you have followed all the steps in this tutorial, you must have installed this library in the section 2. Otherwise, you can run `!pip install matplotlib` command in a notebook cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7830b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_matrix = test_results[2].to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "im = ax.imshow(conf_matrix)\n",
    "\n",
    "ax.set_xticks(np.arange(10))\n",
    "ax.set_yticks(np.arange(10))\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        text = ax.text(j, i, conf_matrix[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "        \n",
    "ax.set_xlabel('Actual targets')\n",
    "ax.set_ylabel('Predicted targets')\n",
    "ax.set_title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712585ed",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.4 Plotting Loss for Each round\n",
    "In this section, we will plot loss values that are obtained over the test dataset using model parameters of every round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12b8eb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "for i in range(rounds):\n",
    "    fed_model = expN2.training_plan().model()\n",
    "    fed_model.load_state_dict(expN2.aggregated_params()[i]['params'])\n",
    "    loss = testing_accuracy(fed_model, test_loader)[0]\n",
    "    errors.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15910d61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Plotting \n",
    "plt.plot(errors, label = 'Federated Test Loss')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Log Likelihood Loss evolution over number of rounds\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab76e3e624c52aae5e80807d730e4eaa4ecc8ddffbfd9d62b69327b02ed88a35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
