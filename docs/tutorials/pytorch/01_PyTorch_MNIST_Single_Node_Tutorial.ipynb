{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab5b6f6a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PyTorch MNIST Basic Example\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial focuses on how to train a CNN model with Fed-BioMed nodes using the PyTorch framework on the MNIST dataset. You will learn; \n",
    "\n",
    "- How to prepare your environment to be able to train your model\n",
    "- How to create a training plan class to run it in a single node which works as a different process in your local machine.\n",
    "- How to create a federated learning experiment. \n",
    "- How to load and inspect your model parameters. \n",
    "- How to test your model using test dataset.\n",
    "\n",
    "**Note:** In the following steps, we will be running this example using two nodes. \n",
    "\n",
    "## Before you start\n",
    "\n",
    "Before starting this tutorial please make sure that you have a clean environment. Additionally, please terminate running nodes if there is any. Please run the following command in the main Fed-BioMed directory to clean your environment. If you need more help to manage your environment please visit the tutorial for [setting up an enviroment](../../installation/1-setting-up-environment).\n",
    " \n",
    "\n",
    "```\n",
    "$ source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment clean\n",
    "```\n",
    "\n",
    "**Note**: `${FEDBIOMED_DIR}` is a path relative to based directory of the cloned Fed-BioMed repository. You can set it by running command `export FEDBIOMED_DIR=/path/to/fedbiomed`. This is not required for Fed-BioMed to work but enables you to run the tutorials more easily. \n",
    "\n",
    "## 1. Configuring Nodes \n",
    "\n",
    "In this tutorial, you will learn how to train your model with a single Fed-BioMed node. Thus, you need to configure a node and add MNIST dataset into it. Node configuration steps require `fedbiomed-node` conda environment. Please make sure that you have the necessary conda environment which is explained in the [installation tutorial](../../installation/0-basic-software-installation). You can check your environment by running the following command.\n",
    "\n",
    "```\n",
    "$ conda env list\n",
    "```\n",
    "If you have all Fed-BioMed environments you are good to go for the node configuration steps. \n",
    "\n",
    "Please open a terminal and follow the steps below.    \n",
    "\n",
    "* **Configuration Steps:**\n",
    "    * Run `${FEDBIOMED_DIR}/scripts/fedbiomed_run node add` in the terminal\n",
    "    * It will ask you to select the data type that you want to add. The second option (which is the default) has been configured to add the MNIST dataset. Please type `2` and continue. \n",
    "    * Please use default tags which are `#MNIST` and `#dataset`.\n",
    "    * For the next step, please select the directory that you want to download the MNIST dataset.\n",
    "    * After the download is completed you will see the details of the MNIST dataset on the screen.\n",
    " \n",
    "Please run the command below in the same terminal to make sure the MNIST dataset is successfully added to the node.  \n",
    "\n",
    "```\n",
    "$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node list\n",
    "```\n",
    "\n",
    "Before starting the node, please make sure that you have already launched the network using command `scripts/fedbiomed_run network`. Afterward, all you need to do is to start the node.\n",
    "\n",
    "\n",
    "```\n",
    "$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run node start\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Start your notebook\n",
    "\n",
    "You need to start a jupyter notebook and create a new notebook to be able to follow the tutorial. Please open a new terminal window and run the following command to start your notebook.\n",
    "\n",
    "```\n",
    "$ ${FEDBIOMED_DIR}/scripts/fedbiomed_run researcher start\n",
    "```\n",
    "\n",
    "\n",
    "**Note:** If you are having a problem understanding the steps above, we recommend you to follow the [installation tutorial](../../installation/0-basic-software-installation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc9b7c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. What is MNIST dataset?\n",
    "\n",
    "[MNIST](http://yann.lecun.com/exdb/mnist/) dataset contains 60000 grayscale images (of size 28 * 28 pixels) of handwritten digits between 0 and 9. MNIST is commonly used for image classification task: the goal is to classify each image by assigning it to the correct digit. \n",
    "\n",
    "For a better visual understanding, we display a few samples from MNIST testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c41c9d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66b023",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from fedbiomed.researcher.environ import environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f6624",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "# Get the test dataset\n",
    "test_set = datasets.MNIST(root = os.path.join(environ['TMP_DIR'], 'mnist_testing.tmp'),\n",
    "                          download = True, train = False, transform = transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85a5e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# display a few digits from MNIST testing dataset\n",
    "\n",
    "nb_digits_to_display = 10\n",
    "\n",
    "plt.figure(figsize=(10,2)) \n",
    "plt.title(\"Few images of MNIST dataset\")\n",
    "for i in range(nb_digits_to_display):\n",
    "    plt.subplot(1,nb_digits_to_display, i+1)\n",
    "    plt.imshow(test_set.data[i].numpy())\n",
    "    plt.title(f\"label: {test_set.targets[i].numpy()}\")\n",
    "\n",
    "plt.suptitle(\"Few images of MNIST testing dataset\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4877a56",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Training a model \n",
    "\n",
    "In this section, you will learn how to train a model in Fed-BioMed, by creating a training plan class that includes special methods to retrieve mode, optimizer, training data as well as training step to execute at each iteration of training. Then the federated training will be launched by wrapping the training plan in an experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2963c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1 Creating A PyTorch Training Plan\n",
    "A PyTorch training plan is a Python class that inherits from `fedbiomed.common.training_plans.TorchTrainingPlan` which is an abstract class. The abstract methods `init_model`, `training_data` and `training_step` should be provided in the training plan to retrieve/execute model (`nn.Module`), data loader and training actions at each iteration.\n",
    "\n",
    "<div class=\"note\">\n",
    "    <p>\n",
    "        Please visit <a href=\"../../../user-guide/researcher/training-plan/\">training plan user guide</a> for more information and other special methods of training plan.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c3f83",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Here we define the training plan to be used.\n",
    "# You can use any class name (here 'MyTrainingPlan')\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, model_args):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "            self.dropout1 = nn.Dropout(0.25)\n",
    "            self.dropout2 = nn.Dropout(0.5)\n",
    "            self.fc1 = nn.Linear(9216, 128)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = self.dropout1(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "            output = F.log_softmax(x, dim=1)\n",
    "            return output\n",
    "\n",
    "    def init_model(self, model_args):\n",
    "        return self.Net(model_args = model_args)\n",
    "\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        return Adam(self.model().parameters(), lr = optimizer_args[\"lr\"])\n",
    "\n",
    "    def init_dependencies(self):\n",
    "        return [\"from torchvision import datasets, transforms\",\n",
    "                \"from torch.optim import Adam\"]\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset1 = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        loader_arguments = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset1, **loader_arguments)\n",
    "\n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss   = torch.nn.functional.nll_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d08c12d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"note\">\n",
    "    <p>\n",
    "        Fed-BioMed nodes can be configured to accept only approved training plans. Under this configuration, the training plan files that are sent by a researcher must be approved by the node in advance. For more details, you can visit the tutorial for <a href=\"../../security/training-with-approved-training-plans\">working with approved training plan file</a> and <a href=\"../../../user-guide/nodes/training-plan-security-manager\">user guide for managing nodes.</a>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118afa22",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2 Define an Experiment\n",
    " \n",
    " An experiment is a class that orchestrates the training processes that run on different nodes.\n",
    "\n",
    " - `model_arg` includes arguments that will pass into `init_model` method of training plan class. In our case, we don't need to add any arguments.\n",
    " - `training_args` includes arguments related to optimizer, data loader and training step/routine such as learning rate, number of epochs.\n",
    " - `tags` is a list that includes tags that are going to be used for searching related datasets in nodes. In our case, we saved the MNIST dataset with #MNIST and #dataset tags.\n",
    " - `rounds` represents the number of training rounds that will be applied in nodes. In each round every node complete epochs and send model parameters to the experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74e0e1c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48,\n",
    "    'optimizer_args': {\n",
    "        'lr': 1e-3\n",
    "    },\n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d4efe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training plan class should be passed to the experiment. The experiment will be responsible for uploading the training plan file to the file repository. Afterwards, the nodes will receive training request that includes the URL where the training plan class is stored.\n",
    "\n",
    "Finally, you should indicate which method should be chosen to aggregate model parameters after every round. The basic federation scheme is federated averaging, implemented in Fed-BioMed in the class  `FedAverage`.\n",
    "\n",
    "### 3.3 What happens during the initialization of an experiment?\n",
    "\n",
    "1. The experiment searches for nodes whose available datasets have been saved with tags indicated in the `tags` argument. Nodes are selected based on a node selection strategy.\n",
    "2. The experiment prepares the `job` for the nodes according to the provided arguments\n",
    "3. The training plan file is uploaded to the provided repository to make it available for the nodes.\n",
    "\n",
    "For more details, you can visit [`Experiment` webpage](../../../user-guide/researcher/experiment).\n",
    "\n",
    "Now, let's create our experiment. Since only one node has been created, the experiment will only find a single node for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9b8d2",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25747d3d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As an output, you should see the log message from the node which dispose of the MNIST dataset. It means that the search request that contains `#MNIST, #dataset` tags has been successfully received by the node. In the example displayed here, we received messages only from one node, which we created before. \n",
    "\n",
    "The experiment also receives loss values during training on each node. In Fed-BioMed, it is possible to use a tensorboard to display loss values during training. Please refer to Fed-BioMed's [tensorboard documentation](../../../user-guide/researcher/tensorboard) for how to enable the tensorboard.\n",
    "\n",
    "Now, let's run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8e54d",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45284e9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After running the experiment, according to the provided arguments 4 training rounds should be completed on the node that you created. You can check the node id from the output and compare it with the node id which is defined in the config.ini file. After the process is finished, you are ready to inspect the model parameters. \n",
    "\n",
    "### 3.4 Extracting Training Results\n",
    "\n",
    "#### Timing \n",
    "\n",
    "Training replies for each round are available via `exp.training_replies()` (index 0 to (`rounds` - 1) ). You can display the keys of each round by running the following script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e400c831",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b71f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's see how training details can be accessed from `training_replies()`. The following parameters will be inspected;\n",
    "\n",
    "- `rtime_training` : Real-time (clock time) spent in the training function on the node \n",
    "- `ptime_training`: Process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total`   : Real-time (clock time) spent in the researcher between sending training requests and handling the responses\n",
    "\n",
    "_Note: The following code accesses the training replies of the last round of the experiment._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26f4b1",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "round_data = exp.training_replies()[rounds - 1].data()\n",
    "for c in range(len(round_data)):\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = round_data[c]['node_id'],\n",
    "        rtraining = round_data[c]['timing']['rtime_training'],\n",
    "        ptraining = round_data[c]['timing']['ptime_training'],\n",
    "        rtotal = round_data[c]['timing']['rtime_total']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638d0cbc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Federated Parameters\n",
    "Federated model parameters for each round are available via `exp.aggregated_params()` (index 0 to (`rounds` - 1) ).\n",
    "For example, you can easily view the federated parameters for the last round of the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3699adf",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n",
    "\n",
    "print(\"\\nAccess the federated params for the last training round : \")\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69682367",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also access the path where the model parameters are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92782fec",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\t- params_path: \", exp.aggregated_params()[rounds - 1]['params_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa002f83",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, to access specific parameters of last round:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae944d",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\t- Parameters of CONV1 layer's biases of last round: \\n\", exp.aggregated_params()[rounds - 1]['params']['conv1.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15bfe6b",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp.training_plan()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d62c9a9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. PyTorch MNIST Example with Two Nodes\n",
    "\n",
    "In this section, we will be working on two nodes. Following the previous example, the experiment and training routine will remain unchanged.  Therefore, you just need to configure another node, and add the MNIST dataset with the default tags.\n",
    "\n",
    "### 4.1 Configuring Second Node\n",
    "\n",
    "While creating a second node you need to be careful with the node that has already been created. To configure the second node, a different config file has to be defined. Please follow the steps below to configure your second node. \n",
    "\n",
    "1. Please open a new terminal and cd into the base directory of fedbiomed.\n",
    "2. In this step, you need to name a new config file using the `config` parameter. Instead of downloading a new MNIST dataset, you can use the one that you already downloaded in the previous example. To do that please run `${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n2.ini --add-mnist /path/to/your/mnist`. It will create a config file named `config-n2.ini`\n",
    "3. You need to start the new node by indicating the newely created config file. Please run the following command to start the second node `${FEDBIOMED_DIR}/scripts/fedbiomed_run node config config-n2.ini start`\n",
    "\n",
    "### 4.2 Defining an Experiment \n",
    "\n",
    "Since a training plan has already been created and saved in the previous example, you don't need to repeat this step here again: the same training plan with same model will be used for training. However, you can define a new experiment for testing purposes. The experiment will search the MNIST dataset in available nodes. Training arguments are kept the same as in the previous example.\n",
    "\n",
    "_You can also list datasets and select specific nodes to perform traning. You can visit [listing datasets and selecting nodes](../../../user-guide/researcher/listing-datasets-and-selecting-nodes) documentation to get more information._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc4c303",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'batch_size': 48,\n",
    "    'optimizer_args': {\n",
    "        'lr': 1e-3\n",
    "    },\n",
    "    'epochs': 1,\n",
    "    'dry_run': False,  \n",
    "    'batch_maxnum': 100 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 4\n",
    "\n",
    "expN2 = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99b18a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can see from the output that the search request has been sent to 2 nodes by the experiment.\n",
    "\n",
    "Now, let's run the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047c5aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expN2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5240a0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Testing Federated Model\n",
    "\n",
    "In this section, we will create a test function to obtain accuracy, loss, and confusion matrix using the test MNIST dataset. \n",
    "\n",
    "### 5.1 Aggregated Parameters \n",
    "\n",
    "`training_plan()` returns the training plan and `training_plan().model()` returns the model that is created in the training plan.  It is possible to load specific aggregated parameters which are obtained in every round. Thereafter, it will be ready to make predictions using those parameters. The last round gives the last aggregated model parameters which represents the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c251b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fed_model = expN2.training_plan().model()\n",
    "fed_model.load_state_dict(expN2.aggregated_params()[rounds - 1]['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa271d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.2 Creating A Test Function\n",
    "\n",
    "Let's create a test function that returns loss, accuracy, and confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe10d8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "# Test function \n",
    "def testing_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    device = 'cpu'\n",
    "\n",
    "    y_pred = []\n",
    "    y_actu = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            y_pred.extend(torch.flatten(pred).tolist()) \n",
    "            y_actu.extend(target.tolist())\n",
    "           \n",
    "    y_pred = pd.Series(y_pred, name='Actual')\n",
    "    y_actu = pd.Series(y_actu, name='Predicted')\n",
    "    cm = pd.crosstab(y_actu, y_pred)\n",
    "    correct = sum([cm.iloc[i,i] for i in range(len(cm))])\n",
    "    \n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100*correct/len(test_loader.dataset)\n",
    "\n",
    "    return(test_loss, accuracy, cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc7747c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will use the MNIST test dataset for testing our federated model. You can download this dataset set from `torchvision.dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ea693",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "# Get the test dataset\n",
    "test_set = datasets.MNIST(root = os.path.join(environ['TMP_DIR'], 'mnist_testing.tmp'), download = True, train = False, transform = transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103ee4b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, it is time to get performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f040bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_results = testing_accuracy(fed_model, test_loader)\n",
    "\n",
    "print(\"- Test Loss: \", test_results[0], \"\\n\")\n",
    "print(\"- Accuracy: \", test_results[1], \"\\n\")\n",
    "print(\"- Confusion Matrix: \\n \\n\",  test_results[2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ea7e2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.3 Creating Heatmap for Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55b9c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will use `matplotlib` for plotting results. If you have followed all the steps in this tutorial, you must have installed this library in the section 2. Otherwise, you can run `!pip install matplotlib` command in a notebook cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7830b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_matrix = test_results[2].to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "im = ax.imshow(conf_matrix)\n",
    "\n",
    "ax.set_xticks(np.arange(10))\n",
    "ax.set_yticks(np.arange(10))\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        text = ax.text(j, i, conf_matrix[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "        \n",
    "ax.set_xlabel('Actual targets')\n",
    "ax.set_ylabel('Predicted targets')\n",
    "ax.set_title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712585ed",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.4 Plotting Loss for Each round\n",
    "In this section, we will plot loss values that are obtained over the test dataset using model parameters of every round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12b8eb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "for i in range(rounds):\n",
    "    fed_model = expN2.training_plan().model()\n",
    "    fed_model.load_state_dict(expN2.aggregated_params()[i]['params'])\n",
    "    loss = testing_accuracy(fed_model, test_loader)[0]\n",
    "    errors.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15910d61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Plotting \n",
    "plt.plot(errors, label = 'Federated Test Loss')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Log Likelihood Loss evolution over number of rounds\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}