{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c753a58",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PyTorch aggregation methods in Fed-BioMed\n",
    "\n",
    "**Difficulty level**: **advanced**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial focuses on how to deal with heterogeneous datasets by changing its `Aggregator`. Fed-BioMed provides different methods for Aggregation. Selecting an appropriate Aggregation method can be critical when being confronted to unbalanced or heterogeneous datasets.\n",
    "\n",
    "`Aggregators` provide a way to merge local models sent by `Nodes` into a global, more generalized model. Please note that designing `Node` sampling `Strategies` can also help when working on heterogeneous datasets.\n",
    "\n",
    "For more information about `Aggregators` object in Fed-BioMed, and on how to create your own `Aggregator`; please see [`Aggregators` in the User Guide](../../../user-guide/researcher/aggregation)\n",
    "\n",
    "### Mednist Dataset\n",
    "For this tutorial, we will be using heterogeneous the MedNIST dataset. MedNIST is a collection of 2-D grayscale medical images. The MedNIST dataset was gathered from several sets from TCIA, the RSNA Bone Age Challenge, and the NIH Chest X-ray dataset. The dataset is kindly made available by Dr. Bradley J. Erickson M.D., Ph.D. (Department of Radiology, Mayo Clinic) under the Creative Commons CC BY-SA 4.0 license and is distributed by MONAI for teaching and benchmarking simple deep-learning pipelines. For more information regarding the dataset please see [MedNIST Dataset](../../../user-guide/datasets/mednist-dataset).\n",
    "\n",
    "### Before you start\n",
    "\n",
    "Make sure that you have configured your nodes for training. For configuration, we create our node via\n",
    "\n",
    "```fedbiomed node --path path/to/your/node create```\n",
    "\n",
    "And after creation, add the MedNist Dataset:\n",
    "\n",
    "```fedbiomed node dataset add```\n",
    "\n",
    "This command will give you a menu with dataset options similar to below:\n",
    "\n",
    "    1) csv\n",
    "    2) default\n",
    "    3) mednist\n",
    "    4) images\n",
    "    5) medical-folder\n",
    "    6) custom\n",
    "\n",
    "Choose the 3rd option to add the MedNIST dataset. You can select y to add with the default tags. And for the path select the folder where you want to download (or have downloaded) the dataset.\n",
    "\n",
    "**Nota**: Tags are important in FedBioMed. They are used as identifiers to select the datasets in the nodes, that will be used for training. Make sure that you use the same tags when adding a dataset, and defining an experiment (which will be seen below soon).\n",
    "\n",
    "For more information regarding Node Configuration, please refer to the [User Guide](../../../getting-started/configuration)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58226b70-b445-4030-afbf-c226615c1850",
   "metadata": {},
   "source": [
    "## 1. Defining an `Experiment` using `FedAverage` `Aggregator`\n",
    "\n",
    "In this example, we reuse the TrainingPlan that was defined in the [previous MedNIST tutorial](../05_Transfer-learning_tutorial_usingDenseNet-121). It uses a pretrained DenseNet model, where only the classifier is changed in order to adapt it to our task. We normalize the MedNIST dataset before feeding it to our model.\n",
    "\n",
    "The only change from the previous tutorial is going to be the `Aggregators`. We are going to show FedAveraging, FedProx and Scaffold Aggregation. We will start with FedAveraging. FedAveraging has been introduced by McMahan et al. as the first aggregation method in the Federated Learning literature. It does the weighted sum of all `Nodes` local models parameters in order to obtain a global model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c872dd2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.datamanager import DataManager\n",
    "from fedbiomed.common.dataset import MedNistDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "\n",
    "    def init_model(self, model_args):\n",
    "        model = models.densenet121(weights=None)  # here model coefficients are set to random weights\n",
    "\n",
    "        # add the classifier \n",
    "        num_classes = model_args['num_classes'] \n",
    "        num_ftrs = model.classifier.in_features\n",
    "        model.classifier= nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )      \n",
    "        return model\n",
    "\n",
    "    def init_dependencies(self):\n",
    "        return [\n",
    "            \"from torchvision import transforms, models\",\n",
    "            \"import torch.optim as optim\",\n",
    "            \"from torchvision.models import densenet121\",\n",
    "            \"from fedbiomed.common.dataset import MedNistDataset\"\n",
    "        ]\n",
    "\n",
    "    def init_optimizer(self, optimizer_args):        \n",
    "        return optim.Adam(self.model().parameters(), lr=optimizer_args[\"lr\"])\n",
    "\n",
    "    def training_data(self):\n",
    "\n",
    "        # Transform images and do data augmentation \n",
    "        preprocess = transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "        target_transform = transforms.Lambda(lambda y: y.long())\n",
    "    \n",
    "        train_data = MedNistDataset(transform = preprocess, target_transform=target_transform)\n",
    "        train_kwargs = { 'shuffle': True}\n",
    "        return DataManager(dataset=train_data, **train_kwargs)\n",
    "\n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        loss   = loss_func(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd8684",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We define hereafter parameters for `Experiment` to be used with vanilla `FedAverage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b6df1d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    'loader_args': {\n",
    "        'batch_size': 32,\n",
    "    }, \n",
    "    'random_seed': 1234,\n",
    "    'optimizer_args': {'lr': 1e-3}, \n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'num_updates': 50, # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'num_classes': 6, # adapt this number to the number of classes in your dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340ba9a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We then import `FedAverage` `Aggregator` from Fed-BioMed's `Aggregators`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287b3d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.federated_workflows import Experiment\n",
    "from fedbiomed.researcher.aggregators import FedAverage\n",
    "from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n",
    "\n",
    "tags =  ['#MEDNIST', '#dataset']\n",
    "rounds = 3\n",
    "\n",
    "exp_fed_avg = Experiment()\n",
    "exp_fed_avg.set_model_args(model_args=model_args)\n",
    "exp_fed_avg.set_training_args(training_args=training_args)\n",
    "exp_fed_avg.set_training_plan_class(training_plan_class=MyTrainingPlan)\n",
    "exp_fed_avg.set_tags(tags = tags)\n",
    "exp_fed_avg.set_training_data(training_data=None, from_tags=True)\n",
    "exp_fed_avg.set_aggregator(aggregator=FedAverage())\n",
    "exp_fed_avg.set_strategy(node_selection_strategy=DefaultStrategy())\n",
    "exp_fed_avg.set_round_limit(rounds)\n",
    "exp_fed_avg.set_tensorboard(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa8f02",
   "metadata": {},
   "source": [
    "Activate Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de222a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832fe786",
   "metadata": {},
   "outputs": [],
   "source": [
    "fedavg_tensorboard_dir = exp_fed_avg.tensorboard_results_path\n",
    "\n",
    "%tensorboard --logdir {fedavg_tensorboard_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff7a682",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp_fed_avg.run(increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4f89d",
   "metadata": {},
   "source": [
    "Save trained model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0849f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_fed_avg.training_plan().export_model('./trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f587bda",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Defining an `Experiment` using `FedProx` `Aggregator`\n",
    "\n",
    "The second aggregator we show is going to be `FedProx`. `FedProx` is a modification of `FedAverage` that adds a regularization term to the local training objective, which prevents the model from deviating too far from the global model. This helps improve convergence and handles non-IID (non-independent and identically distributed) data.\n",
    "\n",
    "To implement it in FedBioMed, it is sufficient to just add the regularization parameter `fedprox_mu` into the `training_args`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10780ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_args_fedprox = {\n",
    "    'loader_args': {\n",
    "        'batch_size': 32,\n",
    "    }, \n",
    "    'random_seed': 1234,\n",
    "    'optimizer_args': {'lr': 1e-3}, \n",
    "    'epochs': 1, \n",
    "    'dry_run': False,  \n",
    "    'num_updates': 50,\n",
    "    'fedprox_mu': .1,  # This parameter indicates that we are going to use FedProx\n",
    "    \n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'num_classes': 6, # adapt this number to the number of classes in your dataset\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d3573",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.federated_workflows import Experiment\n",
    "from fedbiomed.researcher.aggregators import FedAverage\n",
    "from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n",
    "\n",
    "tags =  ['#MEDNIST', '#dataset']\n",
    "rounds = 3\n",
    "\n",
    "exp_fedprox = Experiment()\n",
    "\n",
    "\n",
    "exp_fedprox.set_model_args(model_args=model_args)\n",
    "exp_fedprox.set_training_args(training_args=training_args_fedprox)\n",
    "exp_fedprox.set_training_plan_class(training_plan_class=MyTrainingPlan)\n",
    "exp_fedprox.set_tags(tags = tags)\n",
    "exp_fedprox.set_training_data(training_data=None, from_tags=True)\n",
    "exp_fedprox.set_aggregator(aggregator=FedAverage())\n",
    "exp_fedprox.set_strategy(node_selection_strategy=DefaultStrategy())\n",
    "exp_fedprox.set_round_limit(rounds)\n",
    "exp_fedprox.set_tensorboard(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8764aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fedprox_tensorboard_dir = exp_fedprox.tensorboard_results_path\n",
    "\n",
    "%tensorboard --logdir {fedavg_tensorboard_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70abc04a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp_fedprox.run(increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff24c33",
   "metadata": {},
   "source": [
    "Save trained model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecdb992",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_fedprox.training_plan().export_model('./trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32bdb8d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Defining an `Experiment` using `SCAFFOLD` `Aggregator`\n",
    "\n",
    "In traditional federated learning algorithms like FedAvg, each client trains a local model using its own data. However, since clients have different data distributions, the local models may deviate significantly from the global model, causing slow convergence and instability in the aggregation process. This problem is addressed as **Client Drift**.\n",
    "\n",
    "Scaffold introduces a set of control variates (or auxiliary variables) that help track the differences between local and global updates. These control variates are maintained for each client and used to adjust the local gradients during the training process.\n",
    "\n",
    "In each training round *t*, the control variate for client *k* \\( c^{(k)}_t \\) is updated as:\n",
    "\n",
    "$$ c^{(k)}_{t+1} = c^{(k)}_t + g^{(k)}_t - \\bar{g}_t $$\n",
    "\n",
    "Where \\( g^{(k)}_t \\) is the gradient for client \\( k \\), and \\( \\bar{g}_t \\) is the global average of the gradients.\n",
    "\n",
    "The updated local gradient (corrected update) is given by:\n",
    "\n",
    "$$ \\hat{g}^{(k)}_t = g^{(k)}_t - c^{(k)}_t $$\n",
    "\n",
    "By reducing client drift, Scaffold provides a more stable convergence and better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9b032",
   "metadata": {},
   "source": [
    "To use `Scaffold` in Fedbiomed, we import another it from the `fedbiomed.researcher.aggregators` module.\n",
    "\n",
    "`Scaffold` aggregator takes two arguments:\n",
    "\n",
    " - `server_lr` is the Server Learning Rate (the gradient descent on the global model after receiving each corrected update from the nodes)\n",
    " - `fds` is the `Federated Dataset` containing information about the `Nodes` connected to the network after issuing a `TrainRequest`\n",
    "\n",
    "*Please note that it is also possible to use `Scaffold` with a regularization parameter as suggested in `FedProx`. For that, you just have to specify `fedprox_mu` into the `training_args` dictionary, as shown in the `FedProx` example*\n",
    "\n",
    "**Attention**: this version of `Scaffold` exchanges correction terms that are not protected, even when using [Secure Aggregation](../../../user-guide/secagg/introduction). Please do not use this version of `Scaffold` under heavy security constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33bb89",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.aggregators import Scaffold\n",
    "from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n",
    "\n",
    "server_lr = .8\n",
    "exp_scaffold = Experiment()\n",
    "\n",
    "exp_scaffold.set_model_args(model_args=model_args)\n",
    "exp_scaffold.set_training_args(training_args=training_args)\n",
    "exp_scaffold.set_training_plan_class(training_plan_class=MyTrainingPlan)\n",
    "exp_scaffold.set_tags(tags = tags)\n",
    "exp_scaffold.set_training_data(training_data=None, from_tags=True)\n",
    "exp_scaffold.set_aggregator(Scaffold(server_lr=server_lr))\n",
    "exp_scaffold.set_strategy(node_selection_strategy=DefaultStrategy())\n",
    "exp_scaffold.set_round_limit(rounds)\n",
    "exp_scaffold.set_tensorboard(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6887568",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold_tensorboard_dir = exp_scaffold.tensorboard_results_path\n",
    "\n",
    "%tensorboard --logdir {fedavg_tensorboard_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da799e9c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp_scaffold.run(increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e9eb9a",
   "metadata": {},
   "source": [
    "Save trained model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_scaffold.training_plan().export_model('./trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f382a844",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Going further\n",
    "\n",
    "In this tutorial we presented 3 important `Aggregators` that can be found in the Federated Learning Literature. If you want to create your custom `Aggregator`, please check our [Aggregation User guide](../../../user-guide/researcher/aggregation)\n",
    "\n",
    "\n",
    "You may have noticed that thanks to Fed-BioMed's modular structure, it is possible to alternate from one aggregator to another while conducting an `Experiment`. For instance, you may start with the `SCAFFOLD` `Aggregator` for the 3 first rounds, and then switch to `FedAverage` `Aggregator` for the remaining rounds, as shown in the example below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e80fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.aggregators import Scaffold, FedAverage\n",
    "from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n",
    "\n",
    "server_lr = .8\n",
    "exp_multi_agg = Experiment()\n",
    "\n",
    "# selecting how many rounds of each aggregator we will perform\n",
    "rounds_scaffold = 3\n",
    "rounds_fedavg = 1\n",
    "\n",
    "exp_multi_agg.set_model_args(model_args=model_args)\n",
    "exp_multi_agg.set_training_args(training_args=training_args)\n",
    "exp_multi_agg.set_training_plan_class(training_plan_class=MyTrainingPlan)\n",
    "exp_multi_agg.set_tags(tags = tags)\n",
    "exp_multi_agg.set_training_data(training_data=None, from_tags=True)\n",
    "exp_multi_agg.set_aggregator(Scaffold(server_lr=server_lr))\n",
    "exp_multi_agg.set_strategy(node_selection_strategy=DefaultStrategy())\n",
    "exp_multi_agg.set_round_limit(rounds_scaffold + rounds_fedavg)\n",
    "\n",
    "exp_multi_agg.run(rounds=rounds_scaffold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9effabcf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp_multi_agg.set_aggregator(FedAverage())\n",
    "exp_multi_agg.run(rounds=rounds_fedavg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a054b0af",
   "metadata": {},
   "source": [
    "Save trained model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_multi_agg.training_plan().export_model('./trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086fb124",
   "metadata": {},
   "source": [
    "For more advanced Aggregators and Regularizers, like `FedOpt`, you may be interested by [`DecLearn` optimizers](../../optimizers/01-fedopt-and-scaffold) that are compatible with Fed-BioMed and provide more options for Aggregation and Optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
