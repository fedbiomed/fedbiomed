{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PyTorch Used Cars Dataset Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This tutorial focuses on how to train a federated regression model on Non-IID dataset using PyTorch framework. We will be working on the Used Cars dataset to perform federated learning. The sections of this tutorial are presented as follows;\n",
    "\n",
    "- Dataset Preparation;\n",
    "- Node Configurations;\n",
    "- Create an Experiment to Train a Model;\n",
    "- Testing Federated Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Dataset Preparation\n",
    "\n",
    "In this tutorial, we will be using the Used Cars dataset. The goal of the model will be to predict the price of the car based on given features. \n",
    "\n",
    "You can download the dataset from [here](https://www.kaggle.com/adityadesai13/used-car-dataset-ford-and-mercedes). To be able to download this dataset you need to have a Kaggle account. After downloading, you can create folders for the dataset in the Fed-BioMed `Researcher` `fbm-researcher/notebooks/data` directory.\n",
    "\n",
    "```shell\n",
    "cd /path/to/fedbiomed/directory\n",
    "fedbiomed component create -c researcher\n",
    "mkdir fbm-researcher/notebooks/data/UsedCars && mkdir fbm-researcher/notebooks/data/UsedCars/raw \n",
    "```\n",
    "\n",
    "You can extract CSV files in the zip file into `fbm-researcher/notebooks/data/UsedCars/raw`. Your file tree should be like the tree below;\n",
    "\n",
    "```shell\n",
    "├── data\n",
    "│   └── UsedCars\n",
    "│       └── raw\n",
    "│           ├── audi.csv\n",
    "│           ├── bmw.csv\n",
    "│           ├── cclass.csv\n",
    "│           ├── focus.csv\n",
    "│           ├── ford.csv\n",
    "│           ├── hyundi.csv\n",
    "│           ├── merc.csv\n",
    "│           ├── skoda.csv\n",
    "│           ├── toyota.csv\n",
    "│           ├── unclean cclass.csv\n",
    "│           ├── unclean focus.csv\n",
    "│           ├── vauxhall.csv\n",
    "│           └── vw.csv\n",
    "\n",
    "```\n",
    "\n",
    "### 2.1 Selecting CSV Dataset for Each Node\n",
    "\n",
    "Each CSV dataset contains features for different car brands. It is a good example for applying federated learning through each dataset if we assume that these datasets will be stored in different locations. We will be working on 3 datasets that are `audi.csv`, `bmw.csv`, and `ford.csv`. We will deploy `audi.csv` and `bmw.csv` on different nodes and use `ford.csv` for final testing at the central researcher using the model trained on two nodes.   \n",
    "\n",
    "### 2.2 Preprocessing \n",
    "\n",
    "Before deploying datasets we need to apply some preprocessing to make them ready for the federated training. Since car `model` and `fuelType` features are not consistent across the dataset, we can drop them. We also need to apply label encoding for the `transmission` feature.\n",
    "\n",
    "**Note:** Dropping and encoding columns can be also done in the `training_data` method of `TrainingPlan` but it is always better to deploy clean/prepared datasets in the nodes. \n",
    " \n",
    "\n",
    "Let's starting with loading CSV datasets using `pd.read_csv` API. Please make sure that you have launched your Jupyter notebook using the command `fedbiomed researcher start` so you can follow code examples without changing file paths.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from fedbiomed.researcher.config import config\n",
    "# Optional: to be able use different researcher configuration.\n",
    "# config.load(root=<different-component-root>) \n",
    "\n",
    "\n",
    "root_dir = config.root\n",
    "\n",
    "audi = pd.read_csv(os.path.join(root_dir,'notebooks','data','UsedCars', 'raw', 'audi.csv'))\n",
    "bmw = pd.read_csv(os.path.join(root_dir,'notebooks', 'data', 'UsedCars', 'raw', 'bmw.csv'))\n",
    "ford = pd.read_csv(os.path.join(root_dir,'notebooks', 'data', 'UsedCars', 'raw', \"ford.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Drop columns for car `model` & `fuelType` as labels are not consistent across files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "audi.drop(columns = ['model','fuelType'], inplace = True)\n",
    "bmw.drop(columns = ['model','fuelType'], inplace = True)\n",
    "ford.drop(columns = ['model','fuelType'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Label encoding for `transmission` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "audi['transmission'] = audi['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "bmw['transmission'] = bmw['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})\n",
    "ford['transmission'] = ford['transmission'].map({'Automatic':0,'Manual':1,'Semi-Auto':2,'Other':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we can save our new CSV files into `data/UsedCars` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "audi.to_csv(os.path.join(root_dir, 'notebooks', 'data', 'UsedCars', 'audi_transformed.csv'),header = True,index= False)\n",
    "bmw.to_csv(os.path.join(root_dir, 'notebooks', 'data', 'UsedCars', 'bmw_transformed.csv'),header = True,index= False)\n",
    "ford.to_csv(os.path.join(root_dir, 'notebooks', 'data', 'UsedCars', 'ford_transformed.csv'),header = True,index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Node Configurations\n",
    "\n",
    "We will deploy the `audi_transformed.csv` and `bmw_transformed.csv` datasets on different nodes.  \n",
    "\n",
    "1. **Configuring First Node**\n",
    "    * Run `fedbiomed component create -c node --path ./node-audi`\n",
    "    * Move `audi_transformed.csv` from `fbm-researcher/notebooks/data/UsedCars` to the `Node` `node-audi/data` with the following command: \n",
    "        - `mv fbm-researcher/notebooks/data/UsedCars/audi_transformed.csv node-audi/data/`\n",
    "    * Run `fedbiomed node --path ./node-audi dataset add`\n",
    "    * Select option 1 to add a csv file (audi_transformed.csv)\n",
    "    * Choose a name for dataset, For example `Used-Cars-Audi`\n",
    "    * Choose tag for the dataset. This part is important because we will be sending search request to nodes with this \n",
    "    specified tag. Therefore please type `#UsedCars` and enter.\n",
    "    * Enter a description for the dataset \n",
    "    * Select the audi_transformed.csv file in the file selection window\n",
    "  \n",
    "   \n",
    "2. **Configuring Second Node**\n",
    "    * Run `fedbiomed component create -c node --path ./node-bmw`\n",
    "    * Move `bmw_transformed.csv` from `fbm-researcher/notebooks/data/UsedCars` to the `Node` `node-bmw/data` with the following command: \n",
    "        - `mv fbm-researcher/notebooks/data/UsedCars/bmw_transformed.csv node-bmw/data/`\n",
    "    * Run `fedbiomed node --path ./node-bmw dataset add`\n",
    "    * Select option 1 to add a csv file (bmw_transformed.csv)\n",
    "    * Choose a name for dataset, For example `Used-Cars-BMW`\n",
    "    * Since we entered the tag as `#UsedCars`, we need to use the same tag for this one too. \n",
    "    * Enter a description for the dataset\n",
    "    * Select the bmw_trasnformed.csv file in the file selection window\n",
    "    \n",
    "3. **Starting Nodes**\n",
    "    Please run the following command to start the node that has the `audi` dataset.\n",
    "    \n",
    "    ```shell\n",
    "    fedbiomed node --path ./node-audi start\n",
    "    ```\n",
    "    Please open a new terminal window to start the node that has the `bmw` dataset \n",
    "    \n",
    "    ```shell\n",
    "    fedbiomed node --path ./node-bmw start\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Create an Experiment to Train a Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before creating an experiment, we need to define training plan class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "\n",
    "\n",
    "# Here we define the training plan to be used for the experiment.\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "\n",
    "    # Model\n",
    "    def init_model(self):\n",
    "        model_args = self.model_args()\n",
    "        model = self.Net(model_args)\n",
    "        return model\n",
    "\n",
    "    # Dependencies\n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torch.utils.data import Dataset\",\n",
    "                \"import pandas as pd\"]\n",
    "        return deps\n",
    "\n",
    "    # network\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, model_args):\n",
    "            super().__init__()\n",
    "            self.in_features = model_args['in_features']\n",
    "            self.out_features = model_args['out_features']\n",
    "            self.fc1 = nn.Linear(self.in_features, 5)\n",
    "            self.fc2 = nn.Linear(5, self.out_features)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data).float()\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss   = torch.sqrt(criterion(output, target.unsqueeze(1)))\n",
    "        return loss\n",
    "\n",
    "    class csv_Dataset(Dataset):\n",
    "    # Here we define a custom Dataset class inherited from the general torch Dataset class\n",
    "    # This class takes as argument a .csv file path and creates a torch Dataset\n",
    "        def __init__(self, dataset_path, x_dim):\n",
    "            self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)\n",
    "            x_train = self.input_file.loc[:,('year','transmission','mileage','tax','mpg','engineSize')].values\n",
    "            y_train = self.input_file.loc[:,'price'].values\n",
    "            self.X_train = torch.from_numpy(x_train).float()\n",
    "            self.Y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.Y_train)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            return (self.X_train[idx], self.Y_train[idx])\n",
    "\n",
    "    def training_data(self):\n",
    "    # The training_data creates the Dataloader to be used for training in the general class TorchTrainingPlan of fedbiomed\n",
    "        dataset = self.csv_Dataset(self.dataset_path, self.model_args()[\"in_features\"])\n",
    "        train_kwargs = { 'shuffle': True}\n",
    "        return DataManager(dataset=dataset , **train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### 4.1 Defining Arguments for The Experiment\n",
    "An experiment is a class that orchestrates the training processes that run on different nodes. The experiment has to be initialized with necessary arguments to inform nodes about the training plan. In this case, first, you need to define `model_arg`, `training_args`, `tags`, and `round`.\n",
    "\n",
    "Please visit [experiment documentation](../../../user-guide/researcher/experiment/) to get detailed information about the experiment class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "model_args = {\n",
    "    'in_features': 6,\n",
    "    'out_features': 1\n",
    "}\n",
    "\n",
    "# training parameters\n",
    "training_args = {\n",
    "    'loader_args': { 'batch_size': 40, },\n",
    "    'optimizer_args': {\n",
    "          'lr': 1e-3\n",
    "    },\n",
    "    'epochs': 2,\n",
    "#    'batch_maxnum': 2,  # can be used to debugging to limit the number of batches per epoch\n",
    "#    'log_interval': 1,  # output a logging message every log_interval batches\n",
    "}\n",
    "\n",
    "tags =  ['#UsedCars']\n",
    "rounds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The other arguments that should be passed to the experiment is the training plan class which is `MyTrainingPlan`. \n",
    "\n",
    "You should also indicate which method should be chosen to aggregate model parameters after every round. The basic federation scheme is federated averaging, implemented in Fed-BioMed in the class `FedAverage`. You can also visit [aggregation documentation](../../../user-guide/researcher/aggregation/) to have more information about aggregation process.\n",
    "\n",
    "Since we are going to use every `Node` that has `UsedCars` datasets, the `node_selection_strategy` should be `None` which means that every `Node` will be part of the federated training. \n",
    "\n",
    "### 4.2 What happens during the initialization of an experiment?\n",
    "\n",
    "1. The experiment searches for nodes that have datasets that have been saved with the `#UsedCars` tag.\n",
    "2. The experiment is set up to manage the training process across the nodes with the given arguments\n",
    "\n",
    "**Note**: It is possible to send search requests to only specified nodes with the `Nodes` argument of the experiment. Please visit [listing datasets and selecting nodes](../../../user-guide/researcher/listing-datasets-and-selecting-nodes) documentation for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.federated_workflows import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 model_args=model_args,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The experiment also receives loss values during training on each node. In Fed-BioMed, it is possible to use a tensorboard to display loss values during training. Please refer to Fed-BioMed's [tensorboard documentation](../../../user-guide/researcher/tensorboard) to enable tensorboard.\n",
    "\n",
    "Let's start the experiment. By default, this function doesn't stop until all the `round_limit` rounds are done for all the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save trained model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.training_plan().export_model('./trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.3 Extracting Training Results\n",
    "\n",
    "#### Timing \n",
    "\n",
    "Training replies for each round are available in `exp.training_replies()` (index 0 to (`rounds` - 1) ). You can display the keys of each round by running the following script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's see how training details can be accessed via `training_replies()`. The following parameters will be inspected;\n",
    "\n",
    "- `rtime_training` : Real-time (clock time) spent in the training function on the node \n",
    "- `ptime_training`: Process time (user and system CPU) spent in the training function on the node\n",
    "- `rtime_total`   : Real-time (clock time) spent in the researcher between sending training requests and handling the responses\n",
    "\n",
    "_Note: The following code accesses the training replies of the last round of the experiment._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())\n",
    "\n",
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "round_data = exp.training_replies()[rounds - 1]\n",
    "for r in round_data.values():\n",
    "    print(\"\\t- {id} :\\\n",
    "    \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "    \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "    \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = r['node_id'],\n",
    "        rtraining = r['timing']['rtime_training'],\n",
    "        ptraining = r['timing']['ptime_training'],\n",
    "        rtotal = r['timing']['rtime_total']))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Federated Parameters\n",
    "Federated model parameters for each round are available in `exp.aggregated_params()` (index 0 to (`rounds` - 1) ).\n",
    "For example, you can easily view the federated parameters for the last round of the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.aggregated_params().keys())\n",
    "print(\"\\nAccess the federated params for the last training round :\")\n",
    "print(\"\\t- parameter data: \", exp.aggregated_params()[rounds - 1]['params'].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp.training_plan().model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Testing Federated Model\n",
    "\n",
    "In this section, we will create a test function to obtain RMSE on `ford_transformed.csv` dataset by using federated model. \n",
    "\n",
    "### 5.1 Aggregated Parameters \n",
    "\n",
    "`model_instance` returns the model that we have created in the previous section. You can load specific aggregated parameters which are obtained in the round. Thereafter, it will make the predictions using those parameters. The last round gives the last aggregated model parameters which represents the final model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fed_model = exp.training_plan().model()\n",
    "fed_model.load_state_dict(exp.aggregated_params()[rounds - 1]['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " fed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset_path = os.path.join(root_dir, 'notebooks', 'data', 'UsedCars', 'ford_transformed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.2 Creating A Test Function\n",
    "\n",
    "Let's create a test function that returns `rmse`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "def cal_rmse(actual, prediction):\n",
    "    return ((actual- prediction)**2).mean()**0.5\n",
    "\n",
    "def testing_rmse(model, data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    device = 'cpu'\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            preds.append(output.numpy().flatten())\n",
    "    rmse = cal_rmse(data_loader.dataset.Y_train.numpy(),np.hstack(preds))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We also need to create a Dataset class for PyTorch data loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class csv_Dataset(Dataset):\n",
    "        def __init__(self, dataset_path):\n",
    "            self.input_file = pd.read_csv(dataset_path,sep=',',index_col=False)\n",
    "            x_train = self.input_file.loc[:,('year','transmission','mileage','tax','mpg','engineSize')].values\n",
    "            y_train = self.input_file.loc[:,'price'].values\n",
    "            self.X_train = torch.from_numpy(x_train).float()\n",
    "            self.Y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "        def __len__(self):            \n",
    "            return len(self.Y_train)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            return (self.X_train[idx], self.Y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = csv_Dataset(test_dataset_path)\n",
    "train_kwargs = { 'shuffle': True}\n",
    "data_loader = DataLoader(dataset, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rmse = testing_rmse(fed_model, data_loader)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.3 Plotting RMSE Values of Each Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "for i in range(rounds):\n",
    "    fed_model = exp.training_plan().model()\n",
    "    fed_model.load_state_dict(exp.aggregated_params()[i]['params'])\n",
    "    loss = testing_rmse(fed_model, data_loader)\n",
    "    errors.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Plotting \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(errors, label = 'Federated Test Loss')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e416aa0399b226346633f35c0f9bb77d7e7cf1619eb46cae5c1dd017cab61cfc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
