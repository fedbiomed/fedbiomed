{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b1c6a5",
   "metadata": {},
   "source": [
    "# FLamby integration in Fed-BioMed\n",
    "\n",
    "This notebook showcases some examples of the integration between [FLamby](https://github.com/owkin/FLamby) and Fed-BioMed.\n",
    "\n",
    "For a thorough understanding, please visit the Tutorials section of our [documentation](../../../getting-started/what-is-fedbiomed/).\n",
    "\n",
    "This tutorial assumes that you know and understand the basics of Fed-BioMed, that you have already set up the network component, and are familiar with flow of adding data through the node CLI interface.\n",
    "For an introduction to Fed-BioMed, please follow our PyTorch MNIST [tutorial](../../pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/).\n",
    "\n",
    "## Downloading FLamby datasets\n",
    "\n",
    "Before using FLamby, you need to [download the FLamby datasets](https://github.com/owkin/FLamby/blob/main/flamby/datasets/fed_heart_disease/README.md#download-and-preprocessing-instructions) that you plan to use. For licensing reasons, these are not including directly in the FLamby installation.\n",
    "\n",
    "To download the `fed_ixi` dataset in `${FEDBIOMED_DIR}/data`, follow [FLamby download instructions](https://github.com/owkin/FLamby/blob/main/flamby/datasets/fed_ixi/README.md#download). In a nutshell:\n",
    "* execute on the **researcher**\n",
    "```bash\n",
    "source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment researcher\n",
    "pip install nibabel\n",
    "```\n",
    "* then execute on each **node** (where `${FEDBIOMED_DIR}` is the base directory of Fed-BioMed):\n",
    "```bash\n",
    "source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment node\n",
    "pip install nibabel\n",
    "python $(find $CONDA_PREFIX -path */fed_ixi/dataset_creation_scripts/download.py) -o ${FEDBIOMED_DIR}/data\n",
    "```\n",
    "\n",
    "To download the `fed_heart_disease` dataset in `${FEDBIOMED_DIR}/data`, follow [FLamby download instructions](https://github.com/owkin/FLamby/blob/main/flamby/datasets/fed_heart_disease/README.md#download-and-preprocessing-instructions). In a nutshell:\n",
    "* execute on the **researcher**\n",
    "```bash\n",
    "source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment researcher\n",
    "pip install wget\n",
    "```\n",
    "* then execute on each **node** (where `${FEDBIOMED_DIR}` is the base directory of Fed-BioMed):\n",
    "```bash\n",
    "source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment node\n",
    "pip install wget\n",
    "python $(find $CONDA_PREFIX -path */fed_heart_disease/dataset_creation_scripts/download.py) --output-folder ${FEDBIOMED_DIR}/data\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6588e",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "If you haven't done so already, install the additional dependencies required by the flamby datasets/features that you intend on using. \n",
    "\n",
    "You may check out which dependencies are needed by each dataset directly from Flamby's `setup.py` [file](https://github.com/owkin/FLamby/blob/main/setup.py#L42). In our case we'll be using the federated IXI and federated heart disease datasets, hence we'll need wget, monai and nibabel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ad6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install wget nibabel  # monai comes already packaged within fed-biomed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da2a74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Running a FLamby experiment in a federated setting with Fed-BioMed\n",
    "\n",
    "Before running a federated experiment, we need to add a FLamby dataset to a node.\n",
    "From a terminal, `cd` to the Fed-BioMed root installation directory and run\n",
    "\n",
    "```bash\n",
    "  ./scripts/fedbiomed_run node add\n",
    " ```\n",
    "\n",
    "Then follow these instructions:\n",
    "   * Select option 6 (`flamby`) when prompted about the data type\n",
    "   * type any name for the database (suggested `flamby-ixi`), press <kbd>Enter</kbd> to continue\n",
    "   * type `flixi` when prompted for tags, press <kbd>Enter</kbd> to continue\n",
    "   * type any description (suggested `flamby-ixi`), press <kbd>Enter</kbd> to continue\n",
    "   * select option 3 (`fed_ixi`) when prompted for the FLamby dataset to be configured\n",
    "   * type a number in the given range, press <kbd>Enter</kbd> to continue\n",
    "   * type any description for the data loading plan (suggested `flamby-ixi-dlp`), press <kbd>Enter</kbd> to continue\n",
    "\n",
    "\n",
    "Optionally, repeat the instructions above for the `fed_heart_disease` dataset, using `flheart` for tags.\n",
    "\n",
    "Finally, start the node with\n",
    "\n",
    "```bash\n",
    "  ./scripts/fedbiomed_run node start\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be586433",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Basic example: Fed-IXI\n",
    "\n",
    "The first example will use the model, optimizer and loss function provided by FLamby for the [IXI](https://github.com/owkin/FLamby/tree/main/flamby/datasets/fed_ixi) dataset.\n",
    "\n",
    "The instructions for using FLamby are:\n",
    "\n",
    "- define a `TorchTrainingPlan`\n",
    "- in the `training_data` function, instantiate a `FlambyDataset`\n",
    "- make sure to include the necessary dependencies in the `init_dependencies` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f6a456",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\n",
    "from fedbiomed.common.data import FlambyDataset, DataManager\n",
    "\n",
    "\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    def init_model(self, model_args):\n",
    "        return Baseline()\n",
    "\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        return Optimizer(self.model().parameters(), lr=optimizer_args[\"lr\"])\n",
    "\n",
    "    def init_dependencies(self):\n",
    "        return [\"from flamby.datasets.fed_ixi import Baseline, BaselineLoss, Optimizer\",\n",
    "                \"from fedbiomed.common.data import FlambyDataset, DataManager\"]\n",
    "\n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        return BaselineLoss().forward(output, target)\n",
    "\n",
    "    def training_data(self):\n",
    "        dataset = FlambyDataset()\n",
    "        loader_arguments = { 'shuffle': True}\n",
    "        return DataManager(dataset, **loader_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad92d03",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'loader_args': { 'batch_size': 8, },\n",
    "    'optimizer_args': {\n",
    "        \"lr\" : 1e-3\n",
    "    },\n",
    "    'epochs': 1,\n",
    "    'dry_run': False,\n",
    "    'batch_maxnum': 2 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93c59b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['flixi']\n",
    "rounds = 1\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793631ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp.run_once(increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6e5a3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Basic example: Fed-Heart-Disease\n",
    "\n",
    "We showcase similar functionalities as the above fed-ixi case, but with FLamby's [Heart Disease](https://github.com/owkin/FLamby/tree/main/flamby/datasets/fed_heart_disease) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc6976a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from flamby.datasets.fed_heart_disease import Baseline, BaselineLoss, Optimizer\n",
    "from fedbiomed.common.data import FlambyDataset, DataManager\n",
    "\n",
    "class FedHeartTrainingPlan(TorchTrainingPlan):\n",
    "    def init_model(self, model_args):\n",
    "        return Baseline()\n",
    "\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        return Optimizer(self.model().parameters(), lr=optimizer_args[\"lr\"])\n",
    "\n",
    "    def init_dependencies(self):\n",
    "        return [\"from flamby.datasets.fed_heart_disease import Baseline, BaselineLoss, Optimizer\",\n",
    "                \"from fedbiomed.common.data import FlambyDataset, DataManager\"]\n",
    "\n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        return BaselineLoss().forward(output, target)\n",
    "\n",
    "    def training_data(self):\n",
    "        dataset = FlambyDataset()\n",
    "        train_kwargs = { 'shuffle': True}\n",
    "        return DataManager(dataset, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e81615",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    'loader_args': { 'batch_size': 4, },\n",
    "    'optimizer_args': {\n",
    "        'lr': 0.001,\n",
    "    },\n",
    "    'epochs': 1,\n",
    "    'dry_run': False,\n",
    "    'log_interval': 2,\n",
    "    'batch_maxnum': 8,\n",
    "    'test_ratio' : 0.0,\n",
    "    'test_on_global_updates': False,\n",
    "    'test_on_local_updates': False,\n",
    "}\n",
    "\n",
    "model_args = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992fe47",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['flheart']\n",
    "num_rounds = 1\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 training_plan_class=FedHeartTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 model_args=model_args,\n",
    "                 round_limit=num_rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd857d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp.run_once(increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9647434",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Complex example: Fed-IXI with data preprocessing and custom training elements\n",
    "\n",
    "This example demonstrates how to define transformations for data preprocessing and provide a customized model, optimizer, and loss function.\n",
    "Incidentally, it also shows how to use `model_args` and `training_args` to parametrize the model, optimizer, and training loop.\n",
    "\n",
    "### Definition of preprocessing transforms\n",
    "\n",
    "This is achieved in the `training_data` function.\n",
    "After instantiating the `FlambyDataset`, you may use the `init_transform` function to attach a preprocessing transformation for your data.\n",
    "Note that the transform that you define must be of type `torchvision.transforms.Compose` or `monai.transforms.Compose`.\n",
    "\n",
    "### Definition of custom model, optimizer and loss\n",
    "\n",
    "This is achieved just like any `TorchTrainingPlan`, through the functions `init_model`, `init_optimizer`, and `training_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20367701",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from unet import UNet\n",
    "from monai.transforms import Compose, NormalizeIntensity, Resize\n",
    "\n",
    "class UNetTrainingPlan(TorchTrainingPlan):\n",
    "\n",
    "    class MyUNet(nn.Module):\n",
    "        CHANNELS_DIMENSION = 1\n",
    "\n",
    "        def __init__(self, model_args):\n",
    "            super().__init__()\n",
    "            self.unet = UNet(\n",
    "            in_channels = model_args.get('in_channels',1),\n",
    "            out_classes = model_args.get('out_classes',2),\n",
    "            dimensions = model_args.get('dimensions',2),\n",
    "            num_encoding_blocks = model_args.get('num_encoding_blocks',5),\n",
    "            out_channels_first_layer = model_args.get('out_channels_first_layer',64),\n",
    "            normalization = model_args.get('normalization', None),\n",
    "            pooling_type = model_args.get('pooling_type', 'max'),\n",
    "            upsampling_type = model_args.get('upsampling_type','conv'),\n",
    "            preactivation = model_args.get('preactivation',False),\n",
    "            residual = model_args.get('residual',False),\n",
    "            padding = model_args.get('padding',0),\n",
    "            padding_mode = model_args.get('padding_mode','zeros'),\n",
    "            activation = model_args.get('activation','ReLU'),\n",
    "            initial_dilation = model_args.get('initial_dilation',None),\n",
    "            dropout = model_args.get('dropout',0),\n",
    "            monte_carlo_dropout = model_args.get('monte_carlo_dropout',0)\n",
    "        )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.unet.forward(x)\n",
    "            x = F.softmax(x, dim=UNetTrainingPlan.MyUNet.CHANNELS_DIMENSION)\n",
    "            return x\n",
    "\n",
    "    def init_model(self, model_args):\n",
    "        return UNetTrainingPlan.MyUNet(model_args)\n",
    "\n",
    "    def init_dependencies(self):\n",
    "        return [\"from torch import nn\",\n",
    "               'import torch.nn.functional as F',\n",
    "               'from torch.optim import AdamW',\n",
    "               'from unet import UNet',\n",
    "               'from monai.transforms import Compose, NormalizeIntensity, Resize',\n",
    "                'from fedbiomed.common.data import FlambyDataset']\n",
    "\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        return AdamW(self.model().parameters(),\n",
    "                     lr=optimizer_args[\"lr\"],\n",
    "                     betas=optimizer_args[\"betas\"],\n",
    "                     eps=optimizer_args[\"eps\"])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_dice_loss(output, target, epsilon=1e-9):\n",
    "        SPATIAL_DIMENSIONS = 2, 3, 4\n",
    "        p0 = output\n",
    "        g0 = target\n",
    "        p1 = 1 - p0\n",
    "        g1 = 1 - g0\n",
    "        tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
    "        fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n",
    "        fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
    "        num = 2 * tp\n",
    "        denom = 2 * tp + fp + fn + epsilon\n",
    "        dice_score = num / denom\n",
    "        return 1. - dice_score\n",
    "\n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss = UNetTrainingPlan.get_dice_loss(output, target)\n",
    "        avg_loss = loss.mean()\n",
    "        return avg_loss\n",
    "\n",
    "    def testing_step(self, data, target):\n",
    "        prediction = self.model().forward(data)\n",
    "        loss = UNetTrainingPlan.get_dice_loss(prediction, target)\n",
    "        avg_loss = loss.mean()  # average per batch\n",
    "        return avg_loss\n",
    "\n",
    "    def training_data(self):\n",
    "        dataset = FlambyDataset()\n",
    "        transform = Compose([Resize((48,60,48)), NormalizeIntensity()])\n",
    "        dataset.init_transform(transform)\n",
    "        train_kwargs = { 'shuffle': True}\n",
    "        return DataManager(dataset, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361fa67",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'in_channels': 1,\n",
    "    'out_classes': 2,\n",
    "    'dimensions': 3,\n",
    "    'num_encoding_blocks': 3,\n",
    "    'out_channels_first_layer': 8,\n",
    "    'normalization': 'batch',\n",
    "    'upsampling_type': 'linear',\n",
    "    'padding': True,\n",
    "    'activation': 'PReLU',\n",
    "}\n",
    "\n",
    "training_args = {\n",
    "    'loader_args': { 'batch_size': 16, },\n",
    "    'optimizer_args': {\n",
    "        'lr': 0.001,\n",
    "        'betas': (0.9, 0.999),\n",
    "        'eps': 1e-08\n",
    "    },\n",
    "    'epochs': 1,\n",
    "    'dry_run': False,\n",
    "    'log_interval': 2,\n",
    "    'test_ratio' : 0.0,\n",
    "    'test_on_global_updates': False,\n",
    "    'test_on_local_updates': False,\n",
    "    'batch_maxnum': 2 # Fast pass for development : only use ( batch_maxnum * batch_size ) samples\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc891e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.experiment import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['flixi']\n",
    "num_rounds = 1\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=UNetTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=num_rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d15be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp.run_once(increase=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}