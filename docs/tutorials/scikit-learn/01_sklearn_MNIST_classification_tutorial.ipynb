{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217752eb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MNIST classification with [Scikit-Learn Classifier (Perceptron)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)\n",
    "\n",
    "**Overview of the tutorial**: \n",
    "\n",
    "In this tutorial, we are going to train [Scikit-Learn `Perceptron`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) as a federated model over a `Node`.\n",
    "\n",
    "At the end of this tutorial, you will learn:\n",
    "\n",
    "* how to define a Sklearn classifier in Fed-BioMed (especially `Perceptron` model)\n",
    "* how to train it\n",
    "* how to evaluate the resulting model\n",
    "\n",
    "\n",
    "**HINT** : to reload the notebook,  please click on the following button:\n",
    "\n",
    "`Kernel` -> `Restart and clear Output`\n",
    "\n",
    "    \n",
    "![reload-notebook](../../../assets/img/sketch_reload_notebook.jpg#img-md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06315710",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Clean your environments\n",
    "\n",
    "Before executing notebook and starting nodes, it is safer to remove all configuration scripts automatically generated by Fed-BioMed. To do so, enter the following in a terminal:\n",
    "```\n",
    "source ${FEDBIOMED_DIR}/scripts/fedbiomed_environment clean\n",
    "```\n",
    "**Note**: `${FEDBIOMED_DIR}` is a path relative to based directory of the cloned Fed-BioMed repository. You can set it by running command `export FEDBIOMED_DIR=/path/to/fedbiomed`. This is not required for Fed-BioMed to work but enables you to run the tutorials more easily. \n",
    "\n",
    "## 2. Setting the node up\n",
    "\n",
    "1. `${FEDBIOMED_DIR}/scripts/fedbiomed_run node dataset add`\n",
    "  * Select option 2 (default) to add MNIST to the node\n",
    "  * Confirm default tags by hitting \"y\" and ENTER\n",
    "  * Pick the folder where MNIST is downloaded (this is due torch issue https://github.com/pytorch/vision/issues/3549)\n",
    "  * Data must have been added (if you get a warning saying that data must be unique is because it has been already added)\n",
    "  \n",
    "2. Check that your data has been added by executing `${FEDBIOMED_DIR}/scripts/fedbiomed_run node dataset list`\n",
    "3. Run the node using `${FEDBIOMED_DIR}/scripts/fedbiomed_run node start`. Wait until you get `Connected with result code 0`. it means your node is working and ready to participate to a Federated training.\n",
    "\n",
    "More details are given in tutorial : [Installation/setting up environment ](../../installation/1-setting-up-environment/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb09b8af",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Create Sklearn Federated Perceptron Training Plan\n",
    "\n",
    "\n",
    "The class `FedPerceptron` constitutes the Fed-BioMed wrapper for executing Federated Learning using Scikit-Learn Perceptron model based on mini-batch Stochastic Gradient Descent (SGD). As we have done with Pytorch model in previous chapter, we create a new training plan class `SkLearnClassifierTrainingPlan` that inherits from it. For a refresher on how Training Plans work in Fed-BioMed, please refer to our [Training Plan user guide](../../../user-guide/researcher/training-plan).\n",
    "\n",
    "In scikit-learn Training Plans, you typically need to define only the `training_data` function, and optionally an `init_dependencies` function if your code requires additional module imports. \n",
    "\n",
    "The `training_data` function defines how datasets should be loaded in nodes to make them ready for training. It takes a `batch_size` argument and returns a `DataManager` class. For scikit-learn, the `DataManager` must be instantiated with a `dataset` and a `target` argument, both `np.ndarrays` of the same length.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb84ea0d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.common.training_plans import FedPerceptron\n",
    "from fedbiomed.common.data import DataManager\n",
    "\n",
    "\n",
    "class SkLearnClassifierTrainingPlan(FedPerceptron):\n",
    "    def init_dependencies(self):\n",
    "        \"\"\"Define additional dependencies.\n",
    "        return [\"from torchvision import datasets, transforms\",\n",
    "                \"from torch.utils.data import DataLoader\"]\n",
    "\n",
    "    def training_data(self):\n",
    "        \n",
    "        In this case, we rely on torchvision functions for preprocessing the images.\n",
    "        \"\"\"\n",
    "        return [\"from torchvision import datasets, transforms\",]\n",
    "\n",
    "    def training_data(self):\n",
    "        \"\"\"Prepare data for training.\n",
    "        \n",
    "        This function loads a MNIST dataset from the node's filesystem, applies some\n",
    "        preprocessing and converts the full dataset to a numpy array. \n",
    "        Finally, it returns a DataManager created with these numpy arrays.\n",
    "        \"\"\"\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset = datasets.MNIST(self.dataset_path, train=True, download=False, transform=transform)\n",
    "        \n",
    "        X_train = dataset.data.numpy()\n",
    "        X_train = X_train.reshape(-1, 28*28)\n",
    "        Y_train = dataset.targets.numpy()\n",
    "        return DataManager(dataset=X_train, target=Y_train,  shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a9c80",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Provide dynamic arguments for the model and training. These may potentially be changed at every round.\n",
    "\n",
    "### Model arguments\n",
    "\n",
    "`model_args` is a dictionary with the arguments related to the model, that will be passed to the `Perceptron` constructor. \n",
    "\n",
    "**IMPORTANT** For classification tasks, you are **required** to specify the following two fields:\n",
    "- `n_features`: the number of features in each input sample (in our case, the number of pixels in the images)\n",
    "- `n_classes`: the number of classes in the target data\n",
    "\n",
    "Furthermore, the classes may not be represented by arbitrary values: **classes must be identified by integers in the range 0..n_classes**\n",
    "\n",
    "### Training arguments\n",
    "\n",
    "`training_args` is a dictionary containing the arguments for the training routine (e.g. batch size, learning rate, epochs, etc.). This will be passed to the routine on the node side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7bed8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {'n_features': 28*28,\n",
    "              'n_classes' : 10,\n",
    "              'eta0':1e-6,\n",
    "              'random_state':1234,\n",
    "              'alpha':0.1 }\n",
    "\n",
    "training_args = {\n",
    "    'epochs': 3, \n",
    "    'batch_maxnum': 20,  # can be used to debugging to limit the number of batches per epoch\n",
    "#    'log_interval': 1,  # output a logging message every log_interval batches\n",
    "    'loader_args': {\n",
    "        'batch_size': 4,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8bc423",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Train your model on MNIST dataset\n",
    "\n",
    "MNIST dataset is composed of handwritten digits images, from 0 to 9. The purpose of our classifier is to associate an image to the corresponding represented digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c54acbb",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.federated_workflows import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "tags =  ['#MNIST', '#dataset']\n",
    "rounds = 3\n",
    "\n",
    "# select nodes participating in this experiment\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=SkLearnClassifierTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df22732e",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.run(increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save trained model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.training_plan().export_model('./trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90328b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Testing on MNIST test dataset\n",
    "\n",
    "Let's assess performance of our classifier with  MNIST testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a497d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "from fedbiomed.researcher.config import config\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tmp_dir_model = tempfile.TemporaryDirectory(dir=config.vars['TMP_DIR']+os.sep)\n",
    "model_file = os.path.join(tmp_dir_model.name, 'class_export_mnist.py')\n",
    "\n",
    "# collecting MNIST testing dataset: for that we are downloading the whole dataset on en temporary file\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "testing_MNIST_dataset = datasets.MNIST(root = os.path.join(config.vars['TMP_DIR'], 'local_mnist.tmp'),\n",
    "                                       download = True,\n",
    "                                       train = False,\n",
    "                                       transform = transform)\n",
    "\n",
    "testing_MNIST_data = testing_MNIST_dataset.data.numpy().reshape(-1, 28*28)\n",
    "testing_MNIST_targets = testing_MNIST_dataset.targets.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b60ae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. Getting Loss function \n",
    "\n",
    "Here we use the `aggregated_params()` getter to access all model weights at the end of each round to plot the evolution of Perceptron loss funciton, as well as its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c5e21",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# retrieve Sklearn model and losses at the end of each round\n",
    "\n",
    "from sklearn.linear_model import  SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, hinge_loss\n",
    "\n",
    "fed_perceptron_model = exp.training_plan().model()\n",
    "perceptron_args = {key: model_args[key] for key in model_args.keys() if key in fed_perceptron_model.get_params().keys()}\n",
    "\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for r in range(rounds):\n",
    "    fed_perceptron_model = fed_perceptron_model.set_params(**perceptron_args)\n",
    "    fed_perceptron_model.classes_ = np.unique(testing_MNIST_dataset.targets.numpy())\n",
    "    fed_perceptron_model.coef_ = exp.aggregated_params()[r]['params']['coef_'].copy()\n",
    "    fed_perceptron_model.intercept_ = exp.aggregated_params()[r]['params']['intercept_'].copy()  \n",
    "\n",
    "    prediction = fed_perceptron_model.decision_function(testing_MNIST_data)\n",
    "    losses.append(hinge_loss(testing_MNIST_targets, prediction))\n",
    "    accuracies.append(fed_perceptron_model.score(testing_MNIST_data,\n",
    "                                                testing_MNIST_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e614d861",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7. Comparison with a local `Perceptron` model\n",
    "\n",
    "In this section, we implement a local `Perceptron` model, so we can compare remote and local models accuracies.\n",
    "\n",
    "You can use this section as an insight on how things are implemented within the Fed-BioMed network. \n",
    "In particular, looking at the code in the next few cells you may learn how:\n",
    "- we implement mini-batch gradient descent for scikit-learn models\n",
    "- we implement `Perceptron` based on `SGDClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e091443d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# downloading MNIST dataset\n",
    "training_MNIST_dataset = datasets.MNIST(root = os.path.join(config.vars['TMP_DIR'], 'local_mnist.tmp'),\n",
    "                                       download = True,\n",
    "                                       train = True,\n",
    "                                       transform = transform)\n",
    "\n",
    "training_MNIST_data = training_MNIST_dataset.data.numpy().reshape(-1, 28*28)\n",
    "training_MNIST_targets = training_MNIST_dataset.targets.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418a6c64",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Local Model training loop : a new model is trained locally, then compared with the remote `FedPerceptron` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193ab24",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fed_perceptron_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2925d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "local_perceptron_losses = []\n",
    "local_perceptron_accuracies = []\n",
    "classes = np.unique(training_MNIST_targets)\n",
    "batch_size = training_args[\"loader_args\"][\"batch_size\"]\n",
    "\n",
    "# model definition\n",
    "local_perceptron_model = SGDClassifier()\n",
    "perceptron_args = {key: model_args[key] for key in model_args.keys() if key in fed_perceptron_model.get_params().keys()}\n",
    "local_perceptron_model.set_params(**perceptron_args)\n",
    "model_param_list = ['coef_', 'intercept_']\n",
    "\n",
    "# Model initialization\n",
    "local_perceptron_model.intercept_ = np.zeros((model_args[\"n_classes\"],))\n",
    "local_perceptron_model.coef_ = np.zeros((model_args[\"n_classes\"], model_args[\"n_features\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e27b4a",
   "metadata": {},
   "source": [
    "Implementation of mini-batch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9368ca",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for r in range(rounds):\n",
    "    for e in range(training_args[\"epochs\"]):\n",
    "        \n",
    "        tot_samples_processed = 0\n",
    "        for idx_batch in range(training_args[\"batch_maxnum\"]):\n",
    "            param = {k: getattr(local_perceptron_model, k) for k in model_param_list}\n",
    "            grads = {k: np.zeros_like(v) for k, v in param.items()}\n",
    "            \n",
    "            # for each sample: 1) call partial_fit 2) accumulate the gradients 3) reset the model parameters\n",
    "            for sample_idx in range(tot_samples_processed, tot_samples_processed+batch_size):\n",
    "                local_perceptron_model.partial_fit(training_MNIST_data[sample_idx:sample_idx+1,:],\n",
    "                                                   training_MNIST_targets[sample_idx:sample_idx+1],\n",
    "                                                   classes=classes)\n",
    "                for key in model_param_list:\n",
    "                    grads[key] += getattr(local_perceptron_model, key)\n",
    "                    setattr(local_perceptron_model, key, param[key])\n",
    "                    \n",
    "            tot_samples_processed += batch_size\n",
    "\n",
    "            # after each epoch, we update the model with the averaged gradients over the batch\n",
    "            for key in model_param_list:\n",
    "                setattr(local_perceptron_model, key, grads[key] / batch_size)\n",
    "                \n",
    "    predictions = local_perceptron_model.decision_function(testing_MNIST_data)\n",
    "    local_perceptron_losses.append(hinge_loss(testing_MNIST_targets, predictions))\n",
    "    local_perceptron_accuracies.append(local_perceptron_model.score(testing_MNIST_data,\n",
    "                                                testing_MNIST_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9855e54",
   "metadata": {},
   "source": [
    "Compare the local and federated models. The two curves should overlap almost identically, although slight numerical errors are acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb7911e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(losses, label=\"federated Perceptron losses\")\n",
    "plt.plot(local_perceptron_losses, \"--\", color='r', label=\"local Perceptron losses\")\n",
    "plt.ylabel('Perceptron Cost Function (Hinge)')\n",
    "plt.xlabel('Number of Rounds')\n",
    "plt.title('Perceptron loss evolution on test dataset')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(accuracies, label=\"federated Perceptron accuracies\")\n",
    "plt.plot(local_perceptron_accuracies, \"--\", color='r',\n",
    "         label=\"local Perceptron accuracies\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of Rounds')\n",
    "plt.title('Perceptron accuracy over rounds (on test dataset)')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863d411",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this example, plots appear to be the same: this means that Federated and local `Perceptron` models are performing equivalently!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214bd56",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 8. Getting accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b427d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# federated model predictions\n",
    "fed_prediction = fed_perceptron_model.predict(testing_MNIST_data)\n",
    "acc = accuracy_score(testing_MNIST_targets, fed_prediction)\n",
    "print('Federated Perceptron Model accuracy :', acc)\n",
    "\n",
    "# local model predictions\n",
    "local_prediction = local_perceptron_model.predict(testing_MNIST_data)\n",
    "acc = accuracy_score(testing_MNIST_targets, local_prediction)\n",
    "print('Local Perceptron Model accuracy :', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a1136",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(fig, ax, conf_matrix, title, xlabel, ylabel, n_image=0):\n",
    "    \n",
    "    im = ax[n_image].imshow(conf_matrix)\n",
    "\n",
    "    ax[n_image].set_xticks(np.arange(10))\n",
    "    ax[n_image].set_yticks(np.arange(10))\n",
    "\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            text = ax[n_image].text(j, i, conf_matrix[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax[n_image].set_xlabel(xlabel)\n",
    "    ax[n_image].set_ylabel(ylabel)\n",
    "    ax[n_image].set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2476bb4f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fed_conf_matrix = confusion_matrix(testing_MNIST_targets, fed_prediction)\n",
    "local_conf_matrix = confusion_matrix(testing_MNIST_targets, local_prediction)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2,figsize=(10,5))\n",
    "\n",
    "\n",
    "\n",
    "plot_confusion_matrix(fig, axs, fed_conf_matrix,\n",
    "                      \"Federated Perceptron Confusion Matrix\",\n",
    "                      \"Actual values\", \"Predicted values\", n_image=0)\n",
    "        \n",
    "plot_confusion_matrix(fig, axs, local_conf_matrix,\n",
    "                      \"Local Perceptron Confusion Matrix\",\n",
    "                      \"Actual values\", \"Predicted values\", n_image=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6b64a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Congrats !\n",
    "\n",
    "\n",
    "You have figured out how to train your first Federated Sklearn classifier model !\n",
    "\n",
    "If you want to practise more, you can try to deploy such classifier on two or more nodes. As you can see, `Perceptron` is a limited model: its generalization is `SGDCLassifier`, provided by Fed-BioMed as a `FedSGDCLassifier` Training Plan. You can thus try to apply `SGDCLassifier`, providing more feature such as different cost functions, regularizations and learning rate decays. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
