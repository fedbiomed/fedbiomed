{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace BG Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and configure dataset\n",
    "- Download `REPLACE-BG-Dataset.zip` from this [link](https://public.jaeb.org/datasets/diabetes);\n",
    "- Create a folder in `fedbiomed/data/` named `replace-bg`;\n",
    "- Unzip `REPLACE-BG-Dataset.zip` and move it the folder `Data Tables` in `{FEDBIOMED_DIR}/data/replace-bg`;\n",
    "- Run `python ${FEDBIOMED_DIR}/docs/tutorials/advanced/client-selection/preprocessing.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add 202 nodes and start --num_nodes nodes (up to 202)\n",
    "- Open a terminal and run `python ${FEDBIOMED_DIR}/docs/tutorials/client-selection/add_nodes.py`\n",
    "- run `python ${FEDBIOMED_DIR}/docs/tutorials/sec-agg/client-selection/start_nodes.py --num_nodes 10 #up to 202` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. FL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-05-26 20:10:07,714 fedbiomed DEBUG - Node: NODE_3e1e57c8-1364-41cc-8854-15c82e9d5b93 polling for the tasks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2024-05-26 20:10:15,949 fedbiomed DEBUG - Node: NODE_b14a8bc6-ff16-4c92-8772-4a755ad2855f polling for the tasks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2024-05-26 20:10:18,324 fedbiomed DEBUG - Node: NODE_61dbf902-17f9-4bc9-b9f2-d46bfdca0ec5 polling for the tasks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "FEDBIOMED_DIR = os.environ.get('FEDBIOMED_DIR')\n",
    "SEED = 42\n",
    "\n",
    "DATA_DIR = f\"{FEDBIOMED_DIR}/data/replace-bg/raw/patients\"\n",
    "\n",
    "\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_length = 12 # it mean past 3 hours\n",
    "        self.pred_length = 4 # it mean next hour\n",
    "    \n",
    "    # Defines and return model \n",
    "    def init_model(self, model_args):\n",
    "        return self.CNN_LSTM(model_args = model_args)\n",
    "    \n",
    "    # Defines and return optimizer\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        return torch.optim.SGD(self.model().parameters(), lr = optimizer_args[\"lr\"])\n",
    "    \n",
    "    # Declares and return dependencies\n",
    "    def init_dependencies(self):\n",
    "        deps = [\"import pandas as pd\", \"from torch.utils.data import Dataset\", \"import numpy as np\", \"import datetime\"]\n",
    "        return deps\n",
    "    \n",
    "\n",
    "    class CNN_LSTM(nn.Module):\n",
    "        def __init__(self, model_args):\n",
    "            super().__init__()\n",
    "            # Channels for prediction (assuming one-dimensional time-series data)\n",
    "            predict_channels = [0]\n",
    "            self.predict_channels = predict_channels\n",
    "\n",
    "            # Define the convolutional layers\n",
    "            self.conv_layers = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=3, out_channels=32, kernel_size=7, padding=3),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(in_channels=32, out_channels=64, kernel_size=7, padding=3),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=2),  # MaxPool reduces dimensionality\n",
    "                nn.Conv1d(in_channels=64, out_channels=64, kernel_size=5, padding=2),  # Added padding to maintain dimensions\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, padding=2),  # Added padding to maintain dimensions\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=2),  # MaxPool reduces dimensionality\n",
    "            )\n",
    "\n",
    "            # Define the LSTM layers\n",
    "            self.lstm = nn.LSTM(input_size=128, hidden_size=100, num_layers=2, batch_first=True, dropout=0.2)\n",
    "\n",
    "            # Define the fully connected layers\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.Linear(100, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 6),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(6, len(predict_channels))\n",
    "            )\n",
    "\n",
    "        def _forward(self, x):\n",
    "            \"\"\"\n",
    "            Perform forward pass through the CNN and LSTM layers.\n",
    "\n",
    "            Args:\n",
    "                x: Input tensor with shape (batch_size, input_length, num_features)\n",
    "\n",
    "            Returns:\n",
    "                Output tensor after passing through the CNN and LSTM layers\n",
    "            \"\"\"\n",
    "            x = x.permute(0, 2, 1)  # Change shape to (batch_size, num_features, input_length) for Conv1d\n",
    "            x = self.conv_layers(x)\n",
    "            x = x.permute(0, 2, 1)  # Change back to (batch_size, input_length, num_features) for LSTM\n",
    "            x, _ = self.lstm(x)\n",
    "            x = self.fc_layers(x[:, -1, :])  # Use the output of the last LSTM cell\n",
    "            return x\n",
    "\n",
    "        def forward(self, whole_example, input_length):\n",
    "            \"\"\"\n",
    "            Perform sequential prediction using teacher forcing.\n",
    "\n",
    "            Args:\n",
    "                whole_example: Input tensor with shape (batch_size, total_length, num_features)\n",
    "                input_length: Length of the sequence to use as input initially\n",
    "\n",
    "            Returns:\n",
    "                Tensor with the same shape as whole_example, where the values from input_length\n",
    "                onward have been replaced with model predictions\n",
    "            \"\"\"\n",
    "            whole_example_clone = whole_example.clone().detach()  # Create a detached clone of the input tensor\n",
    "            total_length = whole_example_clone.shape[1]  # Get the total length of the sequence\n",
    "            assert input_length < total_length, \"input_length should be less than total_length\"\n",
    "\n",
    "            # Sequentially predict and replace the values in the cloned tensor\n",
    "            # This process uses teacher forcing: it feeds the true previous (up to :input_length) values as input\n",
    "            # And from [input_length:total_length] it feeds the prediction\n",
    "            while input_length < total_length:\n",
    "                # Use the sequence up to the current input_length as input\n",
    "                x = whole_example[:, :input_length, :]\n",
    "                # Perform forward pass to get the prediction for the next time step\n",
    "                y_hat = self._forward(x)\n",
    "                # Replace the value at the current input_length position with the prediction\n",
    "                whole_example_clone[:, input_length, self.predict_channels] = y_hat[:, self.predict_channels]\n",
    "                # Increment input_length to move to the next position\n",
    "                input_length += 1\n",
    "\n",
    "            return whole_example_clone\n",
    "\n",
    "    class ReplaceBGDataset(Dataset):\n",
    "        \"\"\"The Replace-BG dataset for Torch training.\"\"\"\n",
    "    \n",
    "        def __init__(self, raw_df, example_len, unimodal=False):\n",
    "            \"\"\"\n",
    "            Args\n",
    "                raw_df: dataframe\n",
    "                example_len: int\n",
    "                unimodal: bool\n",
    "                    If True, data contains glucose only\n",
    "            \"\"\"\n",
    "            raw_df.replace(to_replace=-1, value=np.nan, inplace=True)\n",
    "            self.example_len = example_len\n",
    "            self.unimodal = unimodal\n",
    "            self.data, self.times = self._initial(raw_df)  # (len, n_features)\n",
    "            self.example_indices = self._example_indices(self.times)\n",
    "            self.external_mean = [160.87544032,   0.21893523,   1.49783614]\n",
    "            self.external_std = [63.60143682,  1.14457581,  8.92042825]\n",
    "            self._standardise(self.external_mean, self.external_std)\n",
    "            print(\"Dataset loaded, total examples: {}.\".format(len(self)))\n",
    "    \n",
    "        \n",
    "        @staticmethod\n",
    "        def str2dt(s):\n",
    "            return datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "        def _initial(self, raw_df):\n",
    "            times = [self.str2dt(s) for s in raw_df[\"time\"]]\n",
    "            glucose = raw_df[\"GlucoseValue\"].to_numpy(dtype=np.float32)\n",
    "            bolus = raw_df[\"Normal\"].to_numpy(dtype=np.float32)\n",
    "            carbs = raw_df[\"CarbInput\"].to_numpy(dtype=np.float32)\n",
    "    \n",
    "            bolus[np.isnan(bolus)] = 0.0\n",
    "            carbs[np.isnan(carbs)] = 0.0\n",
    "    \n",
    "            return (\n",
    "                np.array(\n",
    "                    [\n",
    "                        glucose,\n",
    "                        bolus,\n",
    "                        carbs,\n",
    "                    ],\n",
    "                    dtype=np.float32,\n",
    "                ).T,\n",
    "                times,\n",
    "            )\n",
    "    \n",
    "        def _example_indices(self, times):\n",
    "            \"\"\"Extract every possible example from the dataset, st. all data entry in this example is not missing.\n",
    "    \n",
    "            Returns:\n",
    "                [(start_row, end_row)]\n",
    "                    Starting and ending indices for each possible example from this dataframe.\n",
    "            \"\"\"\n",
    "            res = []\n",
    "            total_len = self.data.shape[0]\n",
    "    \n",
    "            def look_ahead(start):\n",
    "                end = start\n",
    "                res = []\n",
    "                while end < total_len:\n",
    "                    if np.any(np.isnan(self.data[end, :])):\n",
    "                        break\n",
    "                    if end - start + 1 >= self.example_len:\n",
    "                        # check that between start and end, there is the difference of self.example_len * 15 minutes\n",
    "                        gap_min = self.example_len * 15\n",
    "                        if (times[end] - times[end - self.example_len + 1]) <= datetime.timedelta(minutes=gap_min):\n",
    "                            res.append((end - self.example_len + 1, end))\n",
    "                    end += 1\n",
    "                return res, end\n",
    "    \n",
    "            i = 0\n",
    "            while i < total_len:\n",
    "                if not np.any(np.isnan(self.data[i, :])):\n",
    "                    temp_res, temp_end = look_ahead(i)\n",
    "                    res += temp_res\n",
    "                    i = temp_end + 1\n",
    "                else:\n",
    "                    i += 1\n",
    "            return res\n",
    "    \n",
    "        def _standardise(self, external_mean=None, external_std=None):\n",
    "            if external_mean is None and external_std is None:\n",
    "                mean = []\n",
    "                std = []\n",
    "                for i in range(self.data.shape[1]):\n",
    "                    mean.append(np.nanmean(self.data[:, i]))\n",
    "                    std.append(np.nanstd(self.data[:, i]))\n",
    "            else:\n",
    "                mean = external_mean\n",
    "                std = external_std\n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "            print(\"Standardising with mean: {} and std: {}.\".format(mean, std))\n",
    "            for i in range(self.data.shape[1]):\n",
    "                self.data[:, i] = (self.data[:, i] - mean[i]) / std[i]\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.example_indices)\n",
    "    \n",
    "        def __getitem__(self, idx):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                idx: int\n",
    "            Returns:\n",
    "                (example_len, channels)\n",
    "            \"\"\"\n",
    "            start_row, end_row = self.example_indices[idx]\n",
    "            res = torch.from_numpy(self.data[start_row : end_row + 1, :])\n",
    "            # print(f\"start_row: {self.times[start_row]}, end_row: {self.times[end_row +1]}\")\n",
    "            return res,0  #Dummy variable to be compatible with TorchTrainingPlan\n",
    "\n",
    "    def training_data(self):\n",
    "        dataset1 = self.ReplaceBGDataset(raw_df=pd.read_csv(self.dataset_path), example_len=16)\n",
    "        train_kwargs = { 'shuffle': True}\n",
    "        return DataManager(dataset=dataset1, **train_kwargs)\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        target = data[:, -self.pred_length :, 0]\n",
    "        output = self.model().forward(data, self.input_length)[:, -self.pred_length :, 0] \n",
    "        loss = torch.nn.functional.l1_loss(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_args = {}\n",
    "\n",
    "training_args = {\n",
    "    'loader_args': { 'batch_size': 64, }, \n",
    "    'optimizer_args': {\n",
    "        \"lr\" : 1e-3\n",
    "    },\n",
    "    'num_updates': 10, \n",
    "    'dry_run': False,  \n",
    "    'random_seed':SEED, \n",
    "    'log_interval': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.strategies import DefaultStrategy\n",
    "import random\n",
    "class UniformSelectStrategy(DefaultStrategy):\n",
    "    \n",
    "    def __init__(self, sampling_fraction = 1.0):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Initialize the class with a sampling fraction.\n",
    "\n",
    "        Args:\n",
    "            sampling_fraction (float): Fraction of nodes to sample each round. Must be between 0 and 1.\n",
    "        \"\"\"\n",
    "        self.sampling_fraction = sampling_fraction\n",
    "\n",
    "    def sample_nodes(self, from_nodes, round_i):\n",
    "        \"\"\"\n",
    "        Samples and selects nodes on which to train the local model. This strategy considers a subsample of existing nodes.\n",
    "\n",
    "        Args:\n",
    "            from_nodes: List of node ids which may be sampled.\n",
    "            round_i: Number of the round.\n",
    "\n",
    "        Returns:\n",
    "            List of node ids considered for training during this round `round_i`.\n",
    "        \"\"\"\n",
    "        random.seed(SEED + round_i)  # Using round_i to ensure different seeds for different rounds\n",
    "        num_nodes_to_sample = int(len(from_nodes) * self.sampling_fraction)\n",
    "        sampled_nodes = random.sample(from_nodes, num_nodes_to_sample)\n",
    "\n",
    "        self._sampling_node_history[round_i] = sampled_nodes\n",
    "\n",
    "        return sampled_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.federated_workflows import Experiment\n",
    "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
    "\n",
    "\n",
    "tags =  ['replace-bg']\n",
    "rounds = 5\n",
    "\n",
    "exp = Experiment(tags=tags,\n",
    "                 model_args=model_args,\n",
    "                 training_plan_class=MyTrainingPlan,\n",
    "                 training_args=training_args,\n",
    "                 round_limit=rounds,\n",
    "                 aggregator=FedAverage(),\n",
    "                 node_selection_strategy=UniformSelectStrategy(0.5))\n",
    "exp.set_retain_full_history(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nList the training rounds : \", exp.training_replies().keys())\n",
    "\n",
    "print(\"\\nList the nodes for the last training round and their timings : \")\n",
    "for rnd in range(rounds):\n",
    "    round_data = exp.training_replies()[rnd]\n",
    "    for r in round_data.values():\n",
    "        print(\"\\t- {id} :\\\n",
    "        \\n\\t\\trtime_training={rtraining:.2f} seconds\\\n",
    "        \\n\\t\\tptime_training={ptraining:.2f} seconds\\\n",
    "        \\n\\t\\trtime_total={rtotal:.2f} seconds\".format(id = r['node_id'],\n",
    "            rtraining = r['timing']['rtime_training'],\n",
    "            ptraining = r['timing']['ptime_training'],\n",
    "            rtotal = r['timing']['rtime_total'],))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test the global model for each round over testing nodes: 200,201, 203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = [f for f in os.listdir(DATA_DIR) if f.endswith('.csv')]\n",
    "patients.sort()\n",
    "patients = [int(p.replace(\".csv\", \"\")) for p in patients]\n",
    "\n",
    "patients_testing = patients[200:]\n",
    "test_dataset = torch.utils.data.ConcatDataset(\n",
    "        [\n",
    "            MyTrainingPlan.ReplaceBGDataset(\n",
    "                raw_df=pd.read_csv(os.path.join(DATA_DIR, \"{}.csv\".format(p))),\n",
    "            example_len=16)\n",
    "            for p in patients_testing\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def testing_rmse(model, data_loader):\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    input_length = 12\n",
    "    pred_length = 4\n",
    "\n",
    "    device = \"gpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize lists to collect predictions and targets\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, target) in enumerate(data_loader):\n",
    "            target = data[:, -pred_length:, 0]\n",
    "            target = target * 63.60143682 + 160.87544032\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data, input_length)[:, -pred_length:, 0]\n",
    "            output = output * 63.60143682 + 160.87544032\n",
    "\n",
    "            # Collect predictions and targets\n",
    "            predictions.extend(output.cpu().tolist())\n",
    "            targets.extend(target.cpu().tolist())\n",
    "\n",
    "            # Only uses 10% of the dataset for faster results\n",
    "            if idx >= len(data_loader) / 10:\n",
    "                break\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    predictions = torch.tensor(predictions)\n",
    "    targets = torch.tensor(targets)\n",
    "\n",
    "    # Calculate the Root Mean Squared Error\n",
    "    final_rmse = torch.sqrt(torch.mean((predictions - targets) ** 2))\n",
    "\n",
    "    return final_rmse.item()\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32  # Adjust as needed\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.43421936035156\n",
      "143.565185546875\n",
      "143.51353454589844\n",
      "143.60012817382812\n",
      "143.7473907470703\n",
      "143.89306640625\n",
      "143.95703125\n",
      "143.8656005859375\n",
      "143.8738555908203\n",
      "144.01394653320312\n",
      "143.94305419921875\n",
      "143.94717407226562\n",
      "144.00003051757812\n"
     ]
    }
   ],
   "source": [
    "for r in range(rounds):\n",
    "    fed_model = exp.training_plan().model()\n",
    "    fed_model.load_state_dict(exp.aggregated_params()[r]['params'])\n",
    "    print(f\"Round{r}, RMSE{testing_rmse(fed_model,test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
