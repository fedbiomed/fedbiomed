{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b098b814",
   "metadata": {},
   "source": [
    "# Advanced optimizers in Fed-BioMed\n",
    "\n",
    "\n",
    "**Difficulty level**: **advanced**\n",
    "    \n",
    "## Introduction\n",
    "\n",
    "This tutorial presents on how  to deal with heterogeneous dataset by changing its `Optimizer`. \n",
    "In `Fed-BioMed`, one can specify two sort of `Optimizer`s:\n",
    "\n",
    "1. a `Optimizer` on the `Node` side, defined on the `Training Plan`\n",
    "2. a `Optimizer` on the `Researcher` side, configured in the `Experiment`\n",
    "\n",
    "Advanced `Optimizer` are backed by [`declearn` package](https://gitlab.inria.fr/magnet/declearn/declearn2), a python package focused on `Optimization` for Federated Learning. Advanced `Optimizer` can be used regardless of the machine learning framework (meaning it is compatible with both sklearn and PyTorch)\n",
    "\n",
    "\n",
    "In this tutorial you will learn:\n",
    "- how to use and chain one or several `Optimizers` on `Node` and `Researcher` side\n",
    "- how to use fedopt\n",
    "- how to use `Optimizers` that exchange auxiliary variables such as `Scaffold`\n",
    "\n",
    "For further details you can refer to the [`Optimizer` section in the User Guide](../../../user-guide/advanced-optimization) as well as [the declearn documentation on `Optimizers`](https://gitlab.inria.fr/magnet/declearn/declearn2/-/blob/optimizer-guide/docs/user-guide/optimizer.md).\n",
    "\n",
    "\n",
    "## 1. Configuring `Nodes`\n",
    "\n",
    "Before starting, we need to configure several `Nodes` and add MedNist dataset to it. Node configuration steps require `fedbiomed-node` conda environment. Please make sure that you have the necessary conda environment: this is explained in the [installation tutorial](../../installation/0-basic-software-installation). \n",
    "\n",
    "\n",
    "Please open a terminal, `cd` to the base directory of the cloned fedbiomed project and follow the steps below.    \n",
    "\n",
    "* **Configuration Steps:**\n",
    "    * Run `fedbiomed node dataset add` in the terminal\n",
    "    * It will ask you to select the data type that you want to add. The third option has been configured to add the MedNIST dataset. Please type `3` and continue. \n",
    "    * Please use default tags which are `#MEDNIST` and `#dataset`.\n",
    "    * For the next step, please select the directory that you want to download the MNIST dataset.\n",
    "    * After the download is completed you will see the details of the MNIST dataset on the screen.\n",
    " \n",
    "Please run the command below in the same terminal to make sure the MNIST dataset is successfully added to the Node. \n",
    "\n",
    "```bash\n",
    "$ fedbiomed node --path my-node dataset add\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "$ fedbiomed node --path my-node start\n",
    "```\n",
    "\n",
    "In another terminal, you may proceed by launching a second `Node`. Please repeat the above **configuration steps**, but by specifying another configuration file (for instance `conf2.ini`).\n",
    "\n",
    "```bash\n",
    "$ fedbiomed node --path my-second-node dataset add\n",
    "$ fedbiomed node --path my-second-node start\n",
    "```\n",
    "\n",
    "## 2. Defining an `Optimizer` on `Node` side\n",
    "\n",
    "`Optimizers` are defined through the `init_optimizer` method of the `training plan`. They must be set using `Fed-BioMed` `Optimizer` object (ie from `fedbiomed.common.optimizers.optimizer.Optimizer`)\n",
    "\n",
    "### 2.1 With PyTorch framework\n",
    "\n",
    "In [this tutorial](../../../tutorials/pytorch/01_PyTorch_MNIST_Single_Node_Tutorial/) we have showcased the use of a PyTorch model with [PyTorch native optimizers](https://pytorch.org/docs/stable/optim.html), such as `torch.optim.SGD`. In the present tutorial, we will see how to use `declearn` cross frameworks optimizers\n",
    "\n",
    "**PyTorch `Training Plan`**\n",
    "\n",
    "Below is a simple implementation of a `declearn` SGD `Optimizer` on a PyTorch model. It is equivalent to the following `Training Plan` (that uses native Pytorch Optimizer `torch.optim.SGD`):\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    ...\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        return torch.optim.SGD(self.model().parameters(), lr = optimizer_args['lr'])\n",
    "```\n",
    "<a id=first-training-plan> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6546baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import densenet121\n",
    "from fedbiomed.common.optimizers.optimizer import Optimizer\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# we will use the densnet121 model\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    \n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "                \"from torchvision.models import densenet121\",\n",
    "                \"from fedbiomed.common.optimizers.optimizer import Optimizer\"]\n",
    "\n",
    "        return deps\n",
    "    \n",
    "    def init_model(self):\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        model = densenet121(pretrained=True)\n",
    "        model.classifier =nn.Sequential(nn.Linear(1024,512), nn.Softmax())\n",
    "        return model \n",
    "    \n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        # Defines and return a declearn optimizer\n",
    "        # equivalent: Optimizer(lr=optimizer_args['lr'], modules=[], regurlarizers=[])\n",
    "        return Optimizer(lr=optimizer_args['lr'])\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        preprocess = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(\n",
    "                                            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                                        )])\n",
    "        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset=train_data, **train_kwargs)\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss   = self.loss_function(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb6e0a",
   "metadata": {},
   "source": [
    "### 2.2 Sklearn `Training Plan`\n",
    "\n",
    "For another machine learning framework such as sklearn, `init_optimizer` method syntax is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.common.training_plans import FedSGDClassifier\n",
    "from fedbiomed.common.data import DataManager\n",
    "from fedbiomed.common.optimizers.optimizer import Optimizer\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyTrainingPlan(FedSGDClassifier):\n",
    "    # Declares and return dependencies\n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "                \"from fedbiomed.common.optimizers.optimizer import Optimizer\",\n",
    "                \"import torch\"]\n",
    "        return deps\n",
    "\n",
    "    def training_data(self, batch_size):\n",
    "        # in comparison to PyTorch Training Plan, preprocess involves additional steps in order to be used \n",
    "        # with sklearn SGDClassifier, which is expecting vectors in lieu of arrays\n",
    "        # here we are grayscaling and reshaping images\n",
    "        squeezer = lambda x: torch.squeeze(x) # removes extra dimensions\n",
    "        preprocess = transforms.Compose([transforms.ToTensor(),\n",
    "                                                transforms.Normalize(\n",
    "                                                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                                                ),\n",
    "                                         transforms.Grayscale(1),\n",
    "                                         transforms.Resize((64*64, 1)),\n",
    "                                         transforms.Lambda(squeezer)\n",
    "                                        ])\n",
    "\n",
    "        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n",
    "        return DataManager(dataset=train_data, batch_size=batch_size)\n",
    "\n",
    "    # Defines and return a declearn optimizer\n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        return Optimizer(lr=optimizer_args['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4477a5",
   "metadata": {},
   "source": [
    "### 2.3 Using a more advanced `Optimizer` with `Regularizer`\n",
    "\n",
    "`Optimizer` from `fedbiomed.common.optimizers.optimizer` with learning rate equals `.1` can be written as ```Optimizer(lr=.1, decay=0., modules=[], regularizers=[])```, where:\n",
    "\n",
    "- `decay` is the weight decay ;\n",
    "- `modules` is a python list containing no, one or several [`declearn` `OptiModules`](https://magnet.gitlabpages.inria.fr/declearn/docs/2.2/api-reference/optimizer/modules/OptiModule/) ;\n",
    "- `regularizers` is a python list containing no, one or several [`declearn` `Regularizers`](https://magnet.gitlabpages.inria.fr/declearn/docs/2.2/api-reference/optimizer/regularizers/Regularizer/).\n",
    "\n",
    "We will re-use the `Pytorch Training Plan` already defined above and show how to use a `Adam` `Optimizer` with `Ridge` as the `Regularizer`. For that, we need to import the `Adam`  and the `Ridge` versions of `declearn` (`AdamModule` and `RidgeRegularizer`).\n",
    "\n",
    "Then the `Training Plan` can be defined as follow (for PyTorch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0835dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import densenet121\n",
    "from fedbiomed.common.optimizers.optimizer import Optimizer\n",
    "from fedbiomed.common.optimizers.declearn import AdamModule, RidgeRegularizer\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# we will use the densnet121 model\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    \n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "                \"from torchvision.models import densenet121\",\n",
    "                \"from fedbiomed.common.optimizers.optimizer import Optimizer\",\n",
    "                \"fedbiomed.common.optimizers.declearn import AdamModule\",\n",
    "                \"fedbiomed.common.optimizers.declearn import RidgeRegularizer\"]\n",
    "\n",
    "        return deps\n",
    "    \n",
    "    def init_model(self):\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        model = densenet121(pretrained=True)\n",
    "        model.classifier =nn.Sequential(nn.Linear(1024,512), nn.Softmax())\n",
    "        return model \n",
    "    \n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        # Defines and return a declearn optimizer\n",
    "        # equivalent: Optimizer(lr=optimizer_args['lr'], modules=[], regurlarizers=[])\n",
    "        return Optimizer(lr=optimizer_args['lr'], modules=[AdamModule()], regularizers=[RidgeRegularizer()])\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        preprocess = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(\n",
    "                                            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                                        )])\n",
    "        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset=train_data, **train_kwargs)\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss   = self.loss_function(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7dc22",
   "metadata": {},
   "source": [
    "### 2.4. Create the `Experiment`\n",
    "\n",
    "Once the `Training Plan` has been created with a specific framework model, definition of the `Experiment` is the same as the one in PyTorch or Scikit-Learn, as shown below:\n",
    "\n",
    "*Note: There are a small additional parameters you have to configure in the `model_args` for scikit-learn, that have been added but that will be ignored for PyTorch model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02bd7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "model_args = {'n_features': 64*64,\n",
    "              'n_classes' : 6,\n",
    "              'eta0':lr}\n",
    "\n",
    "training_args = {\n",
    "    'loader_args': {\n",
    "        'batch_size': 8,\n",
    "    },\n",
    "    'optimizer_args': {\n",
    "        \"lr\" : lr\n",
    "    },\n",
    "    'dry_run': False,\n",
    "    'num_updates': 50\n",
    "}\n",
    "\n",
    "tags =  ['#dataset', '#MEDNIST']\n",
    "rounds = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf4a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.federated_workflows import Experiment\n",
    "from fedbiomed.researcher.aggregators import FedAverage\n",
    "from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n",
    "\n",
    "\n",
    "exp = Experiment()\n",
    "exp.set_training_plan_class(training_plan_class=MyTrainingPlan)\n",
    "exp.set_model_args(model_args=model_args)\n",
    "exp.set_training_args(training_args=training_args)\n",
    "exp.set_tags(tags = tags)\n",
    "exp.set_aggregator(aggregator=FedAverage())\n",
    "exp.set_round_limit(rounds)\n",
    "exp.set_training_data(training_data=None, from_tags=True)\n",
    "exp.set_strategy(node_selection_strategy=DefaultStrategy())\n",
    "\n",
    "exp.run(increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d5730",
   "metadata": {},
   "source": [
    "Save trained model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec4f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.training_plan().export_model('./trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33934af",
   "metadata": {},
   "source": [
    "To get and display the content of all `OptiModules` (respectively the `Regularizers`) available **and compatible with Fed-BioMed** , one can use `list_optim_modules` (resp. `list_optim_regularizers`), as shown as below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.common.optimizers.declearn import list_optim_modules,  list_optim_regularizers\n",
    "\n",
    "list_optim_modules(), list_optim_regularizers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5ea3f",
   "metadata": {},
   "source": [
    "## 3. Defining an `Optimizer` on `Researcher` side: `FedOpt`\n",
    "\n",
    "In some case, you may want to use **Adaptive Federated Optimization**, also called [`FedOpt`](https://arxiv.org/pdf/2003.00295.pdf): the idea behind `FedOpt` is to optimize also the global model on `Researcher` side in addition to the `Nodes` local models, mainly to tackle data heterogeneity. Optimization on `Researcher` side is done by computing a pseudo gradient, which is the difference of the updates whithin 2 successive `Round`s. \n",
    "\n",
    "Adaptative Federated Optimization can be done in `Fed-BioMed` with `declearn` modules through the use of [`Experiment.set_agg_optimizer` method](https://fedbiomed.org/latest/developer/api/researcher/experiment/#fedbiomed.researcher.federated_workflows.Experiment.Experiment.set_agg_optimizer).\n",
    "\n",
    "**Important**: Please note that it is not possible to use native framework optimizers on `Researcher` side (such as `torch.optim.Optimizer` for instance). Only `Fed-BioMed`/`declearn` `Optimizer` can be used.\n",
    "\n",
    "For instance, if one wants to use `FedYogi`, using [the first `Training Plan` (that is based on SGD optimizer)](#first-training-plan), the `Experiment` will be written as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05decc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.federated_workflows import Experiment\n",
    "from fedbiomed.researcher.aggregators import FedAverage\n",
    "from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n",
    "from fedbiomed.common.optimizers.declearn import YogiModule as FedYogi\n",
    "\n",
    "exp = Experiment()\n",
    "exp.set_training_plan_class(training_plan_class=MyTrainingPlan)\n",
    "exp.set_model_args(model_args=model_args)\n",
    "exp.set_training_args(training_args=training_args)\n",
    "exp.set_tags(tags = tags)\n",
    "exp.set_aggregator(aggregator=FedAverage())\n",
    "exp.set_round_limit(rounds)\n",
    "exp.set_training_data(training_data=None, from_tags=True)\n",
    "exp.set_strategy(node_selection_strategy=DefaultStrategy())\n",
    "\n",
    "# here we are adding an Optimizer on Researcher side (FedYogi)\n",
    "fed_opt = Optimizer(lr=.8, modules=[FedYogi()])\n",
    "exp.set_agg_optimizer(fed_opt)\n",
    "\n",
    "exp.run(increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33773a4b",
   "metadata": {},
   "source": [
    "Save trained model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c4cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.training_plan().export_model('./trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b343f",
   "metadata": {},
   "source": [
    "## 4. Defining [`Scaffold`](https://arxiv.org/pdf/1910.06378.pdf) through `Optimizer`\n",
    "\n",
    "In the following subsection, we will present [`Scaffold`](https://arxiv.org/pdf/1910.06378.pdf): `Scaffold` purpose is to limit the so called client drift that may happen when dealing with heterogenous dataset accross `Node`s. For that, `Scaffold` involves the exchange between `Node`s and `Researcher` of additional parameters called correction states, which quantitize how much clients has drifted (drift can be considered as the difference between client's local extrema and global extrema). In `Fed-BioMed`, additional parameters that are requiered by `Optimizers` are called `auxiliary variables`: correction states in `Scaffold` is one of them.\n",
    "\n",
    "`declearn` comes with `Scaffold` as 2 `OptiModules`: \n",
    "1. a [`ScaffoldClientModule`](https://magnet.gitlabpages.inria.fr/declearn/docs/2.2/api-reference/optimizer/modules/ScaffoldClientModule/) on `Node` side ;\n",
    "2. a [`ScaffoldServerModule`](https://magnet.gitlabpages.inria.fr/declearn/docs/2.2/api-reference/optimizer/modules/ScaffoldServerModule/) on `Researcher` side.\n",
    "\n",
    "For plain `Scaffold`, the `Training Plan` would look like (for a PyTorch model):\n",
    "\n",
    "**Important: `FedAvg` `Aggregator` in `Fed-BioMed` refers to the way model weights are aggregated, and should not be confused with the `FedAvg` algorithm, which is basically a SGD optimizer performed on `Node` side using `FedAvg` `Aggregtor`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
    "from fedbiomed.common.data import DataManager\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import densenet121\n",
    "from fedbiomed.common.optimizers.optimizer import Optimizer\n",
    "from fedbiomed.common.optimizers.declearn import ScaffoldClientModule\n",
    "\n",
    "# Here we define the model to be used. \n",
    "# we will use the densnet121 model\n",
    "class MyTrainingPlan(TorchTrainingPlan):\n",
    "    \n",
    "    def init_dependencies(self):\n",
    "        deps = [\"from torchvision import datasets, transforms\",\n",
    "                \"from torchvision.models import densenet121\",\n",
    "                \"from fedbiomed.common.optimizers.optimizer import Optimizer\",\n",
    "                \"from declearn.optimizer.modules import ScaffoldClientModule\"]\n",
    "\n",
    "        return deps\n",
    "    \n",
    "    def init_model(self):\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        model = densenet121(pretrained=True)\n",
    "        model.classifier =nn.Sequential(nn.Linear(1024,512), nn.Softmax())\n",
    "        return model \n",
    "    \n",
    "    def init_optimizer(self, optimizer_args):\n",
    "        # Defines and return a declearn optimizer\n",
    "        return Optimizer(lr=optimizer_args['lr'], modules=[ScaffoldClientModule()])\n",
    "\n",
    "    def training_data(self, batch_size = 48):\n",
    "        preprocess = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(\n",
    "                                            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                                        )])\n",
    "        train_data = datasets.ImageFolder(self.dataset_path,transform = preprocess)\n",
    "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
    "        return DataManager(dataset=train_data, **train_kwargs)\n",
    "    \n",
    "    def training_step(self, data, target):\n",
    "        output = self.model().forward(data)\n",
    "        loss   = self.loss_function(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95950b4b",
   "metadata": {},
   "source": [
    "The `Experiment` will be defined that way, with an `Optimizer` configured with `ScaffoldServerModule` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "model_args = {}\n",
    "training_args = {\n",
    "    'loader_args': {\n",
    "        'batch_size': 8,\n",
    "    },\n",
    "    'optimizer_args': {\n",
    "        \"lr\" : lr\n",
    "    },\n",
    "    'dry_run': False,\n",
    "    'num_updates': 50\n",
    "}\n",
    "\n",
    "tags =  ['#dataset', '#MEDNIST']\n",
    "rounds = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d93e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedbiomed.researcher.federated_workflows import Experiment\n",
    "from fedbiomed.researcher.aggregators import FedAverage\n",
    "from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n",
    "from fedbiomed.common.optimizers.declearn import ScaffoldServerModule\n",
    "\n",
    "exp = Experiment()\n",
    "exp.set_training_plan_class(training_plan_class=MyTrainingPlan)\n",
    "exp.set_model_args(model_args=model_args)\n",
    "exp.set_training_args(training_args=training_args)\n",
    "exp.set_tags(tags = tags)\n",
    "exp.set_aggregator(aggregator=FedAverage())\n",
    "exp.set_round_limit(rounds)\n",
    "exp.set_training_data(training_data=None, from_tags=True)\n",
    "exp.set_strategy(node_selection_strategy=DefaultStrategy())\n",
    "\n",
    "# here we are adding an Optimizer on Researcher side (FedYogi)\n",
    "fed_opt = Optimizer(lr=.8, modules=[ScaffoldServerModule()])\n",
    "exp.set_agg_optimizer(fed_opt)\n",
    "\n",
    "exp.run(increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save trained model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.training_plan().export_model('./trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd1873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.run(rounds=1, increase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save trained model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.training_plan().export_model('./trained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5bacc7",
   "metadata": {},
   "source": [
    "## 5. Explore advanced `Optimizer` feature through `declearn` and the Fed-BioMed user guide\n",
    "\n",
    "\n",
    "**Congrats!**\n",
    "\n",
    "In this tutorial, you learned how to conduct your `Experiment` using advanced cross framework `Optimizer`provided by `declearn`. `declearn` modules offers the possibilty to chain `Optimizers` and `Regularizers`, making possible to customize as much as possible your federated `Expermient`. `declearn` compatible modules with Fed-BioMed are provided in `fedbiomed.common.optimizers.declearn`\n",
    "\n",
    "For more in depth analysis on `declearn` `Optimizer`, please reach the [`Optimizer` section in the `User Guide`]()\n",
    "\n",
    "\n",
    "Please also check [`declearn` documentation](https://magnet.gitlabpages.inria.fr/declearn/docs/2.2/user-guide/) for further details reagrding `declearn` package."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
